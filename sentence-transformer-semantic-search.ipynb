{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**The reference:**\n* https://huggingface.co/blog/how-to-train-sentence-transformers\n* https://www.kaggle.com/code/andtaichi/finetunig-sentencetransformer\n* https://www.kaggle.com/code/quincyqiang/download-huggingface-pretrain-for-kaggle/notebook\n* https://towardsdatascience.com/easy-kaggle-offline-submission-with-chaining-kernels-30bba5ea5c4d\n* https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/109679\n* https://www.kaggle.com/code/jamiealexandre/sample-notebook-data-exploration/notebook\n\nThe pretrained model we use:\nhttps://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n\n**The highlights:**\n1. Join the correlation with topic and content.\n2. Concatenate \"topic_title\" and \"topic_description\" for each topic, and concate all the ancestor topics into \"topic_full\" field.\n3. Concatenate 'content_title', 'content_description', \"content_text\" for each content into \"content_full\" field.\n4. Feed \"topic_full\" and \"content_full\" as embeding pair into the model and fintune the model.\n5. Using the fine-tuned model, generate the embeding for \"topic_full\" and \"content_full\", and put them into embeding dataset.\n6. Create the faiss index by calling add_faiss_index.\n7. For each topic that need to be predict, search in the embeding dataset with faiss index using content_full, find the nearest K content (current value is 20), then filter out the result.\n8. Furhter filter out the result by calculating the cosine_sim between each content and the topic_full, and cutting off using Cosine_Cutoff (current value is larger than 0.99995).\n9. Output the result.","metadata":{"_uuid":"2eebf9ad-e525-43c9-b91a-573dc2b8371c","_cell_guid":"e173c99e-b6e7-456c-bbd1-a1bc7dad5eb2","trusted":true}},{"cell_type":"code","source":"# !pip install sentence-transformers\n# !pip install faiss-gpu\n# !pip install faiss-cpu\n# !pip install tqdm\n# !pip install nvidia-ml-py3\n# !pip install accelerate\n# !pip install scikit-learn","metadata":{"_uuid":"392f30be-26d3-4fd7-99b8-1c681968bdda","_cell_guid":"c4dd020c-7ab0-45bb-a58d-c1465f32b961","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:00:14.628730Z","iopub.execute_input":"2023-03-02T17:00:14.629194Z","iopub.status.idle":"2023-03-02T17:00:14.635991Z","shell.execute_reply.started":"2023-03-02T17:00:14.629155Z","shell.execute_reply":"2023-03-02T17:00:14.634331Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh |  bash\n# !apt-get install -y --allow-unauthenticated git-lfs","metadata":{"_uuid":"d716877c-1b75-497d-81ec-9f9ae4c67c72","_cell_guid":"5d0a841f-e224-4f27-9c35-d4c13ac059e3","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:00:14.644510Z","iopub.execute_input":"2023-03-02T17:00:14.644945Z","iopub.status.idle":"2023-03-02T17:00:14.653704Z","shell.execute_reply.started":"2023-03-02T17:00:14.644908Z","shell.execute_reply":"2023-03-02T17:00:14.651493Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git lfs install\n# !git clone https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n# # if you want to clone without large files â€“ just their pointers\n# # prepend your git clone with the following env var:\n# !GIT_LFS_SKIP_SMUDGE=1","metadata":{"_uuid":"4fc16c87-b0a3-4efe-9446-3f2ef1f96d60","_cell_guid":"185cf237-985c-4dd3-83bf-a752c6201b88","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:00:14.656170Z","iopub.execute_input":"2023-03-02T17:00:14.657055Z","iopub.status.idle":"2023-03-02T17:00:14.664424Z","shell.execute_reply.started":"2023-03-02T17:00:14.657006Z","shell.execute_reply":"2023-03-02T17:00:14.662876Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentence-transformers --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/sentence-transformers-2.2.2.tar.gz --no-index\n!pip install faiss-gpu  --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index\n!pip install faiss-cpu  --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_cpu-1.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  --no-index\n!pip install tqdm --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/tqdm-4.64.1-py2.py3-none-any.whl --no-index\n!pip install nvidia-ml-py3 --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/nvidia-ml-py3-7.352.0.tar.gz --no-index\n!pip install accelerate --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/accelerate-0.16.0-py3-none-any.whl --no-index\n!pip install scikit-learn --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index","metadata":{"_uuid":"dca30fc7-d98b-4492-937d-6e669c33a80c","_cell_guid":"7b817b10-f937-4ade-8451-77b2794f30e7","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:00:14.667029Z","iopub.execute_input":"2023-03-02T17:00:14.667576Z","iopub.status.idle":"2023-03-02T17:01:52.822760Z","shell.execute_reply.started":"2023-03-02T17:00:14.667522Z","shell.execute_reply":"2023-03-02T17:01:52.821005Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sentence_transformers import SentenceTransformer, models, InputExample, losses, util\nfrom datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets, IterableDataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nimport torch \nimport transformers\n","metadata":{"_uuid":"a96f2496-7792-46a2-9437-9463e4173f22","_cell_guid":"9e9ce431-839b-474b-bbf2-35c9ea54e8fc","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:01:52.826386Z","iopub.execute_input":"2023-03-02T17:01:52.826973Z","iopub.status.idle":"2023-03-02T17:02:11.543805Z","shell.execute_reply.started":"2023-03-02T17:01:52.826927Z","shell.execute_reply":"2023-03-02T17:02:11.542283Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformers.logging.set_verbosity_debug()\ndatasets.disable_progress_bar()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = SentenceTransformer(\"/kaggle/input/using-fine-tuned-sentencetransformer-env/paraphrase-multilingual-mpnet-base-v2\")","metadata":{"_uuid":"fff33d73-e01b-420f-b4a3-25b2b8ed823c","_cell_guid":"b71d1981-2895-49dd-9ff2-c775a0b9a27d","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:02:11.545779Z","iopub.execute_input":"2023-03-02T17:02:11.547096Z","iopub.status.idle":"2023-03-02T17:02:11.556329Z","shell.execute_reply.started":"2023-03-02T17:02:11.547034Z","shell.execute_reply":"2023-03-02T17:02:11.552363Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls -l","metadata":{"_uuid":"15089f12-d4a0-43d0-a816-d16251653924","_cell_guid":"ce39c189-1a85-442c-a084-1f3fa600b37c","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:02:11.558349Z","iopub.execute_input":"2023-03-02T17:02:11.558795Z","iopub.status.idle":"2023-03-02T17:02:12.691869Z","shell.execute_reply.started":"2023-03-02T17:02:11.558756Z","shell.execute_reply":"2023-03-02T17:02:12.690100Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Refresh_Topic = True\nRefresh_Train_Data = True\nTrain_model = True\nBuild_Embedding = True \nCalculate_Score = True \nCal_Submission = True\nCosine_Cutoff = 0.99995\nNearest_K = 20\ndver = 506\n\n#kaggle setting\ninput_folder = \"/kaggle/input\"\noutput_folder = \"/kaggle/working\"\nmodel_output_folder = \"/kaggle/working\"\nmodel_input_folder = \"/kaggle/input/using-fine-tuned-sentencetransformer-env\"\n\n#local setting\n# input_folder = \"./data/input\"\n# output_folder = \"./data/output\"\n# model_output_folder = \"./model/output\"\n# model_input_folder = \"./model/input\"\n\nTopic_Full_Data_File = f\"{output_folder}/df_topics_full_{dver}.csv\"\nTrain_Data_File= f\"{output_folder}/df_train_v{dver}.csv\"\nEmbeddings_File = f'{output_folder}/embeddings_topics_dataset_v{dver}'\n\nSubmission_File = f\"{output_folder}/submission_v{dver}.csv\"\nBase_Model_File = f\"{model_input_folder}/paraphrase-multilingual-mpnet-base-v2\"\nTuned_Model_File = f\"{model_output_folder}/paraphrase-multilingual-mpnet-base-v{dver}-tuned\"\n\nprint(f\"Topic_Full_Data_File {Topic_Full_Data_File}\")\nprint(f\"Train_Data_File {Train_Data_File}\")\nprint(f\"Embeddings_File {Embeddings_File}\")\nprint(f\"Submission_File {Submission_File}\")","metadata":{"_uuid":"c7dd6180-086b-4d02-b10e-77a88aaa0c5a","_cell_guid":"fdc288b3-d2e8-404f-a107-a5477e45a923","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:02:12.694320Z","iopub.execute_input":"2023-03-02T17:02:12.694821Z","iopub.status.idle":"2023-03-02T17:02:12.705175Z","shell.execute_reply.started":"2023-03-02T17:02:12.694764Z","shell.execute_reply":"2023-03-02T17:02:12.703696Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pd.set_option('display.max_columns', 9)\n# pd.set_option('display.max_rows', 200)\n# pd.set_option('display.min_rows', 10)\n# pd.set_option(\"expand_frame_repr\", True)\n# pd.set_option('display.width', None)\n# pd.set_option('display.max_colwidth', 50)\n# DATA_PATH = \"/kaggle/input/learning-equality-curriculum-recommendations/\"\nDATA_PATH = f\"{input_folder}/learning-equality-curriculum-recommendations/\"\ntopics = pd.read_csv(DATA_PATH + \"topics.csv\")\ncontent = pd.read_csv(DATA_PATH + \"content.csv\")\ncorrelations = pd.read_csv(DATA_PATH + \"correlations.csv\")\n# sample_submission = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n\nprint(f\"DATA_PATH {DATA_PATH}\")","metadata":{"_uuid":"38b48165-b019-459b-9cc3-baade2f0580f","_cell_guid":"6e184bbe-b44f-41ff-b459-7c1321df1874","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:02:12.706961Z","iopub.execute_input":"2023-03-02T17:02:12.707480Z","iopub.status.idle":"2023-03-02T17:02:40.863448Z","shell.execute_reply.started":"2023-03-02T17:02:12.707427Z","shell.execute_reply":"2023-03-02T17:02:40.861630Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_topics = None\n\nif ( not topics.columns[0].startswith(\"topic_\")):\n    print(f\"renaming topics ...\")\n    topics.rename(columns=lambda x: \"topic_\" + x, inplace=True)\n    content.rename(columns=lambda x: \"content_\" + x, inplace=True)","metadata":{"_uuid":"6e5e9242-bc04-4f78-9090-c03f96397ca9","_cell_guid":"70a808d7-5019-4607-8b34-f4b15a9e144c","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:02:40.866503Z","iopub.execute_input":"2023-03-02T17:02:40.867093Z","iopub.status.idle":"2023-03-02T17:02:40.879086Z","shell.execute_reply.started":"2023-03-02T17:02:40.867035Z","shell.execute_reply":"2023-03-02T17:02:40.877229Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_topic_full(row):\n    topic_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_description = str(row[\"topic_description\"]) if pd.notna(row[\"topic_description\"]) else \"\"\n    topic_full = \"title: \" + topic_title\n    if (topic_description != \"\"):\n        topic_full = topic_full + \"\\r\\n\" + \"description: \" + topic_description\n\n    return topic_full\n\ndef get_parents(df, row):\n    topic_id = row[\"topic_id\"]\n    topic_title_full = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_parent = row[\"topic_parent\"]\n    topic_level = row[\"topic_level\"]\n    topic_full = get_topic_full(row)\n    while not pd.isnull(topic_parent):\n        subset = df.loc[df['topic_id'] == topic_parent]\n        for index, r in subset.iterrows():\n            t_full = get_topic_full(r)\n            topic_full = t_full + \"\\r\\n\" + topic_full \n            t_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n            topic_title_full = t_title + \".\" + topic_title_full \n            topic_parent = r[\"topic_parent\"]\n            topic_level = r[\"topic_level\"]\n            break\n\n    return topic_title_full, topic_full\n\n\ndef refresh_topic(topics):\n\n    df_topics = topics\n    print(f\"Before expand topic full ...\")\n    print(df_topics.head(100))\n\n    topic_title_full = []\n    topic_full = []\n\n#     for index, row in tqdm(df_topics.iterrows(), total=df_topics.shape[0]):\n    for index, row in df_topics.iterrows():\n        t_title_full, t_full = get_parents(df_topics, row)\n        topic_title_full.append(t_title_full)\n        topic_full.append(t_full)\n        if (index % 10000 == 0):\n            print(f\"processing df_topics: \\n{index}, {row}\")\n\n    df_topics['topic_title_full'] = topic_title_full\n    df_topics['topic_full'] = topic_full\n\n    df_topics.to_csv(Topic_Full_Data_File) \n    print(f\"Finished processing df_tpocs, and saved to {Topic_Full_Data_File}\")\n    return df_topics\n\nif (Refresh_Topic):\n    print(\"Freshing topic...\")\n    df_topics = refresh_topic(topics)\nelse:\n    print(f\"Load df_topcis from {Topic_Full_Data_File}\")\n    df_topics = pd.read_csv(Topic_Full_Data_File)","metadata":{"_uuid":"aee2af9d-3d31-40bb-8969-6ce2513102d4","_cell_guid":"ea7315ca-963b-42f7-9028-97a213dfaee0","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:02:40.884033Z","iopub.execute_input":"2023-03-02T17:02:40.884556Z","iopub.status.idle":"2023-03-02T17:02:42.447880Z","shell.execute_reply.started":"2023-03-02T17:02:40.884510Z","shell.execute_reply":"2023-03-02T17:02:42.446332Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"After expand topic full ...\")\nprint(df_topics.head(100))\nprint(f\"df_topics value counet for each columns: \\n{df_topics.nunique()}\")","metadata":{"_uuid":"60b7de1c-8439-4116-9fff-2a3a82f14ac3","_cell_guid":"1368cb4c-2c40-47c5-ad4e-f07657f5c4dd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-02T17:02:42.449690Z","iopub.execute_input":"2023-03-02T17:02:42.450231Z","iopub.status.idle":"2023-03-02T17:02:42.898924Z","shell.execute_reply.started":"2023-03-02T17:02:42.450155Z","shell.execute_reply":"2023-03-02T17:02:42.897461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load train data by combining correlation table with topic and content tables.","metadata":{"_uuid":"5c391db5-7a6f-4796-b498-908d0018ada8","_cell_guid":"88eeb1e1-990c-41ba-a419-1f45f5f1386d","trusted":true}},{"cell_type":"code","source":"def load_train_data(topics):\n    train_df_columns = [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_full\", \"topic_id\",\"content_id\", \"content_description\", \"content_text\" ]\n    \n    correlations[\"content_id\"] = correlations[\"content_ids\"].str.split(\" \")\n    corr = correlations.explode(\"content_id\").drop(columns=[\"content_ids\"])\n\n    corr = corr.merge(df_topics, how=\"left\", on=\"topic_id\")\n    corr = corr.merge(content, how=\"left\", on=\"content_id\")\n\n    train_df = pd.DataFrame(corr[train_df_columns])\n    cols = ['content_title', 'content_description', \"content_text\"]\n    train_df['content_full'] = train_df[cols].apply(lambda row: '\\r\\n'.join(row.values.astype(str)), axis=1)\n    \n    final_train_data = pd.DataFrame(train_df)\n    \n    print(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\n\n    final_train_data.to_csv(Train_Data_File)\n    \n    return final_train_data","metadata":{"_uuid":"cefd47c3-7c15-4a2d-888c-b2fea75b7148","_cell_guid":"4d4d6601-663d-4a41-97e7-1b54eb15cbb7","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:02:42.901860Z","iopub.execute_input":"2023-03-02T17:02:42.902452Z","iopub.status.idle":"2023-03-02T17:02:42.911911Z","shell.execute_reply.started":"2023-03-02T17:02:42.902395Z","shell.execute_reply":"2023-03-02T17:02:42.910511Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Refresh_Train_Data:\n    print(f\"Refresh_Train_Data ==>>>\")\n    final_train_data = load_train_data(topics)\nelse:\n    print(f\"load final_train_data from {Train_Data_File}\")\n    final_train_data = pd.read_csv(Train_Data_File)","metadata":{"_uuid":"4982e343-77be-4132-aef6-5c09e4675f13","_cell_guid":"3a2c3d2e-81c1-40c6-8ede-bacaa784b1f4","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:02:42.913570Z","iopub.execute_input":"2023-03-02T17:02:42.914321Z","iopub.status.idle":"2023-03-02T17:03:26.456908Z","shell.execute_reply.started":"2023-03-02T17:02:42.914255Z","shell.execute_reply":"2023-03-02T17:03:26.455419Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File)\nprint(dataset_dict)\n\ndataset = dataset_dict[\"train\"]\n\ndataset = dataset.train_test_split(test_size=0.2)\n\ntrain_dataset = dataset[\"train\"]\n\nkeeped_columns = [\"topic_full\", \"content_full\"]\ncolumns = train_dataset.column_names\ncolumns_to_keep = [\"topic_full\", \"content_full\"]\ncolumns_to_remove = set(columns_to_keep).symmetric_difference(columns)\ntrain_dataset = train_dataset.remove_columns(columns_to_remove)\n\n# train_dataset = train_dataset.shuffle(seed=42).select(range(10000))\n\ntrain_examples = []\neval_examples = []\n\nn_examples = train_dataset.num_rows\n\nfor dt in train_dataset:\n    # print([dt[\"topic_full\"], dt[\"content_full\"]])\n    # train_examples.append(InputExample(texts=[str(dt[\"topic_full\"]), str(dt[\"content_full\"])]))\n    first = str(dt[\"topic_full\"]) if dt[\"topic_full\"] != \"\" else \"\" \n    second = str(dt[\"content_full\"]) if str(dt[\"content_full\"]) != \"\" else \"\" \n    if (first != \"\" and second != \"\"):\n        train_examples.append(InputExample(texts=[first, second]))\n\nprint(f\"train_examples: {len(train_examples)}\")","metadata":{"_uuid":"52aef0f0-820d-45e0-9e8d-d6e0a3eeb592","_cell_guid":"c4c5fe72-bb4e-4d2d-b7c5-bedac3c03e72","collapsed":false,"execution":{"iopub.status.busy":"2023-03-02T17:21:21.479771Z","iopub.execute_input":"2023-03-02T17:21:21.480582Z","iopub.status.idle":"2023-03-02T17:23:08.822988Z","shell.execute_reply.started":"2023-03-02T17:21:21.480507Z","shell.execute_reply":"2023-03-02T17:23:08.821477Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model():\n    \n    # model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n    print(f\"load base model from {Base_Model_File}\")\n    model = SentenceTransformer(Base_Model_File)\n    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n    # num_epochs = 5\n    num_epochs = 1\n    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n    print(f\"model  to {device}\")\n    model.to(device)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    # training_args = TrainingArguments(\n    #     disable_tqdm=True,\n    #     output_dir='./checkpoints',\n    #     save_total_limit=10,\n    #     logging_dir='/content/logs',\n    #     num_train_epochs=num_epochs,\n    #     evaluation_strategy='epoch'\n    #     save_strategy='steps',\n    #     save_steps=30,\n    #     logging_steps=10,\n    #     overwrite_output_dir=True,\n    #     per_device_train_batch_size=4,\n    #     per_device_eval_batch_size=4,\n    #     gradient_accumulation_steps=4,\n    #     eval_accumulation_steps=4,\n    #     gradient_checkpointing=True,\n    #     max_grad_norm=0.5,\n    #     lr_scheduler_type=\"cosine\",\n    #     learning_rate=1e-4,\n    #     warmup_ratio=0.05,\n    #     weight_decay=0.1,\n    #     fp16_full_eval=True\n    #     fp16=True,\n    #     fp16_opt_level='O1'\n    # )\n    print(f\"start model fine tune\")\n    model.fit(train_objectives=[(train_dataloader, train_loss)],\n            epochs=num_epochs,\n            warmup_steps=warmup_steps)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.save(Tuned_Model_File)\n    print(f\"model saved to {Tuned_Model_File}\")","metadata":{"_uuid":"42a29682-182f-460d-8cee-0afa8335dd4e","_cell_guid":"46d78bd1-1302-4010-9781-448eb1386cd3","collapsed":false,"execution":{"iopub.status.busy":"2023-03-01T19:48:58.982352Z","iopub.status.idle":"2023-03-01T19:48:58.983164Z","shell.execute_reply.started":"2023-03-01T19:48:58.982905Z","shell.execute_reply":"2023-03-01T19:48:58.982931Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Train_model:\n    train_model()","metadata":{"_uuid":"ba4b3edb-54a6-46cd-9a27-f488215a4c09","_cell_guid":"acb5ad1f-fd16-4e88-9637-d755e7833343","collapsed":false,"execution":{"iopub.status.busy":"2023-03-01T19:48:58.984978Z","iopub.status.idle":"2023-03-01T19:48:58.985484Z","shell.execute_reply.started":"2023-03-01T19:48:58.985229Z","shell.execute_reply":"2023-03-01T19:48:58.985254Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(Model_File)\ntrained_model = AutoModel.from_pretrained(Model_File)\n\ntrained_model.to(device)","metadata":{"_uuid":"5232f1b9-27de-4e58-b966-21023f386dfd","_cell_guid":"9fa707d6-f837-4c29-b490-40febefc5d6c","collapsed":false,"execution":{"iopub.status.busy":"2023-03-01T19:48:58.987850Z","iopub.status.idle":"2023-03-01T19:48:58.988824Z","shell.execute_reply.started":"2023-03-01T19:48:58.988537Z","shell.execute_reply":"2023-03-01T19:48:58.988570Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cls_pooling(model_output):\n    return model_output.last_hidden_state[:, 0]\n\ndef get_embeddings(text_list):\n    encoded_input = tokenizer(\n        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    model_output = trained_model(**encoded_input)\n    return cls_pooling(model_output)","metadata":{"_uuid":"e1724244-971c-44bc-b453-45e84090f112","_cell_guid":"d360b8e2-8f3e-4f85-bc52-2efacd8ff2c2","collapsed":false,"execution":{"iopub.status.busy":"2023-03-01T19:48:58.990516Z","iopub.status.idle":"2023-03-01T19:48:58.991046Z","shell.execute_reply.started":"2023-03-01T19:48:58.990785Z","shell.execute_reply":"2023-03-01T19:48:58.990822Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build embedding based on topic_full and content_full, we also need keep topic_id and content_id.\n\ndef embeddings_gen(row):\n    embedding_row = {}\n    embedding_row[\"embeddings\"]=get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0]\n    embedding_row[\"content_id\"] = str(x[\"content_id\"])\n    embedding_ds_raw.append(embedding_row)\n\n\n\ndef build_embedding():\n\n    # def add_embeddings_topic_full(x):\n    #     y = {}\n    #     y[\"content_id\"] = x[\"content_id\"]\n    #     y[\"embeddings\"]=add_embeddings_topic_full( str(x[\"topic_full\"])).detach().cpu().numpy()[0]\n    #     embedding_list.append\n    #     return x\n\n    # def add_embeddings_content_full(x):\n    #     y = {}\n    #     y[\"content_id\"] = x[\"content_id\"]\n    #     y[\"embeddings\"]=add_embeddings_topic_full( str(x[\"content_full\"])).detach().cpu().numpy()[0]\n    #     embedding_list.append\n    #     return x\n\n    dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File,streaming=True)\n    print(dataset_dict)\n    dataset = dataset_dict[\"train\"]\n\n    def generator(ds):\n        for x in ds:\n            y = {}\n            y[\"content_id\"] = x[\"content_id\"]\n            y[\"embeddings\"]=get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0]\n            yield y\n            z = {}\n            z[\"content_id\"] = x[\"content_id\"]\n            z[\"embeddings\"]=get_embeddings( str(x[\"content_full\"])).detach().cpu().numpy()[0]\n            yield z \n\n    embedding_dataset = Dataset.from_generator(generator, gen_kwargs={\"ds\":dataset})\n    embedding_list = {}\n\n\n\n\n    # print(f\"building embeddings for topic_full\")\n    # dataset.map(add_embeddings_topic_full)\n\n    # print(f\"building embeddings for content_full\")\n    # dataset.map(add_embeddings_topic_full)\n\n    # embedding_dataset =  concatenate_datasets([embeddings_topics_dataset, embeddings_content_dataset])\n    print(f\"embedding_dataset => {embedding_dataset}\")\n    embedding_dataset.save_to_disk(Embeddings_File)","metadata":{"_uuid":"d04f6daf-caf0-4176-ac3e-3280afb8474a","_cell_guid":"294dc09e-4ff2-48ea-9483-3ed6230aff2c","collapsed":false,"execution":{"iopub.status.busy":"2023-03-01T19:48:58.992897Z","iopub.status.idle":"2023-03-01T19:48:58.993416Z","shell.execute_reply.started":"2023-03-01T19:48:58.993140Z","shell.execute_reply":"2023-03-01T19:48:58.993167Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Build_Embedding:\n    print(f\"Build Embedding .... from {Train_Data_File}\")\n    build_embedding()\n\nembedding_dataset = load_from_disk(Embeddings_File)\n\nembedding_dataset.add_faiss_index(column=\"embeddings\")\nprint(f\"finishing loading embedding_dataset\")\nprint(embedding_dataset)","metadata":{"_uuid":"46242819-74f7-4fa5-a5c0-83443244f1f6","_cell_guid":"cfad7417-07fd-43b3-b2ed-34790b2538b5","collapsed":false,"execution":{"iopub.status.busy":"2023-03-01T19:48:58.995195Z","iopub.status.idle":"2023-03-01T19:48:58.995705Z","shell.execute_reply.started":"2023-03-01T19:48:58.995453Z","shell.execute_reply":"2023-03-01T19:48:58.995478Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cosine_sim(ebd, text_embedding):\n    cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    return cosine_score.item()\n\ndef get_score_topic(text, cosine_cutoff):\n    # There are only two filed in embedding data set: \"embedding\", \"content_id\"\n    text_embedding = get_embeddings(text).cpu().detach().numpy()\n    scores, samples = embedding_dataset.get_nearest_examples(\n                                                    \"embeddings\", text_embedding, k=Nearest_K\n                                                    )\n    # print(scores)\n    samples_df = pd.DataFrame.from_dict(samples)\n    samples_df[\"scores\"] = scores\n    samples_df.sort_values(\"scores\", ascending=True, inplace=True)\n    \n    samples_df['cosine_sim'] = samples_df.apply(lambda row: get_cosine_sim(row[\"embeddings\"], text_embedding), axis=1)\n    samples_df = samples_df[samples_df['cosine_sim'] >= cosine_cutoff] \n\n    scorelist = samples_df[\"scores\"].values.tolist()\n    cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    result_df = samples_df\n\n    return result_df\n\ndef get_score_topic_json(text, cosine_cutoff):\n\n    res_df = get_score_topic(text, cosine_cutoff)\n    res_dict = res_df.to_dict('records')\n    return res_dict","metadata":{"_uuid":"8a7520b5-1380-4b61-9bc5-92d600a69a75","_cell_guid":"d0655599-fa39-4706-98fd-aec009fc729a","collapsed":false,"execution":{"iopub.status.busy":"2023-03-01T19:48:58.997469Z","iopub.status.idle":"2023-03-01T19:48:58.997984Z","shell.execute_reply.started":"2023-03-01T19:48:58.997718Z","shell.execute_reply":"2023-03-01T19:48:58.997745Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_row(row):\n    res_dict = get_score_topic_json(str(row[\"topic_full\"]), Cosine_Cutoff) \n    # print(f\"calculate_row dict\")\n    # print(res_dict)\n    #scores, topic_ids, content_ids, cosine_sim\n    # result.append([scores, topic_id, topic_title, **evalrow ])\n    row[\"scores\"] = \" \".join([f\"{d['scores']:.10f}\" for d in res_dict])\n    row[\"content_ids\"] = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    row[\"cosine_sim\"] = \" \".join([f\"{d['cosine_sim']:.10f}\" for d in res_dict])\n    return row\n\ndef calculate_score(eval_sampled, res_file):\n    num_rows = eval_sampled.num_rows\n    scores_column = [-2] * num_rows\n    content_ids_column = [\"\"] * num_rows\n    cosine_column = [-1.0] * num_rows\n    eval_sampled = eval_sampled.add_column(\"scores\", scores_column)\n    eval_sampled = eval_sampled.add_column(\"content_ids\", content_ids_column)\n    eval_sampled = eval_sampled.add_column(\"cosine_sim\", cosine_column)\n\n    all_columns = set(eval_sampled.column_names)\n    # keeped_columns = [\"topic_id\",\"topic_full\", \"content_full\",\"content_id\"]\n    keeped_columns = [\"topic_id\",\"scores\", \"cosine_sim\", \"content_ids\"]\n    columns_to_removed = list(all_columns.symmetric_difference(keeped_columns))\n\n    print(f\"calculating scores mapping ...\")\n    eval_sampled = eval_sampled.map(calculate_row)\n    print(f\"result_df value counet for each columns: \\n{eval_sampled.shape}\")\n    eval_sampled.to_csv(res_file) \n    \n\n# Result_file = f\"./data/result_v{dver}.csv\"\n# if Calculate_Score:\n#     eval_sampled =  dataset[\"test\"].shuffle(seed=42).select(range(2000))\n#     calculate_score(eval_sampled, Result_file)\n#     Result_file = f\"./data/result_train_v{dver}.csv\"\n#     eval_sampled =  dataset[\"train\"].shuffle(seed=42).select(range(2000))\n#     calculate_score(eval_sampled, Result_file)\n\n# calc_df = pd.read_csv(Result_file) \n# print(f\"load result file {Result_file}\")\n# print(calc_df.head(100))","metadata":{"_uuid":"8a27c5ca-289e-427f-8d61-64f9d0812036","_cell_guid":"9733969d-3af9-42e8-b253-64c93178147b","collapsed":false,"execution":{"iopub.status.busy":"2023-03-01T19:48:59.000696Z","iopub.status.idle":"2023-03-01T19:48:59.001564Z","shell.execute_reply.started":"2023-03-01T19:48:59.001303Z","shell.execute_reply":"2023-03-01T19:48:59.001330Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(topic_id, df_topics):\n    subset = df_topics.loc[df_topics['topic_id'] == topic_id]\n    text = None\n    for index, r in subset.iterrows():\n            text = r[\"topic_full\"]\n            break\n    if text is None:\n        return \"\"\n\n    res_dict = get_score_topic_json(text, Cosine_Cutoff) \n\n    content_ids = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    return content_ids\n\n\ndef calculate_submission(submission_df):\n\n    submission_df['content_ids'] = submission_df.apply(lambda row: get_neighbors(row[\"topic_id\"], df_topics), axis=1)\n    submission_df.to_csv(DATA_PATH + \"submission.csv\") \n    submission_df.to_csv(Submission_File) \n    print(f\"display submission head\")\n    print(submission_df.head(100))","metadata":{"_uuid":"1879d11c-faab-4f3e-a618-29629249047b","_cell_guid":"619707bb-d7d7-4588-850a-019254c81222","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-01T19:48:59.003102Z","iopub.status.idle":"2023-03-01T19:48:59.004005Z","shell.execute_reply.started":"2023-03-01T19:48:59.003731Z","shell.execute_reply":"2023-03-01T19:48:59.003758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (Cal_Submission):\n    print(f\"Calculating submission \")\n    submission_df = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n    calculate_submission(submission_df)\n\n# from sklearn.metrics import fbeta_score\n# fbeta_score(y_true, y_pred, average='macro', beta=2)","metadata":{"_uuid":"13fe4e94-95e3-43b3-9c08-bb2d23bc0b62","_cell_guid":"b3f2d15a-0313-4c53-8b6c-fceb07a1cabd","collapsed":false,"execution":{"iopub.status.busy":"2023-03-01T19:48:59.005520Z","iopub.status.idle":"2023-03-01T19:48:59.006413Z","shell.execute_reply.started":"2023-03-01T19:48:59.006117Z","shell.execute_reply":"2023-03-01T19:48:59.006147Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}