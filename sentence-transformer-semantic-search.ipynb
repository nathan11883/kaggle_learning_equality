{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**The reference:**\n* https://huggingface.co/blog/how-to-train-sentence-transformers\n* https://www.kaggle.com/code/andtaichi/finetunig-sentencetransformer\n* https://www.kaggle.com/code/quincyqiang/download-huggingface-pretrain-for-kaggle/notebook\n* https://towardsdatascience.com/easy-kaggle-offline-submission-with-chaining-kernels-30bba5ea5c4d\n* https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/109679\n* https://www.kaggle.com/code/jamiealexandre/sample-notebook-data-exploration/notebook\n\nThe pretrained model we use:\nhttps://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n\n**The highlights:**\n1. Join the correlation with topic and content.\n2. Concatenate \"topic_title\" and \"topic_description\" for each topic, and concate all the ancestor topics into \"topic_full\" field.\n3. Concatenate 'content_title', 'content_description', \"content_text\" for each content into \"content_full\" field.\n4. Feed \"topic_full\" and \"content_full\" as embeding pair into the model and fintune the model.\n5. Using the fine-tuned model, generate the embeding for \"topic_full\" and \"content_full\", and put them into embeding dataset.\n6. Create the faiss index by calling add_faiss_index.\n7. For each topic that need to be predict, search in the embeding dataset with faiss index using content_full, find the nearest K content (current value is 20), then filter out the result.\n8. Furhter filter out the result by calculating the cosine_sim between each content and the topic_full, and cutting off using Cosine_Cutoff (current value is larger than 0.99995).\n9. Output the result.","metadata":{"_uuid":"865f67da-dccf-460d-9e03-0155442598b0","_cell_guid":"82ed1c7a-87d3-47cb-b5b7-76690be53d49","trusted":true}},{"cell_type":"code","source":"# !pip install sentence-transformers\n# !pip install faiss-gpu\n# !pip install faiss-cpu\n# !pip install tqdm\n# !pip install nvidia-ml-py3\n# !pip install accelerate\n# !pip install scikit-learn","metadata":{"_uuid":"5529cfb0-71e2-4e9a-a4bc-2c94a2930304","_cell_guid":"769ab21a-4c24-4274-b4aa-ea93132ccde0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:42:12.853745Z","iopub.execute_input":"2023-03-04T19:42:12.854598Z","iopub.status.idle":"2023-03-04T19:42:12.858932Z","shell.execute_reply.started":"2023-03-04T19:42:12.854557Z","shell.execute_reply":"2023-03-04T19:42:12.857808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh |  bash\n# !apt-get install -y --allow-unauthenticated git-lfs","metadata":{"_uuid":"a74145e7-4fd9-4f4e-98cc-b74c5b5160d9","_cell_guid":"326eb1d5-7005-4d98-9783-454c5bb9b875","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:42:12.865677Z","iopub.execute_input":"2023-03-04T19:42:12.866647Z","iopub.status.idle":"2023-03-04T19:42:12.876983Z","shell.execute_reply.started":"2023-03-04T19:42:12.866607Z","shell.execute_reply":"2023-03-04T19:42:12.875868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git lfs install\n# !git clone https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n# # if you want to clone without large files â€“ just their pointers\n# # prepend your git clone with the following env var:\n# !GIT_LFS_SKIP_SMUDGE=1","metadata":{"_uuid":"ba248c22-545d-449b-a41b-b69f86838cbb","_cell_guid":"881bc2ee-cd81-41c0-b881-2534eaf16b99","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:42:12.879641Z","iopub.execute_input":"2023-03-04T19:42:12.880552Z","iopub.status.idle":"2023-03-04T19:42:12.887489Z","shell.execute_reply.started":"2023-03-04T19:42:12.880513Z","shell.execute_reply":"2023-03-04T19:42:12.886280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install transformers --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/transformers-4.26.1-py3-none-any.whl --no-index\n# !pip install datasets --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/datasets-2.10.1-py3-none-any.whl --no-index\n!pip install sentence-transformers --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/sentence-transformers-2.2.2.tar.gz --no-index\n!pip install faiss-gpu  --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index\n!pip install faiss-cpu  --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_cpu-1.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  --no-index\n!pip install tqdm --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/tqdm-4.64.1-py2.py3-none-any.whl --no-index\n!pip install nvidia-ml-py3 --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/nvidia-ml-py3-7.352.0.tar.gz --no-index\n!pip install accelerate --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/accelerate-0.16.0-py3-none-any.whl --no-index\n!pip install scikit-learn --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index","metadata":{"_uuid":"099dab42-0ce1-4e66-a49d-0266306effd8","_cell_guid":"ea7ee182-63c7-4bb3-8876-de1f655a6d43","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:42:12.889251Z","iopub.execute_input":"2023-03-04T19:42:12.890045Z","iopub.status.idle":"2023-03-04T19:43:29.529265Z","shell.execute_reply.started":"2023-03-04T19:42:12.890008Z","shell.execute_reply":"2023-03-04T19:43:29.528016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport torch \nimport transformers\nimport datasets\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nfrom sentence_transformers import SentenceTransformer, models, InputExample, losses, util\nfrom datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets, IterableDataset\nfrom tqdm.auto import tqdm","metadata":{"_uuid":"3dc1a904-ca6a-402c-bd52-bf5578b72db2","_cell_guid":"93aa12d6-701b-4a81-82c5-12178e3d3447","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:43:29.532970Z","iopub.execute_input":"2023-03-04T19:43:29.533380Z","iopub.status.idle":"2023-03-04T19:43:30.299013Z","shell.execute_reply.started":"2023-03-04T19:43:29.533345Z","shell.execute_reply":"2023-03-04T19:43:30.297997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"transformers.__version__: {transformers.__version__}\")\nprint(f\"datasets.__version__: {datasets.__version__}\")","metadata":{"_uuid":"a31e2121-63f4-43d8-86a4-fcf16d3e9eb5","_cell_guid":"f5a57535-51e3-4835-a9dd-0c849515bb37","collapsed":false,"execution":{"iopub.status.busy":"2023-03-04T19:43:30.300505Z","iopub.execute_input":"2023-03-04T19:43:30.300853Z","iopub.status.idle":"2023-03-04T19:43:30.308529Z","shell.execute_reply.started":"2023-03-04T19:43:30.300816Z","shell.execute_reply":"2023-03-04T19:43:30.306959Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformers.logging.set_verbosity_debug()\ndatasets.disable_progress_bar()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"_uuid":"6cc79316-8439-4d92-a500-f58c01fe2351","_cell_guid":"a05dddee-c5bb-4bdc-bb45-e36a08878a4c","collapsed":false,"execution":{"iopub.status.busy":"2023-03-04T19:43:30.310045Z","iopub.execute_input":"2023-03-04T19:43:30.310848Z","iopub.status.idle":"2023-03-04T19:43:30.324712Z","shell.execute_reply.started":"2023-03-04T19:43:30.310755Z","shell.execute_reply":"2023-03-04T19:43:30.323578Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = SentenceTransformer(\"/kaggle/input/using-fine-tuned-sentencetransformer-env/paraphrase-multilingual-mpnet-base-v2\")","metadata":{"_uuid":"2c46ee23-a986-4745-b70f-435811234db2","_cell_guid":"8b200995-79a5-4620-951e-d5cc9034f356","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:43:30.326164Z","iopub.execute_input":"2023-03-04T19:43:30.326550Z","iopub.status.idle":"2023-03-04T19:43:30.335999Z","shell.execute_reply.started":"2023-03-04T19:43:30.326509Z","shell.execute_reply":"2023-03-04T19:43:30.334982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls -l","metadata":{"_uuid":"892f7778-611a-44e9-bffe-cd973418b651","_cell_guid":"285bb01b-4d9b-4037-b7e7-cc7d8c2d5b94","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:43:30.337507Z","iopub.execute_input":"2023-03-04T19:43:30.337912Z","iopub.status.idle":"2023-03-04T19:43:30.345659Z","shell.execute_reply.started":"2023-03-04T19:43:30.337837Z","shell.execute_reply":"2023-03-04T19:43:30.344607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Refresh_Topic = True\nRefresh_Train_Data = True\nTrain_model = True\nBuild_Embedding = True \nCalculate_Score = True \nCal_Submission = True\nCosine_Cutoff = 0.99995\nNearest_K = 20\ndver = 508\n\n#kaggle setting\ninput_folder = \"/kaggle/input\"\noutput_folder = \"/kaggle/working\"\nmodel_output_folder = \"/kaggle/working\"\nmodel_input_folder = \"/kaggle/input/using-fine-tuned-sentencetransformer-env\"\n\n#local setting\n# input_folder = \"./data/input\"\n# output_folder = \"./data/output\"\n# model_output_folder = \"./model/output\"\n# model_input_folder = \"./model/input\"\n\nTopic_Full_Data_File = f\"{output_folder}/df_topics_full_{dver}.csv\"\nTrain_Data_File= f\"{output_folder}/df_train_v{dver}.csv\"\nEmbeddings_File = f'{output_folder}/embeddings_topics_dataset_v{dver}'\n\nSubmission_File = f\"{output_folder}/submission_v{dver}.csv\"\nBase_Model_File = f\"{model_input_folder}/paraphrase-multilingual-mpnet-base-v2\"\nTuned_Model_File = f\"{model_output_folder}/paraphrase-multilingual-mpnet-base-v{dver}-tuned\"\n\nprint(f\"Topic_Full_Data_File {Topic_Full_Data_File}\")\nprint(f\"Train_Data_File {Train_Data_File}\")\nprint(f\"Embeddings_File {Embeddings_File}\")\nprint(f\"Submission_File {Submission_File}\")","metadata":{"_uuid":"e336a710-6d1a-4afe-96dd-c0e9d4daa73b","_cell_guid":"9dee6e01-90b2-4edb-8e3f-428ffcb47d36","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:43:30.350064Z","iopub.execute_input":"2023-03-04T19:43:30.350367Z","iopub.status.idle":"2023-03-04T19:43:30.360408Z","shell.execute_reply.started":"2023-03-04T19:43:30.350340Z","shell.execute_reply":"2023-03-04T19:43:30.359136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pd.set_option('display.max_columns', 9)\n# pd.set_option('display.max_rows', 200)\n# pd.set_option('display.min_rows', 10)\n# pd.set_option(\"expand_frame_repr\", True)\n# pd.set_option('display.width', None)\n# pd.set_option('display.max_colwidth', 50)\n# DATA_PATH = \"/kaggle/input/learning-equality-curriculum-recommendations/\"\nDATA_PATH = f\"{input_folder}/learning-equality-curriculum-recommendations/\"\ntopics = pd.read_csv(DATA_PATH + \"topics.csv\")\ncontent = pd.read_csv(DATA_PATH + \"content.csv\")\ncorrelations = pd.read_csv(DATA_PATH + \"correlations.csv\")\n# sample_submission = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n\nprint(f\"DATA_PATH {DATA_PATH}\")","metadata":{"_uuid":"f66a8e55-faca-4359-a794-04128bc5e1fd","_cell_guid":"539b7e4c-b762-46d7-a73f-8318865d7772","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:43:30.362393Z","iopub.execute_input":"2023-03-04T19:43:30.362785Z","iopub.status.idle":"2023-03-04T19:43:50.440134Z","shell.execute_reply.started":"2023-03-04T19:43:30.362748Z","shell.execute_reply":"2023-03-04T19:43:50.438860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_topics = None\n\nif ( not topics.columns[0].startswith(\"topic_\")):\n    print(f\"renaming topics ...\")\n    topics.rename(columns=lambda x: \"topic_\" + x, inplace=True)\n    content.rename(columns=lambda x: \"content_\" + x, inplace=True)","metadata":{"_uuid":"e6157eb5-f552-48eb-add6-c2fe34217f4e","_cell_guid":"e344fa3a-0635-471e-b9ac-94c32607c274","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:43:50.441815Z","iopub.execute_input":"2023-03-04T19:43:50.442301Z","iopub.status.idle":"2023-03-04T19:43:50.451939Z","shell.execute_reply.started":"2023-03-04T19:43:50.442259Z","shell.execute_reply":"2023-03-04T19:43:50.450879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_topic_full(row):\n    topic_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_description = str(row[\"topic_description\"]) if pd.notna(row[\"topic_description\"]) else \"\"\n    topic_full = \"title: \" + topic_title\n    if (topic_description != \"\"):\n        topic_full = topic_full + \"\\r\\n\" + \"description: \" + topic_description\n\n    return topic_full\n\ndef get_parents(df, row):\n    topic_id = row[\"topic_id\"]\n    topic_title_full = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_parent = row[\"topic_parent\"]\n    topic_level = row[\"topic_level\"]\n    topic_full = get_topic_full(row)\n    while not pd.isnull(topic_parent):\n        subset = df.loc[df['topic_id'] == topic_parent]\n        for index, r in subset.iterrows():\n            t_full = get_topic_full(r)\n            topic_full = t_full + \"\\r\\n\" + topic_full \n            t_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n            topic_title_full = t_title + \".\" + topic_title_full \n            topic_parent = r[\"topic_parent\"]\n            topic_level = r[\"topic_level\"]\n            break\n\n    return topic_title_full, topic_full\n\n\ndef refresh_topic(topics):\n\n    df_topics = topics\n    print(f\"Before expand topic full ...\")\n    print(df_topics.head(100))\n\n    topic_title_full = []\n    topic_full = []\n\n#     for index, row in tqdm(df_topics.iterrows(), total=df_topics.shape[0]):\n    for index, row in df_topics.iterrows():\n        t_title_full, t_full = get_parents(df_topics, row)\n        topic_title_full.append(t_title_full)\n        topic_full.append(t_full)\n        if (index % 10000 == 0):\n            print(f\"processing df_topics: \\n{index}, {row}\")\n\n    df_topics['topic_title_full'] = topic_title_full\n    df_topics['topic_full'] = topic_full\n\n    df_topics.to_csv(Topic_Full_Data_File) \n    print(f\"Finished processing df_tpocs, and saved to {Topic_Full_Data_File}\")\n    return df_topics\n\nif (Refresh_Topic):\n    print(\"Freshing topic...\")\n    df_topics = refresh_topic(topics)\nelse:\n    print(f\"Load df_topcis from {Topic_Full_Data_File}\")\n    df_topics = pd.read_csv(Topic_Full_Data_File)","metadata":{"_uuid":"6d70e282-641f-429f-96d1-9b9b22e7a80f","_cell_guid":"a4898e49-ec37-49a8-99ac-4672d5eb887f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:43:50.453877Z","iopub.execute_input":"2023-03-04T19:43:50.454679Z","iopub.status.idle":"2023-03-04T19:43:51.655027Z","shell.execute_reply.started":"2023-03-04T19:43:50.454640Z","shell.execute_reply":"2023-03-04T19:43:51.653967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"After expand topic full ...\")\nprint(df_topics.head(100))\nprint(f\"df_topics value counet for each columns: \\n{df_topics.nunique()}\")","metadata":{"_uuid":"bac9caf6-76c9-49f8-a7ce-0a344ba27a8f","_cell_guid":"b385d662-b7d0-440d-baf3-7a5ad1b857d3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:43:51.656675Z","iopub.execute_input":"2023-03-04T19:43:51.657048Z","iopub.status.idle":"2023-03-04T19:43:51.984369Z","shell.execute_reply.started":"2023-03-04T19:43:51.657003Z","shell.execute_reply":"2023-03-04T19:43:51.983229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load train data by combining correlation table with topic and content tables.","metadata":{"_uuid":"afefc509-e90c-42ab-aae2-d9207e57e720","_cell_guid":"ed6fba8c-c858-4e79-8d23-9b690e41408a","trusted":true}},{"cell_type":"code","source":"def load_train_data(topics):\n    train_df_columns = [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_full\", \"topic_id\",\"content_id\", \"content_description\", \"content_text\" ]\n    \n    correlations[\"content_id\"] = correlations[\"content_ids\"].str.split(\" \")\n    corr = correlations.explode(\"content_id\").drop(columns=[\"content_ids\"])\n\n    corr = corr.merge(df_topics, how=\"left\", on=\"topic_id\")\n    corr = corr.merge(content, how=\"left\", on=\"content_id\")\n\n    train_df = pd.DataFrame(corr[train_df_columns])\n    cols = ['content_title', 'content_description', \"content_text\"]\n    train_df['content_full'] = train_df[cols].apply(lambda row: '\\r\\n'.join(row.values.astype(str)), axis=1)\n    \n    final_train_data = pd.DataFrame(train_df)\n    \n    print(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\n\n    final_train_data.to_csv(Train_Data_File)\n    \n    return final_train_data","metadata":{"_uuid":"28a905bd-d1cb-4656-ae57-93c35f0b1b30","_cell_guid":"a3d6e1b1-1378-4dd4-a376-3be7adc41ab9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:43:51.986004Z","iopub.execute_input":"2023-03-04T19:43:51.986376Z","iopub.status.idle":"2023-03-04T19:43:51.994710Z","shell.execute_reply.started":"2023-03-04T19:43:51.986338Z","shell.execute_reply":"2023-03-04T19:43:51.993488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Refresh_Train_Data:\n    print(f\"Refresh_Train_Data ==>>>\")\n    final_train_data = load_train_data(topics)\nelse:\n    print(f\"load final_train_data from {Train_Data_File}\")\n    final_train_data = pd.read_csv(Train_Data_File)","metadata":{"_uuid":"595ec42f-6492-4272-9053-49e705e39f16","_cell_guid":"5088b01a-212b-48aa-9e16-fa72097d8e92","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:43:51.996286Z","iopub.execute_input":"2023-03-04T19:43:51.996573Z","iopub.status.idle":"2023-03-04T19:44:27.171587Z","shell.execute_reply.started":"2023-03-04T19:43:51.996548Z","shell.execute_reply":"2023-03-04T19:44:27.170492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File)\nprint(dataset_dict)\n\ndataset = dataset_dict[\"train\"]\n\ndataset = dataset.train_test_split(test_size=0.2)\n\ntrain_dataset = dataset[\"train\"]\n\nkeeped_columns = [\"topic_full\", \"content_full\"]\ncolumns = train_dataset.column_names\ncolumns_to_keep = [\"topic_full\", \"content_full\"]\ncolumns_to_remove = set(columns_to_keep).symmetric_difference(columns)\ntrain_dataset = train_dataset.remove_columns(columns_to_remove)\n\n# train_dataset = train_dataset.shuffle(seed=42).select(range(10000))\n\ntrain_examples = []\neval_examples = []\n\nn_examples = train_dataset.num_rows\n\nfor dt in train_dataset:\n    # print([dt[\"topic_full\"], dt[\"content_full\"]])\n    # train_examples.append(InputExample(texts=[str(dt[\"topic_full\"]), str(dt[\"content_full\"])]))\n    first = str(dt[\"topic_full\"]) if dt[\"topic_full\"] != \"\" else \"\" \n    second = str(dt[\"content_full\"]) if str(dt[\"content_full\"]) != \"\" else \"\" \n    if (first != \"\" and second != \"\"):\n        train_examples.append(InputExample(texts=[first, second]))\n\nprint(f\"train_examples: {len(train_examples)}\")","metadata":{"_uuid":"5f691e0c-98a5-49bc-a1ad-c441ad48e81b","_cell_guid":"a5142266-2647-48cf-b15e-3cce97a07de5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:44:27.175014Z","iopub.execute_input":"2023-03-04T19:44:27.175778Z","iopub.status.idle":"2023-03-04T19:46:13.708106Z","shell.execute_reply.started":"2023-03-04T19:44:27.175732Z","shell.execute_reply":"2023-03-04T19:46:13.706934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model():\n    \n    # model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n    print(f\"load base model from {Base_Model_File}\")\n    model = SentenceTransformer(Base_Model_File)\n    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n    # num_epochs = 5\n    num_epochs = 1\n    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n    print(f\"model  to {device}\")\n    model.to(device)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    # training_args = TrainingArguments(\n    #     disable_tqdm=True,\n    #     output_dir='./checkpoints',\n    #     save_total_limit=10,\n    #     logging_dir='/content/logs',\n    #     num_train_epochs=num_epochs,\n    #     evaluation_strategy='epoch'\n    #     save_strategy='steps',\n    #     save_steps=30,\n    #     logging_steps=10,\n    #     overwrite_output_dir=True,\n    #     per_device_train_batch_size=4,\n    #     per_device_eval_batch_size=4,\n    #     gradient_accumulation_steps=4,\n    #     eval_accumulation_steps=4,\n    #     gradient_checkpointing=True,\n    #     max_grad_norm=0.5,\n    #     lr_scheduler_type=\"cosine\",\n    #     learning_rate=1e-4,\n    #     warmup_ratio=0.05,\n    #     weight_decay=0.1,\n    #     fp16_full_eval=True\n    #     fp16=True,\n    #     fp16_opt_level='O1'\n    # )\n    print(f\"start model fine tune\")\n    model.fit(train_objectives=[(train_dataloader, train_loss)],\n            epochs=num_epochs,\n            warmup_steps=warmup_steps)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.save(Tuned_Model_File)\n    print(f\"model saved to {Tuned_Model_File}\")","metadata":{"_uuid":"fd70ad45-5a43-40cc-b9a2-69b7f05d9165","_cell_guid":"59405150-0f40-40ba-b1a3-91f5c339605c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:46:13.710995Z","iopub.execute_input":"2023-03-04T19:46:13.712778Z","iopub.status.idle":"2023-03-04T19:46:13.723655Z","shell.execute_reply.started":"2023-03-04T19:46:13.712737Z","shell.execute_reply":"2023-03-04T19:46:13.722666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Train_model:\n    train_model()","metadata":{"_uuid":"5cb714f0-43a2-4052-b895-641432a9a966","_cell_guid":"70158e28-53b9-4d0a-823a-b182466c56ea","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:46:13.725468Z","iopub.execute_input":"2023-03-04T19:46:13.725912Z","iopub.status.idle":"2023-03-04T19:46:13.750346Z","shell.execute_reply.started":"2023-03-04T19:46:13.725875Z","shell.execute_reply":"2023-03-04T19:46:13.749259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(Tuned_Model_File)\ntrained_model = AutoModel.from_pretrained(Tuned_Model_File)\n\ntrained_model.to(device)","metadata":{"_uuid":"93fcda1f-dcea-45b5-9a47-4a83be5fce4e","_cell_guid":"847d9d1f-32dc-4fe8-83da-2c866a79752e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:46:13.752023Z","iopub.execute_input":"2023-03-04T19:46:13.752568Z","iopub.status.idle":"2023-03-04T19:46:24.458320Z","shell.execute_reply.started":"2023-03-04T19:46:13.752531Z","shell.execute_reply":"2023-03-04T19:46:24.457209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cls_pooling(model_output):\n    return model_output.last_hidden_state[:, 0]\n\ndef get_embeddings(text_list):\n    encoded_input = tokenizer(\n        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    model_output = trained_model(**encoded_input)\n    return cls_pooling(model_output)","metadata":{"_uuid":"f458c679-392e-4e31-8568-31db759a99a3","_cell_guid":"9290a5e6-5c70-486a-ad3e-07dd885024b9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:46:24.460034Z","iopub.execute_input":"2023-03-04T19:46:24.460438Z","iopub.status.idle":"2023-03-04T19:46:24.466583Z","shell.execute_reply.started":"2023-03-04T19:46:24.460397Z","shell.execute_reply":"2023-03-04T19:46:24.465508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build embedding based on topic_full and content_full, we also need keep topic_id and content_id.\nfrom datasets import Dataset\ndef embeddings_gen(row):\n    embedding_row = {}\n    embedding_row[\"embeddings\"]=get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0]\n    embedding_row[\"content_id\"] = str(x[\"content_id\"])\n    embedding_ds_raw.append(embedding_row)\n\n\n\ndef build_embedding():\n\n    # def add_embeddings_topic_full(x):\n    #     y = {}\n    #     y[\"content_id\"] = x[\"content_id\"]\n    #     y[\"embeddings\"]=add_embeddings_topic_full( str(x[\"topic_full\"])).detach().cpu().numpy()[0]\n    #     embedding_list.append\n    #     return x\n\n    # def add_embeddings_content_full(x):\n    #     y = {}\n    #     y[\"content_id\"] = x[\"content_id\"]\n    #     y[\"embeddings\"]=add_embeddings_topic_full( str(x[\"content_full\"])).detach().cpu().numpy()[0]\n    #     embedding_list.append\n    #     return x\n\n    dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File,streaming=True)\n    print(dataset_dict)\n    dataset = dataset_dict[\"train\"]\n    embedding_data = {\"content_id\":[], \"embeddings\": []}\n    count = 0\n    for x in dataset:\n        count += 1\n        if (count %10000 == 0):\n            print(f\"{count} {x['content_id']}\")\n        y = {}\n        embedding_data[\"content_id\"].append(x[\"content_id\"])\n        embedding_data[\"embeddings\"].append(get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0])\n        embedding_data[\"content_id\"].append(x[\"content_id\"])\n        embedding_data[\"embeddings\"].append(get_embeddings( str(x[\"content_full\"])).detach().cpu().numpy()[0])\n         \n\n    embedding_dataset = Dataset.from_dict(embedding_data)\n    \n#     embedding_list = {}\n\n    # print(f\"building embeddings for topic_full\")\n    # dataset.map(add_embeddings_topic_full)\n\n    # print(f\"building embeddings for content_full\")\n    # dataset.map(add_embeddings_topic_full)\n\n    # embedding_dataset =  concatenate_datasets([embeddings_topics_dataset, embeddings_content_dataset])\n    print(f\"embedding_dataset => {embedding_dataset}\")\n    embedding_dataset.save_to_disk(Embeddings_File)","metadata":{"_uuid":"d8f400ed-5b6b-4aa2-880e-5182133a8640","_cell_guid":"8690bc35-f6b3-4c2c-a9eb-c3f602c95037","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:46:24.468362Z","iopub.execute_input":"2023-03-04T19:46:24.468744Z","iopub.status.idle":"2023-03-04T19:46:24.493571Z","shell.execute_reply.started":"2023-03-04T19:46:24.468688Z","shell.execute_reply":"2023-03-04T19:46:24.492499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(transformers.__version__)\nprint(datasets.__version__)\nif Build_Embedding:\n    print(f\"Build Embedding .... from {Train_Data_File}\")\n    build_embedding()\n\nembedding_dataset = load_from_disk(Embeddings_File)\n\nembedding_dataset.add_faiss_index(column=\"embeddings\")\nprint(f\"finishing loading embedding_dataset\")\nprint(embedding_dataset)","metadata":{"_uuid":"96a026f1-856a-47b4-9a46-8b4c599b5ca0","_cell_guid":"2371b5d0-280d-4145-8104-8dfb0f8a08b5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T19:46:24.494873Z","iopub.execute_input":"2023-03-04T19:46:24.495738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cosine_sim(ebd, text_embedding):\n    cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    return cosine_score.item()\n\ndef get_score_topic(text, cosine_cutoff):\n    # There are only two filed in embedding data set: \"embedding\", \"content_id\"\n    text_embedding = get_embeddings(text).cpu().detach().numpy()\n    scores, samples = embedding_dataset.get_nearest_examples(\n                                                    \"embeddings\", text_embedding, k=Nearest_K\n                                                    )\n    # print(scores)\n    samples_df = pd.DataFrame.from_dict(samples)\n    samples_df[\"scores\"] = scores\n    samples_df.sort_values(\"scores\", ascending=True, inplace=True)\n    \n    samples_df['cosine_sim'] = samples_df.apply(lambda row: get_cosine_sim(row[\"embeddings\"], text_embedding), axis=1)\n    samples_df = samples_df[samples_df['cosine_sim'] >= cosine_cutoff] \n\n    scorelist = samples_df[\"scores\"].values.tolist()\n    cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    result_df = samples_df\n\n    return result_df\n\ndef get_score_topic_json(text, cosine_cutoff):\n\n    res_df = get_score_topic(text, cosine_cutoff)\n    res_dict = res_df.to_dict('records')\n    return res_dict","metadata":{"_uuid":"2d6ec92c-51b8-4c40-ab89-1d72618e5358","_cell_guid":"b6acc7ce-c3db-4efe-a679-2e72d283a279","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_row(row):\n    res_dict = get_score_topic_json(str(row[\"topic_full\"]), Cosine_Cutoff) \n    # print(f\"calculate_row dict\")\n    # print(res_dict)\n    #scores, topic_ids, content_ids, cosine_sim\n    # result.append([scores, topic_id, topic_title, **evalrow ])\n    row[\"scores\"] = \" \".join([f\"{d['scores']:.10f}\" for d in res_dict])\n    row[\"content_ids\"] = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    row[\"cosine_sim\"] = \" \".join([f\"{d['cosine_sim']:.10f}\" for d in res_dict])\n    return row\n\ndef calculate_score(eval_sampled, res_file):\n    num_rows = eval_sampled.num_rows\n    scores_column = [-2] * num_rows\n    content_ids_column = [\"\"] * num_rows\n    cosine_column = [-1.0] * num_rows\n    eval_sampled = eval_sampled.add_column(\"scores\", scores_column)\n    eval_sampled = eval_sampled.add_column(\"content_ids\", content_ids_column)\n    eval_sampled = eval_sampled.add_column(\"cosine_sim\", cosine_column)\n\n    all_columns = set(eval_sampled.column_names)\n    # keeped_columns = [\"topic_id\",\"topic_full\", \"content_full\",\"content_id\"]\n    keeped_columns = [\"topic_id\",\"scores\", \"cosine_sim\", \"content_ids\"]\n    columns_to_removed = list(all_columns.symmetric_difference(keeped_columns))\n\n    print(f\"calculating scores mapping ...\")\n    eval_sampled = eval_sampled.map(calculate_row)\n    print(f\"result_df value counet for each columns: \\n{eval_sampled.shape}\")\n    eval_sampled.to_csv(res_file) \n    \n\n# Result_file = f\"./data/result_v{dver}.csv\"\n# if Calculate_Score:\n#     eval_sampled =  dataset[\"test\"].shuffle(seed=42).select(range(2000))\n#     calculate_score(eval_sampled, Result_file)\n#     Result_file = f\"./data/result_train_v{dver}.csv\"\n#     eval_sampled =  dataset[\"train\"].shuffle(seed=42).select(range(2000))\n#     calculate_score(eval_sampled, Result_file)\n\n# calc_df = pd.read_csv(Result_file) \n# print(f\"load result file {Result_file}\")\n# print(calc_df.head(100))","metadata":{"_uuid":"57ceb8d8-3a0a-4f68-b6c6-26dceafac78f","_cell_guid":"7b968855-bcbe-407f-a6ee-0943ced98a54","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(topic_id, df_topics):\n    subset = df_topics.loc[df_topics['topic_id'] == topic_id]\n    text = None\n    for index, r in subset.iterrows():\n            text = r[\"topic_full\"]\n            break\n    if text is None:\n        return \"\"\n\n    res_dict = get_score_topic_json(text, Cosine_Cutoff) \n\n    content_ids = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    return content_ids\n\n\ndef calculate_submission(submission_df):\n\n    submission_df['content_ids'] = submission_df.apply(lambda row: get_neighbors(row[\"topic_id\"], df_topics), axis=1)\n    submission_df.to_csv(DATA_PATH + \"submission.csv\") \n    submission_df.to_csv(Submission_File) \n    print(f\"display submission head\")\n    print(submission_df.head(100))","metadata":{"_uuid":"7956c606-6f01-4647-849f-efa0e49d67b3","_cell_guid":"58ccb08e-9868-4256-ac92-254bb415742d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (Cal_Submission):\n    print(f\"Calculating submission \")\n    submission_df = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n    calculate_submission(submission_df)\n\n# from sklearn.metrics import fbeta_score\n# fbeta_score(y_true, y_pred, average='macro', beta=2)","metadata":{"_uuid":"c63f7412-8c3a-4a30-a644-5d6fa1f1d7ad","_cell_guid":"330d97e7-8c0a-434a-8ff1-a65b87bc1798","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}