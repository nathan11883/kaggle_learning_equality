{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**The reference:**\n* https://huggingface.co/blog/how-to-train-sentence-transformers\n* https://www.kaggle.com/code/andtaichi/finetunig-sentencetransformer\n* https://www.kaggle.com/code/quincyqiang/download-huggingface-pretrain-for-kaggle/notebook\n* https://towardsdatascience.com/easy-kaggle-offline-submission-with-chaining-kernels-30bba5ea5c4d\n* https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/109679\n* https://www.kaggle.com/code/jamiealexandre/sample-notebook-data-exploration/notebook\n\nThe pretrained model we use:\nhttps://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n\n**The highlights:**\n1. Join the correlation with topic and content.\n2. Concatenate \"topic_title\" and \"topic_description\" for each topic, and concate all the ancestor topics into \"topic_full\" field.\n3. Concatenate 'content_title', 'content_description', \"content_text\" for each content into \"content_full\" field.\n4. Feed \"topic_full\" and \"content_full\" as embeding pair into the model and fintune the model.\n5. Using the fine-tuned model, generate the embeding for \"topic_full\" and \"content_full\", and put them into embeding dataset.\n6. Create the faiss index by calling add_faiss_index.\n7. For each topic that need to be predict, search in the embeding dataset with faiss index using content_full, find the nearest K content (current value is 20), then filter out the result.\n8. Furhter filter out the result by calculating the cosine_sim between each content and the topic_full, and cutting off using Cosine_Cutoff (current value is larger than 0.99995).\n9. Output the result.","metadata":{"_uuid":"2eebf9ad-e525-43c9-b91a-573dc2b8371c","_cell_guid":"e173c99e-b6e7-456c-bbd1-a1bc7dad5eb2","trusted":true}},{"cell_type":"code","source":"# !pip install sentence-transformers\n# !pip install faiss-gpu\n# !pip install faiss-cpu\n# !pip install tqdm\n# !pip install nvidia-ml-py3\n# !pip install accelerate\n# !pip install scikit-learn","metadata":{"_uuid":"392f30be-26d3-4fd7-99b8-1c681968bdda","_cell_guid":"c4dd020c-7ab0-45bb-a58d-c1465f32b961","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T08:11:52.627563Z","iopub.execute_input":"2023-03-04T08:11:52.628024Z","iopub.status.idle":"2023-03-04T08:11:52.633216Z","shell.execute_reply.started":"2023-03-04T08:11:52.627951Z","shell.execute_reply":"2023-03-04T08:11:52.632157Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh |  bash\n# !apt-get install -y --allow-unauthenticated git-lfs","metadata":{"_uuid":"d716877c-1b75-497d-81ec-9f9ae4c67c72","_cell_guid":"5d0a841f-e224-4f27-9c35-d4c13ac059e3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T08:11:52.635189Z","iopub.execute_input":"2023-03-04T08:11:52.635580Z","iopub.status.idle":"2023-03-04T08:11:52.644194Z","shell.execute_reply.started":"2023-03-04T08:11:52.635541Z","shell.execute_reply":"2023-03-04T08:11:52.643024Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# !git lfs install\n# !git clone https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n# # if you want to clone without large files – just their pointers\n# # prepend your git clone with the following env var:\n# !GIT_LFS_SKIP_SMUDGE=1","metadata":{"_uuid":"4fc16c87-b0a3-4efe-9446-3f2ef1f96d60","_cell_guid":"185cf237-985c-4dd3-83bf-a752c6201b88","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T08:11:52.645736Z","iopub.execute_input":"2023-03-04T08:11:52.646903Z","iopub.status.idle":"2023-03-04T08:11:52.655324Z","shell.execute_reply.started":"2023-03-04T08:11:52.646858Z","shell.execute_reply":"2023-03-04T08:11:52.653927Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"!pip install sentence-transformers --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/sentence-transformers-2.2.2.tar.gz --no-index\n!pip install faiss-gpu  --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index\n!pip install faiss-cpu  --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_cpu-1.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  --no-index\n!pip install tqdm --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/tqdm-4.64.1-py2.py3-none-any.whl --no-index\n!pip install nvidia-ml-py3 --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/nvidia-ml-py3-7.352.0.tar.gz --no-index\n!pip install accelerate --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/accelerate-0.16.0-py3-none-any.whl --no-index\n!pip install scikit-learn --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index","metadata":{"_uuid":"dca30fc7-d98b-4492-937d-6e669c33a80c","_cell_guid":"7b817b10-f937-4ade-8451-77b2794f30e7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T08:11:52.658220Z","iopub.execute_input":"2023-03-04T08:11:52.659140Z","iopub.status.idle":"2023-03-04T08:13:02.732780Z","shell.execute_reply.started":"2023-03-04T08:11:52.659100Z","shell.execute_reply":"2023-03-04T08:13:02.731509Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/sentence-transformers-2.2.2.tar.gz\nRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.7/site-packages (2.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.14.0)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.26.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.97)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.12.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.64.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.13.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: faiss-gpu in /opt/conda/lib/python3.7/site-packages (1.7.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_cpu-1.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: faiss-cpu in /opt/conda/lib/python3.7/site-packages (1.7.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/tqdm-4.64.1-py2.py3-none-any.whl\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/nvidia-ml-py3-7.352.0.tar.gz\nRequirement already satisfied: nvidia-ml-py3 in /opt/conda/lib/python3.7/site-packages (7.352.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/accelerate-0.16.0-py3-none-any.whl\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.7/site-packages (0.12.0)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.13.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (23.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.21.6)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from accelerate) (6.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\nRequirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.21.6)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sentence_transformers import SentenceTransformer, models, InputExample, losses, util\nfrom datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets, IterableDataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nimport torch \nimport transformers\nimport datasets\n","metadata":{"_uuid":"a96f2496-7792-46a2-9437-9463e4173f22","_cell_guid":"9e9ce431-839b-474b-bbf2-35c9ea54e8fc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T08:13:02.735738Z","iopub.execute_input":"2023-03-04T08:13:02.736219Z","iopub.status.idle":"2023-03-04T08:13:02.744246Z","shell.execute_reply.started":"2023-03-04T08:13:02.736175Z","shell.execute_reply":"2023-03-04T08:13:02.742941Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"transformers.logging.set_verbosity_debug()\ndatasets.disable_progress_bar()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"execution":{"iopub.status.busy":"2023-03-04T08:13:02.745770Z","iopub.execute_input":"2023-03-04T08:13:02.746146Z","iopub.status.idle":"2023-03-04T08:13:02.760166Z","shell.execute_reply.started":"2023-03-04T08:13:02.746118Z","shell.execute_reply":"2023-03-04T08:13:02.758918Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Using device: cuda\n\nTesla P100-PCIE-16GB\nMemory Usage:\nAllocated: 0.0 GB\nCached:    0.0 GB\n","output_type":"stream"}]},{"cell_type":"code","source":"# model = SentenceTransformer(\"/kaggle/input/using-fine-tuned-sentencetransformer-env/paraphrase-multilingual-mpnet-base-v2\")","metadata":{"_uuid":"fff33d73-e01b-420f-b4a3-25b2b8ed823c","_cell_guid":"b71d1981-2895-49dd-9ff2-c775a0b9a27d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T08:13:02.762087Z","iopub.execute_input":"2023-03-04T08:13:02.762452Z","iopub.status.idle":"2023-03-04T08:13:02.773690Z","shell.execute_reply.started":"2023-03-04T08:13:02.762416Z","shell.execute_reply":"2023-03-04T08:13:02.772563Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# !ls -l","metadata":{"_uuid":"15089f12-d4a0-43d0-a816-d16251653924","_cell_guid":"ce39c189-1a85-442c-a084-1f3fa600b37c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T08:13:02.778621Z","iopub.execute_input":"2023-03-04T08:13:02.778938Z","iopub.status.idle":"2023-03-04T08:13:02.784650Z","shell.execute_reply.started":"2023-03-04T08:13:02.778909Z","shell.execute_reply":"2023-03-04T08:13:02.783511Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"Refresh_Topic = True\nRefresh_Train_Data = True\nTrain_model = True\nBuild_Embedding = True \nCalculate_Score = True \nCal_Submission = True\nCosine_Cutoff = 0.99995\nNearest_K = 20\ndver = 506\n\n#kaggle setting\ninput_folder = \"/kaggle/input\"\noutput_folder = \"/kaggle/working\"\nmodel_output_folder = \"/kaggle/working\"\nmodel_input_folder = \"/kaggle/input/using-fine-tuned-sentencetransformer-env\"\n\n#local setting\n# input_folder = \"./data/input\"\n# output_folder = \"./data/output\"\n# model_output_folder = \"./model/output\"\n# model_input_folder = \"./model/input\"\n\nTopic_Full_Data_File = f\"{output_folder}/df_topics_full_{dver}.csv\"\nTrain_Data_File= f\"{output_folder}/df_train_v{dver}.csv\"\nEmbeddings_File = f'{output_folder}/embeddings_topics_dataset_v{dver}'\n\nSubmission_File = f\"{output_folder}/submission_v{dver}.csv\"\nBase_Model_File = f\"{model_input_folder}/paraphrase-multilingual-mpnet-base-v2\"\nTuned_Model_File = f\"{model_output_folder}/paraphrase-multilingual-mpnet-base-v{dver}-tuned\"\n\nprint(f\"Topic_Full_Data_File {Topic_Full_Data_File}\")\nprint(f\"Train_Data_File {Train_Data_File}\")\nprint(f\"Embeddings_File {Embeddings_File}\")\nprint(f\"Submission_File {Submission_File}\")","metadata":{"_uuid":"c7dd6180-086b-4d02-b10e-77a88aaa0c5a","_cell_guid":"fdc288b3-d2e8-404f-a107-a5477e45a923","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T08:13:02.786273Z","iopub.execute_input":"2023-03-04T08:13:02.787512Z","iopub.status.idle":"2023-03-04T08:13:02.799093Z","shell.execute_reply.started":"2023-03-04T08:13:02.787473Z","shell.execute_reply":"2023-03-04T08:13:02.797929Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Topic_Full_Data_File /kaggle/working/df_topics_full_506.csv\nTrain_Data_File /kaggle/working/df_train_v506.csv\nEmbeddings_File /kaggle/working/embeddings_topics_dataset_v506\nSubmission_File /kaggle/working/submission_v506.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# pd.set_option('display.max_columns', 9)\n# pd.set_option('display.max_rows', 200)\n# pd.set_option('display.min_rows', 10)\n# pd.set_option(\"expand_frame_repr\", True)\n# pd.set_option('display.width', None)\n# pd.set_option('display.max_colwidth', 50)\n# DATA_PATH = \"/kaggle/input/learning-equality-curriculum-recommendations/\"\nDATA_PATH = f\"{input_folder}/learning-equality-curriculum-recommendations/\"\ntopics = pd.read_csv(DATA_PATH + \"topics.csv\")\ncontent = pd.read_csv(DATA_PATH + \"content.csv\")\ncorrelations = pd.read_csv(DATA_PATH + \"correlations.csv\")\n# sample_submission = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n\nprint(f\"DATA_PATH {DATA_PATH}\")","metadata":{"_uuid":"38b48165-b019-459b-9cc3-baade2f0580f","_cell_guid":"6e184bbe-b44f-41ff-b459-7c1321df1874","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T08:13:02.800698Z","iopub.execute_input":"2023-03-04T08:13:02.801976Z","iopub.status.idle":"2023-03-04T08:13:13.854501Z","shell.execute_reply.started":"2023-03-04T08:13:02.801826Z","shell.execute_reply":"2023-03-04T08:13:13.853439Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"DATA_PATH /kaggle/input/learning-equality-curriculum-recommendations/\n","output_type":"stream"}]},{"cell_type":"code","source":"df_topics = None\n\nif ( not topics.columns[0].startswith(\"topic_\")):\n    print(f\"renaming topics ...\")\n    topics.rename(columns=lambda x: \"topic_\" + x, inplace=True)\n    content.rename(columns=lambda x: \"content_\" + x, inplace=True)","metadata":{"_uuid":"6e5e9242-bc04-4f78-9090-c03f96397ca9","_cell_guid":"70a808d7-5019-4607-8b34-f4b15a9e144c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T08:13:13.856879Z","iopub.execute_input":"2023-03-04T08:13:13.858049Z","iopub.status.idle":"2023-03-04T08:13:13.867024Z","shell.execute_reply.started":"2023-03-04T08:13:13.858007Z","shell.execute_reply":"2023-03-04T08:13:13.866014Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"renaming topics ...\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_topic_full(row):\n    topic_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_description = str(row[\"topic_description\"]) if pd.notna(row[\"topic_description\"]) else \"\"\n    topic_full = \"title: \" + topic_title\n    if (topic_description != \"\"):\n        topic_full = topic_full + \"\\r\\n\" + \"description: \" + topic_description\n\n    return topic_full\n\ndef get_parents(df, row):\n    topic_id = row[\"topic_id\"]\n    topic_title_full = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_parent = row[\"topic_parent\"]\n    topic_level = row[\"topic_level\"]\n    topic_full = get_topic_full(row)\n    while not pd.isnull(topic_parent):\n        subset = df.loc[df['topic_id'] == topic_parent]\n        for index, r in subset.iterrows():\n            t_full = get_topic_full(r)\n            topic_full = t_full + \"\\r\\n\" + topic_full \n            t_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n            topic_title_full = t_title + \".\" + topic_title_full \n            topic_parent = r[\"topic_parent\"]\n            topic_level = r[\"topic_level\"]\n            break\n\n    return topic_title_full, topic_full\n\n\ndef refresh_topic(topics):\n\n    df_topics = topics\n    print(f\"Before expand topic full ...\")\n    print(df_topics.head(100))\n\n    topic_title_full = []\n    topic_full = []\n\n#     for index, row in tqdm(df_topics.iterrows(), total=df_topics.shape[0]):\n    for index, row in df_topics.iterrows():\n        t_title_full, t_full = get_parents(df_topics, row)\n        topic_title_full.append(t_title_full)\n        topic_full.append(t_full)\n        if (index % 10000 == 0):\n            print(f\"processing df_topics: \\n{index}, {row}\")\n\n    df_topics['topic_title_full'] = topic_title_full\n    df_topics['topic_full'] = topic_full\n\n    df_topics.to_csv(Topic_Full_Data_File) \n    print(f\"Finished processing df_tpocs, and saved to {Topic_Full_Data_File}\")\n    return df_topics\n\nif (Refresh_Topic):\n    print(\"Freshing topic...\")\n    df_topics = refresh_topic(topics)\nelse:\n    print(f\"Load df_topcis from {Topic_Full_Data_File}\")\n    df_topics = pd.read_csv(Topic_Full_Data_File)","metadata":{"_uuid":"aee2af9d-3d31-40bb-8969-6ce2513102d4","_cell_guid":"ea7315ca-963b-42f7-9028-97a213dfaee0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-04T08:13:13.868674Z","iopub.execute_input":"2023-03-04T08:13:13.869207Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Freshing topic...\nBefore expand topic full ...\n          topic_id                                        topic_title  \\\n0   t_00004da3a1b2                         Откриването на резисторите   \n1   t_000095e03056             Unit 3.3 Enlargements and Similarities   \n2   t_00068291e9a4                    Entradas e saídas de uma função   \n3   t_00069b63a70a                                        Transcripts   \n4   t_0006d41a73a8  Графики на експоненциални функции (Алгебра 2 н...   \n..             ...                                                ...   \n95  t_005dd3c69dc4                                Volume - Ang Volume   \n96  t_005dd69acb26                        Domain of radical functions   \n97  t_005e4df86333          Converting Grams into Kilograms and Grams   \n98  t_005e5769654a                                           Círculos   \n99  t_005f462ff2b3                                 Réaliser une soupe   \n\n                                    topic_description topic_channel  \\\n0   Изследване на материали, които предизвикват на...        000cf7   \n1                                                 NaN        b3f329   \n2                Entenda um pouco mais sobre funções.        8e286a   \n3                                                 NaN        6e3ba4   \n4   Научи повече за графиките на сложните показате...        000cf7   \n..                                                ...           ...   \n95                         English and Tagalog Module        c88ff6   \n96  Learn how the domain of radical functions is d...        0c929f   \n97                                                NaN        fef095   \n98  Construir polígonos regulares inscritos en cír...        36a98b   \n99                                                NaN        65c955   \n\n   topic_category  topic_level topic_language    topic_parent  \\\n0          source            4             bg  t_16e29365b50d   \n1         aligned            2             en  t_aa32fb6252dc   \n2          source            4             pt  t_d14b6c2a2b70   \n3          source            3             en  t_4054df11a74e   \n4          source            4             bg  t_e2452e21d252   \n..            ...          ...            ...             ...   \n95   supplemental            3            fil  t_0fce52cf2752   \n96         source            4             sw  t_c5b850674be3   \n97         source            6             en  t_3f1c3009104b   \n98         source            3             es  t_efbc9c8cea4c   \n99         source            3             fr  t_f3ef621574f2   \n\n    topic_has_content  \n0                True  \n1               False  \n2                True  \n3                True  \n4                True  \n..                ...  \n95               True  \n96               True  \n97               True  \n98              False  \n99               True  \n\n[100 rows x 9 columns]\nprocessing df_topics: \n0, topic_id                                                t_00004da3a1b2\ntopic_title                                 Откриването на резисторите\ntopic_description    Изследване на материали, които предизвикват на...\ntopic_channel                                                   000cf7\ntopic_category                                                  source\ntopic_level                                                          4\ntopic_language                                                      bg\ntopic_parent                                            t_16e29365b50d\ntopic_has_content                                                 True\nName: 0, dtype: object\nprocessing df_topics: \n10000, topic_id                                                t_21bb649eaa72\ntopic_title                                       Classroom strategies\ntopic_description    Strategies to implement Khan Academy in the cl...\ntopic_channel                                                   0ec697\ntopic_category                                                  source\ntopic_level                                                          4\ntopic_language                                                      en\ntopic_parent                                            t_c8beedde01ee\ntopic_has_content                                                 True\nName: 10000, dtype: object\nprocessing df_topics: \n20000, topic_id                                                t_4351f0afb8a2\ntopic_title                                                     Maumbo\ntopic_description    Ainisha maumbo na fanya maswali kwa kutumia si...\ntopic_channel                                                   0c929f\ntopic_category                                                  source\ntopic_level                                                          3\ntopic_language                                                      sw\ntopic_parent                                            t_3fbe64739ddd\ntopic_has_content                                                False\nName: 20000, dtype: object\nprocessing df_topics: \n30000, topic_id                                        t_645da72a7777\ntopic_title          Diviser une fraction par un nombre entier\ntopic_description                                          NaN\ntopic_channel                                           c152d6\ntopic_category                                          source\ntopic_level                                                  4\ntopic_language                                              fr\ntopic_parent                                    t_6098d66311bd\ntopic_has_content                                         True\nName: 30000, dtype: object\nprocessing df_topics: \n40000, topic_id                                                t_859bc741b4d3\ntopic_title          1.2 Branches of Biology -- Supplementary Resou...\ntopic_description    Materials in this folder have been reviewed by...\ntopic_channel                                                   c7ca13\ntopic_category                                                 aligned\ntopic_level                                                          5\ntopic_language                                                      en\ntopic_parent                                            t_d825f25c9e88\ntopic_has_content                                                 True\nName: 40000, dtype: object\nprocessing df_topics: \n50000, topic_id                   t_a74aa3fff8cb\ntopic_title          Revisión de práctica\ntopic_description                     NaN\ntopic_channel                      36a98b\ntopic_category                     source\ntopic_level                             4\ntopic_language                         es\ntopic_parent               t_55543c95085c\ntopic_has_content                    True\nName: 50000, dtype: object\nprocessing df_topics: \n60000, topic_id                                                t_c85bbe2eada6\ntopic_title                                             Trilha A: Vida\ntopic_description    (EF06CI05) Explicar a organização básica das c...\ntopic_channel                                                   57e9d3\ntopic_category                                                 aligned\ntopic_level                                                          4\ntopic_language                                                      pt\ntopic_parent                                            t_b48e3c9bb247\ntopic_has_content                                                False\nName: 60000, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"After expand topic full ...\")\nprint(df_topics.head(100))\nprint(f\"df_topics value counet for each columns: \\n{df_topics.nunique()}\")","metadata":{"_uuid":"60b7de1c-8439-4116-9fff-2a3a82f14ac3","_cell_guid":"1368cb4c-2c40-47c5-ad4e-f07657f5c4dd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load train data by combining correlation table with topic and content tables.","metadata":{"_uuid":"5c391db5-7a6f-4796-b498-908d0018ada8","_cell_guid":"88eeb1e1-990c-41ba-a419-1f45f5f1386d","trusted":true}},{"cell_type":"code","source":"def load_train_data(topics):\n    train_df_columns = [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_full\", \"topic_id\",\"content_id\", \"content_description\", \"content_text\" ]\n    \n    correlations[\"content_id\"] = correlations[\"content_ids\"].str.split(\" \")\n    corr = correlations.explode(\"content_id\").drop(columns=[\"content_ids\"])\n\n    corr = corr.merge(df_topics, how=\"left\", on=\"topic_id\")\n    corr = corr.merge(content, how=\"left\", on=\"content_id\")\n\n    train_df = pd.DataFrame(corr[train_df_columns])\n    cols = ['content_title', 'content_description', \"content_text\"]\n    train_df['content_full'] = train_df[cols].apply(lambda row: '\\r\\n'.join(row.values.astype(str)), axis=1)\n    \n    final_train_data = pd.DataFrame(train_df)\n    \n    print(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\n\n    final_train_data.to_csv(Train_Data_File)\n    \n    return final_train_data","metadata":{"_uuid":"cefd47c3-7c15-4a2d-888c-b2fea75b7148","_cell_guid":"4d4d6601-663d-4a41-97e7-1b54eb15cbb7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Refresh_Train_Data:\n    print(f\"Refresh_Train_Data ==>>>\")\n    final_train_data = load_train_data(topics)\nelse:\n    print(f\"load final_train_data from {Train_Data_File}\")\n    final_train_data = pd.read_csv(Train_Data_File)","metadata":{"_uuid":"4982e343-77be-4132-aef6-5c09e4675f13","_cell_guid":"3a2c3d2e-81c1-40c6-8ede-bacaa784b1f4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File)\nprint(dataset_dict)\n\ndataset = dataset_dict[\"train\"]\n\ndataset = dataset.train_test_split(test_size=0.2)\n\ntrain_dataset = dataset[\"train\"]\n\nkeeped_columns = [\"topic_full\", \"content_full\"]\ncolumns = train_dataset.column_names\ncolumns_to_keep = [\"topic_full\", \"content_full\"]\ncolumns_to_remove = set(columns_to_keep).symmetric_difference(columns)\ntrain_dataset = train_dataset.remove_columns(columns_to_remove)\n\n# train_dataset = train_dataset.shuffle(seed=42).select(range(10000))\n\ntrain_examples = []\neval_examples = []\n\nn_examples = train_dataset.num_rows\n\nfor dt in train_dataset:\n    # print([dt[\"topic_full\"], dt[\"content_full\"]])\n    # train_examples.append(InputExample(texts=[str(dt[\"topic_full\"]), str(dt[\"content_full\"])]))\n    first = str(dt[\"topic_full\"]) if dt[\"topic_full\"] != \"\" else \"\" \n    second = str(dt[\"content_full\"]) if str(dt[\"content_full\"]) != \"\" else \"\" \n    if (first != \"\" and second != \"\"):\n        train_examples.append(InputExample(texts=[first, second]))\n\nprint(f\"train_examples: {len(train_examples)}\")","metadata":{"_uuid":"52aef0f0-820d-45e0-9e8d-d6e0a3eeb592","_cell_guid":"c4c5fe72-bb4e-4d2d-b7c5-bedac3c03e72","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model():\n    \n    # model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n    print(f\"load base model from {Base_Model_File}\")\n    model = SentenceTransformer(Base_Model_File)\n    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n    # num_epochs = 5\n    num_epochs = 1\n    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n    print(f\"model  to {device}\")\n    model.to(device)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    # training_args = TrainingArguments(\n    #     disable_tqdm=True,\n    #     output_dir='./checkpoints',\n    #     save_total_limit=10,\n    #     logging_dir='/content/logs',\n    #     num_train_epochs=num_epochs,\n    #     evaluation_strategy='epoch'\n    #     save_strategy='steps',\n    #     save_steps=30,\n    #     logging_steps=10,\n    #     overwrite_output_dir=True,\n    #     per_device_train_batch_size=4,\n    #     per_device_eval_batch_size=4,\n    #     gradient_accumulation_steps=4,\n    #     eval_accumulation_steps=4,\n    #     gradient_checkpointing=True,\n    #     max_grad_norm=0.5,\n    #     lr_scheduler_type=\"cosine\",\n    #     learning_rate=1e-4,\n    #     warmup_ratio=0.05,\n    #     weight_decay=0.1,\n    #     fp16_full_eval=True\n    #     fp16=True,\n    #     fp16_opt_level='O1'\n    # )\n    print(f\"start model fine tune\")\n    model.fit(train_objectives=[(train_dataloader, train_loss)],\n            epochs=num_epochs,\n            warmup_steps=warmup_steps)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.save(Tuned_Model_File)\n    print(f\"model saved to {Tuned_Model_File}\")","metadata":{"_uuid":"42a29682-182f-460d-8cee-0afa8335dd4e","_cell_guid":"46d78bd1-1302-4010-9781-448eb1386cd3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Train_model:\n    train_model()","metadata":{"_uuid":"ba4b3edb-54a6-46cd-9a27-f488215a4c09","_cell_guid":"acb5ad1f-fd16-4e88-9637-d755e7833343","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(Tuned_Model_File)\ntrained_model = AutoModel.from_pretrained(Tuned_Model_File)\n\ntrained_model.to(device)","metadata":{"_uuid":"5232f1b9-27de-4e58-b966-21023f386dfd","_cell_guid":"9fa707d6-f837-4c29-b490-40febefc5d6c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cls_pooling(model_output):\n    return model_output.last_hidden_state[:, 0]\n\ndef get_embeddings(text_list):\n    encoded_input = tokenizer(\n        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    model_output = trained_model(**encoded_input)\n    return cls_pooling(model_output)","metadata":{"_uuid":"e1724244-971c-44bc-b453-45e84090f112","_cell_guid":"d360b8e2-8f3e-4f85-bc52-2efacd8ff2c2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build embedding based on topic_full and content_full, we also need keep topic_id and content_id.\n\ndef embeddings_gen(row):\n    embedding_row = {}\n    embedding_row[\"embeddings\"]=get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0]\n    embedding_row[\"content_id\"] = str(x[\"content_id\"])\n    embedding_ds_raw.append(embedding_row)\n\n\n\ndef build_embedding():\n\n    # def add_embeddings_topic_full(x):\n    #     y = {}\n    #     y[\"content_id\"] = x[\"content_id\"]\n    #     y[\"embeddings\"]=add_embeddings_topic_full( str(x[\"topic_full\"])).detach().cpu().numpy()[0]\n    #     embedding_list.append\n    #     return x\n\n    # def add_embeddings_content_full(x):\n    #     y = {}\n    #     y[\"content_id\"] = x[\"content_id\"]\n    #     y[\"embeddings\"]=add_embeddings_topic_full( str(x[\"content_full\"])).detach().cpu().numpy()[0]\n    #     embedding_list.append\n    #     return x\n\n    dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File,streaming=True)\n    print(dataset_dict)\n    dataset = dataset_dict[\"train\"]\n\n    count = 0\n    def generator(ds, count):\n        for x in ds:\n            count += 1\n            if (count %10000 == 0):\n                print(f\"{count} {x['content_id']}\")\n            y = {}\n            y[\"content_id\"] = x[\"content_id\"]\n            y[\"embeddings\"]=get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0]\n            yield y\n            z = {}\n            z[\"content_id\"] = x[\"content_id\"]\n            z[\"embeddings\"]=get_embeddings( str(x[\"content_full\"])).detach().cpu().numpy()[0]\n            yield z \n\n    embedding_dataset = Dataset.from_generator(generator, gen_kwargs={\"ds\":dataset, \"count\": count})\n    \n#     embedding_list = {}\n\n    # print(f\"building embeddings for topic_full\")\n    # dataset.map(add_embeddings_topic_full)\n\n    # print(f\"building embeddings for content_full\")\n    # dataset.map(add_embeddings_topic_full)\n\n    # embedding_dataset =  concatenate_datasets([embeddings_topics_dataset, embeddings_content_dataset])\n    print(f\"embedding_dataset => {embedding_dataset}\")\n    embedding_dataset.save_to_disk(Embeddings_File)","metadata":{"_uuid":"d04f6daf-caf0-4176-ac3e-3280afb8474a","_cell_guid":"294dc09e-4ff2-48ea-9483-3ed6230aff2c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Build_Embedding:\n    print(f\"Build Embedding .... from {Train_Data_File}\")\n    build_embedding()\n\nembedding_dataset = load_from_disk(Embeddings_File)\n\nembedding_dataset.add_faiss_index(column=\"embeddings\")\nprint(f\"finishing loading embedding_dataset\")\nprint(embedding_dataset)","metadata":{"_uuid":"46242819-74f7-4fa5-a5c0-83443244f1f6","_cell_guid":"cfad7417-07fd-43b3-b2ed-34790b2538b5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cosine_sim(ebd, text_embedding):\n    cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    return cosine_score.item()\n\ndef get_score_topic(text, cosine_cutoff):\n    # There are only two filed in embedding data set: \"embedding\", \"content_id\"\n    text_embedding = get_embeddings(text).cpu().detach().numpy()\n    scores, samples = embedding_dataset.get_nearest_examples(\n                                                    \"embeddings\", text_embedding, k=Nearest_K\n                                                    )\n    # print(scores)\n    samples_df = pd.DataFrame.from_dict(samples)\n    samples_df[\"scores\"] = scores\n    samples_df.sort_values(\"scores\", ascending=True, inplace=True)\n    \n    samples_df['cosine_sim'] = samples_df.apply(lambda row: get_cosine_sim(row[\"embeddings\"], text_embedding), axis=1)\n    samples_df = samples_df[samples_df['cosine_sim'] >= cosine_cutoff] \n\n    scorelist = samples_df[\"scores\"].values.tolist()\n    cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    result_df = samples_df\n\n    return result_df\n\ndef get_score_topic_json(text, cosine_cutoff):\n\n    res_df = get_score_topic(text, cosine_cutoff)\n    res_dict = res_df.to_dict('records')\n    return res_dict","metadata":{"_uuid":"8a7520b5-1380-4b61-9bc5-92d600a69a75","_cell_guid":"d0655599-fa39-4706-98fd-aec009fc729a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_row(row):\n    res_dict = get_score_topic_json(str(row[\"topic_full\"]), Cosine_Cutoff) \n    # print(f\"calculate_row dict\")\n    # print(res_dict)\n    #scores, topic_ids, content_ids, cosine_sim\n    # result.append([scores, topic_id, topic_title, **evalrow ])\n    row[\"scores\"] = \" \".join([f\"{d['scores']:.10f}\" for d in res_dict])\n    row[\"content_ids\"] = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    row[\"cosine_sim\"] = \" \".join([f\"{d['cosine_sim']:.10f}\" for d in res_dict])\n    return row\n\ndef calculate_score(eval_sampled, res_file):\n    num_rows = eval_sampled.num_rows\n    scores_column = [-2] * num_rows\n    content_ids_column = [\"\"] * num_rows\n    cosine_column = [-1.0] * num_rows\n    eval_sampled = eval_sampled.add_column(\"scores\", scores_column)\n    eval_sampled = eval_sampled.add_column(\"content_ids\", content_ids_column)\n    eval_sampled = eval_sampled.add_column(\"cosine_sim\", cosine_column)\n\n    all_columns = set(eval_sampled.column_names)\n    # keeped_columns = [\"topic_id\",\"topic_full\", \"content_full\",\"content_id\"]\n    keeped_columns = [\"topic_id\",\"scores\", \"cosine_sim\", \"content_ids\"]\n    columns_to_removed = list(all_columns.symmetric_difference(keeped_columns))\n\n    print(f\"calculating scores mapping ...\")\n    eval_sampled = eval_sampled.map(calculate_row)\n    print(f\"result_df value counet for each columns: \\n{eval_sampled.shape}\")\n    eval_sampled.to_csv(res_file) \n    \n\n# Result_file = f\"./data/result_v{dver}.csv\"\n# if Calculate_Score:\n#     eval_sampled =  dataset[\"test\"].shuffle(seed=42).select(range(2000))\n#     calculate_score(eval_sampled, Result_file)\n#     Result_file = f\"./data/result_train_v{dver}.csv\"\n#     eval_sampled =  dataset[\"train\"].shuffle(seed=42).select(range(2000))\n#     calculate_score(eval_sampled, Result_file)\n\n# calc_df = pd.read_csv(Result_file) \n# print(f\"load result file {Result_file}\")\n# print(calc_df.head(100))","metadata":{"_uuid":"8a27c5ca-289e-427f-8d61-64f9d0812036","_cell_guid":"9733969d-3af9-42e8-b253-64c93178147b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(topic_id, df_topics):\n    subset = df_topics.loc[df_topics['topic_id'] == topic_id]\n    text = None\n    for index, r in subset.iterrows():\n            text = r[\"topic_full\"]\n            break\n    if text is None:\n        return \"\"\n\n    res_dict = get_score_topic_json(text, Cosine_Cutoff) \n\n    content_ids = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    return content_ids\n\n\ndef calculate_submission(submission_df):\n\n    submission_df['content_ids'] = submission_df.apply(lambda row: get_neighbors(row[\"topic_id\"], df_topics), axis=1)\n    submission_df.to_csv(DATA_PATH + \"submission.csv\") \n    submission_df.to_csv(Submission_File) \n    print(f\"display submission head\")\n    print(submission_df.head(100))","metadata":{"_uuid":"1879d11c-faab-4f3e-a618-29629249047b","_cell_guid":"619707bb-d7d7-4588-850a-019254c81222","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (Cal_Submission):\n    print(f\"Calculating submission \")\n    submission_df = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n    calculate_submission(submission_df)\n\n# from sklearn.metrics import fbeta_score\n# fbeta_score(y_true, y_pred, average='macro', beta=2)","metadata":{"_uuid":"13fe4e94-95e3-43b3-9c08-bb2d23bc0b62","_cell_guid":"b3f2d15a-0313-4c53-8b6c-fceb07a1cabd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}