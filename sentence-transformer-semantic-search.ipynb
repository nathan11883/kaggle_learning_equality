{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**The reference:**\n1. https://www.kaggle.com/code/andtaichi/finetunig-sentencetransformer\n2. https://www.kaggle.com/code/quincyqiang/download-huggingface-pretrain-for-kaggle/notebook\n3. https://towardsdatascience.com/easy-kaggle-offline-submission-with-chaining-kernels-30bba5ea5c4d\n4. https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/109679\n5. https://www.kaggle.com/code/jamiealexandre/sample-notebook-data-exploration/notebook\n\nThe pretrained model we use:\nhttps://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n\n**The highlights:**\n1. Join the correlation with topic and content.\n2. Concatenate \"topic_title\" and \"topic_description\" for each topic, and concate all the ancestor topics into \"topic_full\" field.\n3. Concatenate 'content_title', 'content_description', \"content_text\" for each content into \"content_full\" field.\n4. Feed \"topic_full\" and \"content_full\" as embeding pair into the model and fintune the model.\n5. Using the fine-tuned model, generate the embeding for \"topic_full\" and \"content_full\", and put them into embeding dataset.\n6. Create the faiss index by calling add_faiss_index.\n7. For each topic that need to be predict, search in the embeding dataset with faiss index using content_full, find the nearest K content (current value is 20), then filter out the result.\n8. Furhter filter out the result by calculating the cosine_sim between each content and the topic_full, and cutting off using Cosine_Cutoff (current value is larger than 0.99995).\n9. Output the result.","metadata":{"_uuid":"e929748d-7dbb-4b7e-9fb6-059c4c696dde","_cell_guid":"e7b24250-08b5-4c63-9ca0-1819fa30e6ae","trusted":true}},{"cell_type":"code","source":"# !pip install sentence-transformers\n# !pip install faiss-gpu\n# !pip install faiss-cpu\n# !pip install tqdm\n# !pip install nvidia-ml-py3\n# !pip install accelerate\n# !pip install scikit-learn","metadata":{"_uuid":"7ca5a5a9-d775-4e72-8832-90a0a72da7b4","_cell_guid":"2718ebfa-0f46-4f8b-bb85-91bb8d92fee8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-03T00:41:32.186656Z","iopub.execute_input":"2023-03-03T00:41:32.187213Z","iopub.status.idle":"2023-03-03T00:41:32.208545Z","shell.execute_reply.started":"2023-03-03T00:41:32.187172Z","shell.execute_reply":"2023-03-03T00:41:32.207662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh |  bash\n# !apt-get install -y --allow-unauthenticated git-lfs","metadata":{"_uuid":"4c3e2e8e-867c-40dd-be5e-2a92a249ed05","_cell_guid":"2eb18250-351a-4ea4-9c1a-518d53ab3613","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-03T00:41:32.210492Z","iopub.execute_input":"2023-03-03T00:41:32.211221Z","iopub.status.idle":"2023-03-03T00:41:32.215542Z","shell.execute_reply.started":"2023-03-03T00:41:32.211186Z","shell.execute_reply":"2023-03-03T00:41:32.214391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git lfs install\n# !git clone https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n# # if you want to clone without large files â€“ just their pointers\n# # prepend your git clone with the following env var:\n# !GIT_LFS_SKIP_SMUDGE=1","metadata":{"_uuid":"4263a48a-a9b8-4f77-87ab-bb9cd6b4c71d","_cell_guid":"43ea758b-ee03-483f-9afe-4fcc83b1c932","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-03T00:41:32.217126Z","iopub.execute_input":"2023-03-03T00:41:32.217943Z","iopub.status.idle":"2023-03-03T00:41:32.226376Z","shell.execute_reply.started":"2023-03-03T00:41:32.217884Z","shell.execute_reply":"2023-03-03T00:41:32.225311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentence-transformers --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/sentence-transformers-2.2.2.tar.gz --no-index\n!pip install faiss-gpu  --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index\n!pip install faiss-cpu  --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_cpu-1.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  --no-index\n!pip install tqdm --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/tqdm-4.64.1-py2.py3-none-any.whl --no-index\n!pip install nvidia-ml-py3 --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/nvidia-ml-py3-7.352.0.tar.gz --no-index\n!pip install accelerate --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/accelerate-0.16.0-py3-none-any.whl --no-index\n!pip install scikit-learn --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index","metadata":{"_uuid":"f990994d-1a74-42be-9afd-c35255b99d2d","_cell_guid":"97fbcd49-4c84-4572-97b7-7479d02ddfb4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-03T00:41:32.229097Z","iopub.execute_input":"2023-03-03T00:41:32.230083Z","iopub.status.idle":"2023-03-03T00:42:50.927700Z","shell.execute_reply.started":"2023-03-03T00:41:32.230046Z","shell.execute_reply":"2023-03-03T00:42:50.926460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sentence_transformers import SentenceTransformer, models, InputExample, losses, util\nfrom datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets, IterableDataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nimport torch \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"3e415092-cf6f-4859-ab44-1b0b14b2996b","_cell_guid":"a6ed936c-d136-4493-a54d-090e9b4271d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-03T00:42:50.929502Z","iopub.execute_input":"2023-03-03T00:42:50.929847Z","iopub.status.idle":"2023-03-03T00:43:02.857382Z","shell.execute_reply.started":"2023-03-03T00:42:50.929793Z","shell.execute_reply":"2023-03-03T00:43:02.856265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = SentenceTransformer(\"/kaggle/input/using-fine-tuned-sentencetransformer-env/paraphrase-multilingual-mpnet-base-v2\")","metadata":{"_uuid":"5aaf79a7-8103-480c-a118-3da37b68a190","_cell_guid":"9e44aa99-480a-4306-acca-94728c804b2d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-03T00:43:02.861363Z","iopub.execute_input":"2023-03-03T00:43:02.862290Z","iopub.status.idle":"2023-03-03T00:43:02.868906Z","shell.execute_reply.started":"2023-03-03T00:43:02.862253Z","shell.execute_reply":"2023-03-03T00:43:02.866396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls -l","metadata":{"_uuid":"aa0c87c1-ad51-4dae-89b3-5d859743caed","_cell_guid":"8bf14958-ba80-43b6-9fb8-9fcfe57049e5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-03T00:43:02.870576Z","iopub.execute_input":"2023-03-03T00:43:02.870986Z","iopub.status.idle":"2023-03-03T00:43:02.883156Z","shell.execute_reply.started":"2023-03-03T00:43:02.870947Z","shell.execute_reply":"2023-03-03T00:43:02.882139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Refresh_Topic = True\nRefresh_Train_Data = True\nTrain_model = True\nBuild_Embedding = True \nCalculate_Score = True \nCal_Submission = True\nCosine_Cutoff = 0.99995\nNearest_K = 20\ndver = 505\n\ninput_folder = \"/kaggle/input\"\n# input_folder = \"./data/input\"\noutput_folder = \"/kaggle/working\"\n# output_folder = \"./data/output\"\nmodel_folder = \"/kaggle/input\"\n# model_folder = \"./model\"\n\nTopic_Full_Data_File = f\"{output_folder}/df_topics_full_{dver}.csv\"\nTrain_Data_File= f\"{output_folder}/df_train_v{dver}.csv\"\nEmbeddings_File = f'{output_folder}/embeddings_topics_dataset_v{dver}'\n\nSubmission_File = f\"{output_folder}/submission_v{dver}.csv\"\nModel_File = f\"{model_folder}/paraphrase-multilingual-mpnet-base-v{dver}-exp\"\n\nprint(f\"Topic_Full_Data_File {Topic_Full_Data_File}\")\nprint(f\"Train_Data_File {Train_Data_File}\")\nprint(f\"Embeddings_File {Embeddings_File}\")\nprint(f\"Submission_File {Submission_File}\")","metadata":{"_uuid":"a591b7ea-5086-412b-acc0-8dba147f0f0d","_cell_guid":"a52475dd-d2db-4817-ab7f-b5b5a237ec18","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-03T00:43:02.885445Z","iopub.execute_input":"2023-03-03T00:43:02.885712Z","iopub.status.idle":"2023-03-03T00:43:02.897799Z","shell.execute_reply.started":"2023-03-03T00:43:02.885687Z","shell.execute_reply":"2023-03-03T00:43:02.896025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pd.set_option('display.max_columns', 9)\n# pd.set_option('display.max_rows', 200)\n# pd.set_option('display.min_rows', 10)\n# pd.set_option(\"expand_frame_repr\", True)\n# pd.set_option('display.width', None)\n# pd.set_option('display.max_colwidth', 50)\n# DATA_PATH = \"/kaggle/input/learning-equality-curriculum-recommendations/\"\nDATA_PATH = f\"{input_folder}/learning-equality-curriculum-recommendations/\"\ntopics = pd.read_csv(DATA_PATH + \"topics.csv\")\ncontent = pd.read_csv(DATA_PATH + \"content.csv\")\ncorrelations = pd.read_csv(DATA_PATH + \"correlations.csv\")\n# sample_submission = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n\nprint(f\"DATA_PATH {DATA_PATH}\")","metadata":{"_uuid":"a7a0aec0-3d3b-45d9-be15-6b1c72636c73","_cell_guid":"ab1d6bff-aebd-4c87-8a32-72116f6859fb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-03T00:43:02.899426Z","iopub.execute_input":"2023-03-03T00:43:02.899696Z","iopub.status.idle":"2023-03-03T00:43:22.965925Z","shell.execute_reply.started":"2023-03-03T00:43:02.899671Z","shell.execute_reply":"2023-03-03T00:43:22.964500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_topics = None\n\nif ( not topics.columns[0].startswith(\"topic_\")):\n    print(f\"renaming topics ...\")\n    topics.rename(columns=lambda x: \"topic_\" + x, inplace=True)\n    content.rename(columns=lambda x: \"content_\" + x, inplace=True)","metadata":{"_uuid":"a38b9b9f-e970-4454-b69a-1fd97cc8a955","_cell_guid":"90a9f463-8676-4604-a6f8-84bdaff28d64","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-03T00:43:22.967619Z","iopub.execute_input":"2023-03-03T00:43:22.967994Z","iopub.status.idle":"2023-03-03T00:43:22.977741Z","shell.execute_reply.started":"2023-03-03T00:43:22.967955Z","shell.execute_reply":"2023-03-03T00:43:22.976599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_topic_full(row):\n    topic_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_description = str(row[\"topic_description\"]) if pd.notna(row[\"topic_description\"]) else \"\"\n    topic_full = \"title: \" + topic_title\n    if (topic_description != \"\"):\n        topic_full = topic_full + \"\\r\\n\" + \"description: \" + topic_description\n\n    return topic_full\n\ndef get_parents(df, row):\n    topic_id = row[\"topic_id\"]\n    topic_title_full = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_parent = row[\"topic_parent\"]\n    topic_level = row[\"topic_level\"]\n    topic_full = get_topic_full(row)\n    while not pd.isnull(topic_parent):\n        subset = df.loc[df['topic_id'] == topic_parent]\n        for index, r in subset.iterrows():\n            t_full = get_topic_full(r)\n            topic_full = t_full + \"\\r\\n\" + topic_full \n            t_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n            topic_title_full = t_title + \".\" + topic_title_full \n            topic_parent = r[\"topic_parent\"]\n            topic_level = r[\"topic_level\"]\n            break\n\n    return topic_title_full, topic_full\n\n\ndef refresh_topic(topics):\n\n    df_topics = topics\n    print(f\"Before expand topic full ...\")\n    print(df_topics.head(100))\n\n    topic_title_full = []\n    topic_full = []\n\n#     for index, row in tqdm(df_topics.iterrows(), total=df_topics.shape[0]):\n    for index, row in df_topics.iterrows():\n        t_title_full, t_full = get_parents(df_topics, row)\n        topic_title_full.append(t_title_full)\n        topic_full.append(t_full)\n        if (index % 10000 == 0):\n            print(f\"processing df_topics: \\n{index}, {row}\")\n\n    df_topics['topic_title_full'] = topic_title_full\n    df_topics['topic_full'] = topic_full\n\n    df_topics.to_csv(Topic_Full_Data_File) \n    print(f\"Finished processing df_tpocs, and saved to {Topic_Full_Data_File}\")\n    return df_topics\n\nif (Refresh_Topic):\n    print(\"Freshing topic...\")\n    df_topics = refresh_topic(topics)\nelse:\n    print(f\"Load df_topcis from {Topic_Full_Data_File}\")\n    df_topics = pd.read_csv(Topic_Full_Data_File)","metadata":{"_uuid":"970e9cc4-e76c-4eb3-a96a-86599c1c4448","_cell_guid":"6e7082a6-0b14-4743-a15d-1f3e3393e884","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-03T00:43:22.982423Z","iopub.execute_input":"2023-03-03T00:43:22.983409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"After expand topic full ...\")\nprint(df_topics.head(100))\nprint(f\"df_topics value counet for each columns: \\n{df_topics.nunique()}\")","metadata":{"_uuid":"e43486fc-55d9-44ef-bf45-ae1bc39d622a","_cell_guid":"9ae878d8-1dfe-4732-b03c-bcf44afa6241","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load train data by combining correlation table with topic and content tables.","metadata":{"_uuid":"f9e4b193-0963-4041-b3ff-a5fd2047e6e9","_cell_guid":"f2bf1d7e-22f6-40b3-8286-d40fb17085fb","trusted":true}},{"cell_type":"code","source":"def load_train_data(topics):\n    train_df_columns = [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_full\", \"topic_id\",\"content_id\", \"content_description\", \"content_text\" ]\n    \n    correlations[\"content_id\"] = correlations[\"content_ids\"].str.split(\" \")\n    corr = correlations.explode(\"content_id\").drop(columns=[\"content_ids\"])\n\n    corr = corr.merge(df_topics, how=\"left\", on=\"topic_id\")\n    corr = corr.merge(content, how=\"left\", on=\"content_id\")\n\n    train_df = pd.DataFrame(corr[train_df_columns])\n    cols = ['content_title', 'content_description', \"content_text\"]\n    train_df['content_full'] = train_df[cols].apply(lambda row: '\\r\\n'.join(row.values.astype(str)), axis=1)\n    \n    final_train_data = pd.DataFrame(train_df)\n    \n    print(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\n\n    final_train_data.to_csv(Train_Data_File)\n    \n    return final_train_data","metadata":{"_uuid":"aa6fb300-8b83-4984-858e-8db561f69662","_cell_guid":"4a7328ae-e666-43cc-9dbb-69c5cae6b57b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Refresh_Train_Data:\n    print(f\"Refresh_Train_Data ==>>>\")\n    final_train_data = load_train_data(topics)\nelse:\n    print(f\"load final_train_data from {Train_Data_File}\")\n    final_train_data = pd.read_csv(Train_Data_File)","metadata":{"_uuid":"fa25cf81-e52d-4feb-bc97-fbab6257547a","_cell_guid":"869d1831-28be-4181-b406-70ac33e31f85","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File)\nprint(dataset_dict)\n\ndataset = dataset_dict[\"train\"]\n\ndataset = dataset.train_test_split(test_size=0.2)\n\ntrain_dataset = dataset[\"train\"]\n\nkeeped_columns = [\"topic_full\", \"content_full\"]\ncolumns = train_dataset.column_names\ncolumns_to_keep = [\"topic_full\", \"content_full\"]\ncolumns_to_remove = set(columns_to_keep).symmetric_difference(columns)\ntrain_dataset = train_dataset.remove_columns(columns_to_remove)\n\n# train_dataset = train_dataset.shuffle(seed=42).select(range(10000))\n\ntrain_examples = []\neval_examples = []\n\nn_examples = train_dataset.num_rows\n\nfor dt in train_dataset:\n    # print([dt[\"topic_full\"], dt[\"content_full\"]])\n    # train_examples.append(InputExample(texts=[str(dt[\"topic_full\"]), str(dt[\"content_full\"])]))\n    first = str(dt[\"topic_full\"]) if dt[\"topic_full\"] != \"\" else \"\" \n    second = str(dt[\"content_full\"]) if str(dt[\"content_full\"]) != \"\" else \"\" \n    if (first != \"\" and second != \"\"):\n        train_examples.append(InputExample(texts=[first, second]))\n\nprint(f\"train_examples: {len(train_examples)}\")","metadata":{"_uuid":"01860d7a-a0bb-497d-95e7-ba4127462020","_cell_guid":"45b18988-0694-409a-a362-8392ac578d9a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model():\n    \n#     model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n    # if not model:\n    model = SentenceTransformer(\"/kaggle/input/using-fine-tuned-sentencetransformer-env/paraphrase-multilingual-mpnet-base-v2\")\n    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n    # num_epochs = 5\n    num_epochs = 1\n    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n\n    model.to(device)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.fit(train_objectives=[(train_dataloader, train_loss)],\n            epochs=num_epochs,\n            warmup_steps=warmup_steps)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.save(Model_File)\n    print(f\"model saved to {Model_File}\")","metadata":{"_uuid":"28a962d5-b08f-4453-8f18-bc38c9a7a43f","_cell_guid":"dfcb3479-8580-4de4-965d-9d5a71bd7d94","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Train_model:\n    train_model()","metadata":{"_uuid":"fa6491de-a199-473f-ae82-f0c74c7c6b9c","_cell_guid":"b1a61e08-0e64-4adb-9515-d5e6b6663814","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(Model_File)\ntrained_model = AutoModel.from_pretrained(Model_File)\n\ntrained_model.to(device)","metadata":{"_uuid":"dd8da6e6-3f66-4d4c-9dff-bd33d348cb5e","_cell_guid":"4949e5d5-d3bf-4241-bd4d-66c3cd4e6fea","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cls_pooling(model_output):\n    return model_output.last_hidden_state[:, 0]\n\ndef get_embeddings(text_list):\n    encoded_input = tokenizer(\n        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    model_output = trained_model(**encoded_input)\n    return cls_pooling(model_output)","metadata":{"_uuid":"59d86a98-b764-4f62-8396-4074b857d2dd","_cell_guid":"e0dcf436-f863-4be7-9308-eba63c216692","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build embedding based on topic_full and content_full, we also need keep topic_id and content_id.\n\ndef embeddings_gen(row):\n    embedding_row = {}\n    embedding_row[\"embeddings\"]=get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0]\n    embedding_row[\"content_id\"] = str(x[\"content_id\"])\n    embedding_ds_raw.append(embedding_row)\n\n\n\ndef build_embedding():\n\n    # def add_embeddings_topic_full(x):\n    #     y = {}\n    #     y[\"content_id\"] = x[\"content_id\"]\n    #     y[\"embeddings\"]=add_embeddings_topic_full( str(x[\"topic_full\"])).detach().cpu().numpy()[0]\n    #     embedding_list.append\n    #     return x\n\n    # def add_embeddings_content_full(x):\n    #     y = {}\n    #     y[\"content_id\"] = x[\"content_id\"]\n    #     y[\"embeddings\"]=add_embeddings_topic_full( str(x[\"content_full\"])).detach().cpu().numpy()[0]\n    #     embedding_list.append\n    #     return x\n\n    dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File,streaming=True)\n    print(dataset_dict)\n    dataset = dataset_dict[\"train\"]\n\n    def generator(ds):\n        for x in ds:\n            y = {}\n            y[\"content_id\"] = x[\"content_id\"]\n            y[\"embeddings\"]=get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0]\n            yield y\n            z = {}\n            z[\"content_id\"] = x[\"content_id\"]\n            z[\"embeddings\"]=get_embeddings( str(x[\"content_full\"])).detach().cpu().numpy()[0]\n            yield z \n\n    embedding_dataset = Dataset.from_generator(generator, gen_kwargs={\"ds\":dataset})\n    embedding_list = {}\n\n\n\n\n    # print(f\"building embeddings for topic_full\")\n    # dataset.map(add_embeddings_topic_full)\n\n    # print(f\"building embeddings for content_full\")\n    # dataset.map(add_embeddings_topic_full)\n\n    # embedding_dataset =  concatenate_datasets([embeddings_topics_dataset, embeddings_content_dataset])\n    print(f\"embedding_dataset => {embedding_dataset}\")\n    embedding_dataset.save_to_disk(Embeddings_File)","metadata":{"_uuid":"37e706ef-a296-47d2-90ca-8a942e4bd585","_cell_guid":"f1246877-1689-4234-b8f0-30083df974cb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Build_Embedding:\n    print(f\"Build Embedding .... from {Train_Data_File}\")\n    build_embedding()\n\nembedding_dataset = load_from_disk(Embeddings_File)\n\nembedding_dataset.add_faiss_index(column=\"embeddings\")\nprint(f\"finishing loading embedding_dataset\")\nprint(embedding_dataset)","metadata":{"_uuid":"7c9dcef0-a6ab-4c13-a673-e8c2f5555349","_cell_guid":"2c5f4372-fba9-4dc0-9f15-16d2f5adbb58","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cosine_sim(ebd, text_embedding):\n    cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    return cosine_score.item()\n\ndef get_score_topic(text, cosine_cutoff):\n    # There are only two filed in embedding data set: \"embedding\", \"content_id\"\n    text_embedding = get_embeddings(text).cpu().detach().numpy()\n    scores, samples = embedding_dataset.get_nearest_examples(\n                                                    \"embeddings\", text_embedding, k=Nearest_K\n                                                    )\n    # print(scores)\n    samples_df = pd.DataFrame.from_dict(samples)\n    samples_df[\"scores\"] = scores\n    samples_df.sort_values(\"scores\", ascending=True, inplace=True)\n    \n    samples_df['cosine_sim'] = samples_df.apply(lambda row: get_cosine_sim(row[\"embeddings\"], text_embedding), axis=1)\n    samples_df = samples_df[samples_df['cosine_sim'] >= cosine_cutoff] \n\n    scorelist = samples_df[\"scores\"].values.tolist()\n    cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    result_df = samples_df\n\n    return result_df\n\ndef get_score_topic_json(text, cosine_cutoff):\n\n    res_df = get_score_topic(text, cosine_cutoff)\n    res_dict = res_df.to_dict('records')\n    return res_dict","metadata":{"_uuid":"773ca4e6-688e-49b2-a57b-8605754a568d","_cell_guid":"6180b13e-8bfa-4dac-8d1c-48b784143516","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_row(row):\n    res_dict = get_score_topic_json(str(row[\"topic_full\"]), Cosine_Cutoff) \n    # print(f\"calculate_row dict\")\n    # print(res_dict)\n    #scores, topic_ids, content_ids, cosine_sim\n    # result.append([scores, topic_id, topic_title, **evalrow ])\n    row[\"scores\"] = \" \".join([f\"{d['scores']:.10f}\" for d in res_dict])\n    row[\"content_ids\"] = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    row[\"cosine_sim\"] = \" \".join([f\"{d['cosine_sim']:.10f}\" for d in res_dict])\n    return row\n\ndef calculate_score(eval_sampled, res_file):\n    num_rows = eval_sampled.num_rows\n    scores_column = [-2] * num_rows\n    content_ids_column = [\"\"] * num_rows\n    cosine_column = [-1.0] * num_rows\n    eval_sampled = eval_sampled.add_column(\"scores\", scores_column)\n    eval_sampled = eval_sampled.add_column(\"content_ids\", content_ids_column)\n    eval_sampled = eval_sampled.add_column(\"cosine_sim\", cosine_column)\n\n    all_columns = set(eval_sampled.column_names)\n    # keeped_columns = [\"topic_id\",\"topic_full\", \"content_full\",\"content_id\"]\n    keeped_columns = [\"topic_id\",\"scores\", \"cosine_sim\", \"content_ids\"]\n    columns_to_removed = list(all_columns.symmetric_difference(keeped_columns))\n\n    print(f\"calculating scores mapping ...\")\n    eval_sampled = eval_sampled.map(calculate_row)\n    print(f\"result_df value counet for each columns: \\n{eval_sampled.shape}\")\n    eval_sampled.to_csv(res_file) \n    \n\n# Result_file = f\"./data/result_v{dver}.csv\"\n# if Calculate_Score:\n#     eval_sampled =  dataset[\"test\"].shuffle(seed=42).select(range(2000))\n#     calculate_score(eval_sampled, Result_file)\n#     Result_file = f\"./data/result_train_v{dver}.csv\"\n#     eval_sampled =  dataset[\"train\"].shuffle(seed=42).select(range(2000))\n#     calculate_score(eval_sampled, Result_file)\n\n# calc_df = pd.read_csv(Result_file) \n# print(f\"load result file {Result_file}\")\n# print(calc_df.head(100))","metadata":{"_uuid":"c26307e3-17e9-4a79-8717-60559c9baa03","_cell_guid":"55cc9034-7f67-45ca-8618-e1b10b682f84","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(topic_id, df_topics):\n    subset = df_topics.loc[df_topics['topic_id'] == topic_id]\n    text = None\n    for index, r in subset.iterrows():\n            text = r[\"topic_full\"]\n            break\n    if text is None:\n        return \"\"\n\n    res_dict = get_score_topic_json(text, Cosine_Cutoff) \n\n    content_ids = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    return content_ids\n\n\ndef calculate_submission(submission_df):\n\n    submission_df['content_ids'] = submission_df.apply(lambda row: get_neighbors(row[\"topic_id\"], df_topics), axis=1)\n    submission_df.to_csv(DATA_PATH + \"submission.csv\") \n    submission_df.to_csv(Submission_File) \n    print(f\"display submission head\")\n    print(submission_df.head(100))","metadata":{"_uuid":"fb44886e-e223-4106-8586-53d95ec44f9c","_cell_guid":"112c6662-2917-4c18-b74a-1b4335c856bb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (Cal_Submission):\n    print(f\"Calculating submission \")\n    submission_df = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n    calculate_submission(submission_df)\n\n# from sklearn.metrics import fbeta_score\n# fbeta_score(y_true, y_pred, average='macro', beta=2)","metadata":{"_uuid":"1360a7c4-ec1f-4131-b375-cbf3cb9666c2","_cell_guid":"869a78d9-a77d-4a4f-a6e3-a050abe44719","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}