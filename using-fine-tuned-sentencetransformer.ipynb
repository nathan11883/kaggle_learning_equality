{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**The reference:**\n1. https://www.kaggle.com/code/andtaichi/finetunig-sentencetransformer\n2. https://www.kaggle.com/code/quincyqiang/download-huggingface-pretrain-for-kaggle/notebook\n3. https://towardsdatascience.com/easy-kaggle-offline-submission-with-chaining-kernels-30bba5ea5c4d\n4. https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/109679\n5. https://www.kaggle.com/code/jamiealexandre/sample-notebook-data-exploration/notebook\n\nThe pretrained model we use:\nhttps://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n\n**The highlights:**\n1. Join the correlation with topic and content.\n2. Concatenate \"topic_title\" and \"topic_description\" for each topic, and concate all the ancestor topics into \"topic_full\" field.\n3. Concatenate 'content_title', 'content_description', \"content_text\" for each content into \"content_full\" field.\n4. Feed \"topic_full\" and \"content_full\" as embeding pair into the model and fintune the model.\n5. Using the fine-tuned model, generate the embeding for \"topic_full\" and \"content_full\", and put them into embeding dataset.\n6. Create the faiss index by calling add_faiss_index.\n7. For each topic that need to be predict, search in the embeding dataset with faiss index using content_full, find the nearest K content (current value is 20), then filter out the result.\n8. Furhter filter out the result by calculating the cosine_sim between each content and the topic_full, and cutting off using Cosine_Cutoff (current value is larger than 0.99995).\n9. Output the result.","metadata":{}},{"cell_type":"code","source":"# !pip install sentence-transformers\n# !pip install faiss-gpu\n# !pip install faiss-cpu\n# !pip install tqdm\n# !pip install nvidia-ml-py3\n# !pip install accelerate\n# !pip install scikit-learn","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:00:14.628730Z","iopub.execute_input":"2023-03-02T17:00:14.629194Z","iopub.status.idle":"2023-03-02T17:00:14.635991Z","shell.execute_reply.started":"2023-03-02T17:00:14.629155Z","shell.execute_reply":"2023-03-02T17:00:14.634331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh |  bash\n# !apt-get install -y --allow-unauthenticated git-lfs","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:00:14.644510Z","iopub.execute_input":"2023-03-02T17:00:14.644945Z","iopub.status.idle":"2023-03-02T17:00:14.653704Z","shell.execute_reply.started":"2023-03-02T17:00:14.644908Z","shell.execute_reply":"2023-03-02T17:00:14.651493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git lfs install\n# !git clone https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n# # if you want to clone without large files â€“ just their pointers\n# # prepend your git clone with the following env var:\n# !GIT_LFS_SKIP_SMUDGE=1","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:00:14.656170Z","iopub.execute_input":"2023-03-02T17:00:14.657055Z","iopub.status.idle":"2023-03-02T17:00:14.664424Z","shell.execute_reply.started":"2023-03-02T17:00:14.657006Z","shell.execute_reply":"2023-03-02T17:00:14.662876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentence-transformers --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/sentence-transformers-2.2.2.tar.gz --no-index\n!pip install faiss-gpu  --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index\n!pip install faiss-cpu  --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_cpu-1.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  --no-index\n!pip install tqdm --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/tqdm-4.64.1-py2.py3-none-any.whl --no-index\n!pip install nvidia-ml-py3 --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/nvidia-ml-py3-7.352.0.tar.gz --no-index\n!pip install accelerate --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/accelerate-0.16.0-py3-none-any.whl --no-index\n!pip install scikit-learn --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:00:14.667029Z","iopub.execute_input":"2023-03-02T17:00:14.667576Z","iopub.status.idle":"2023-03-02T17:01:52.822760Z","shell.execute_reply.started":"2023-03-02T17:00:14.667522Z","shell.execute_reply":"2023-03-02T17:01:52.821005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sentence_transformers import SentenceTransformer, models, InputExample, losses, util\nfrom datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nimport torch \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:01:52.826386Z","iopub.execute_input":"2023-03-02T17:01:52.826973Z","iopub.status.idle":"2023-03-02T17:02:11.543805Z","shell.execute_reply.started":"2023-03-02T17:01:52.826927Z","shell.execute_reply":"2023-03-02T17:02:11.542283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = SentenceTransformer(\"/kaggle/input/using-fine-tuned-sentencetransformer-env/paraphrase-multilingual-mpnet-base-v2\")","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:02:11.545779Z","iopub.execute_input":"2023-03-02T17:02:11.547096Z","iopub.status.idle":"2023-03-02T17:02:11.556329Z","shell.execute_reply.started":"2023-03-02T17:02:11.547034Z","shell.execute_reply":"2023-03-02T17:02:11.552363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -l","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:02:11.558349Z","iopub.execute_input":"2023-03-02T17:02:11.558795Z","iopub.status.idle":"2023-03-02T17:02:12.691869Z","shell.execute_reply.started":"2023-03-02T17:02:11.558756Z","shell.execute_reply":"2023-03-02T17:02:12.690100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Refresh_Topic = True\nRefresh_Train_Data = True\nTrain_model = True\nBuild_Embedding = True \nCalculate_Score = True \nCal_Submission = True\nCosine_Cutoff = 0.99995\nNearest_K = 20\ndver = 505\n\nTopic_Full_Data_File = f\"/kaggle/working/df_topics_full_{dver}.csv\"\nTrain_Data_File= f\"/kaggle/working/df_train_v{dver}.csv\"\nEmbeddings_File = f'/kaggle/working/embeddings_topics_dataset_v{dver}'\n\nSubmission_File = f\"/kaggle/working/submission_v{dver}.csv\"\nModel_File = f\"/kaggle/working/paraphrase-multilingual-mpnet-base-v{dver}-exp\"\n\nprint(f\"Topic_Full_Data_File {Topic_Full_Data_File}\")\nprint(f\"Train_Data_File {Train_Data_File}\")\nprint(f\"Embeddings_File {Embeddings_File}\")\nprint(f\"Submission_File {Submission_File}\")    ","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:02:12.694320Z","iopub.execute_input":"2023-03-02T17:02:12.694821Z","iopub.status.idle":"2023-03-02T17:02:12.705175Z","shell.execute_reply.started":"2023-03-02T17:02:12.694764Z","shell.execute_reply":"2023-03-02T17:02:12.703696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pd.set_option('display.max_columns', 9)\n# pd.set_option('display.max_rows', 200)\n# pd.set_option('display.min_rows', 10)\n# pd.set_option(\"expand_frame_repr\", True)\n# pd.set_option('display.width', None)\n# pd.set_option('display.max_colwidth', 50)\nDATA_PATH = \"/kaggle/input/learning-equality-curriculum-recommendations/\"\n# DATA_PATH = \"./data/input/learning-equality-curriculum-recommendations/\"\ntopics = pd.read_csv(DATA_PATH + \"topics.csv\")\ncontent = pd.read_csv(DATA_PATH + \"content.csv\")\ncorrelations = pd.read_csv(DATA_PATH + \"correlations.csv\")\n# sample_submission = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n\nprint(f\"DATA_PATH {DATA_PATH}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:02:12.706961Z","iopub.execute_input":"2023-03-02T17:02:12.707480Z","iopub.status.idle":"2023-03-02T17:02:40.863448Z","shell.execute_reply.started":"2023-03-02T17:02:12.707427Z","shell.execute_reply":"2023-03-02T17:02:40.861630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_topics = None\n\nif ( not topics.columns[0].startswith(\"topic_\")):\n    print(f\"renaming topics ...\")\n    topics.rename(columns=lambda x: \"topic_\" + x, inplace=True)\n    content.rename(columns=lambda x: \"content_\" + x, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:02:40.866503Z","iopub.execute_input":"2023-03-02T17:02:40.867093Z","iopub.status.idle":"2023-03-02T17:02:40.879086Z","shell.execute_reply.started":"2023-03-02T17:02:40.867035Z","shell.execute_reply":"2023-03-02T17:02:40.877229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_topic_full(row):\n    topic_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_description = str(row[\"topic_description\"]) if pd.notna(row[\"topic_description\"]) else \"\"\n    topic_full = \"title: \" + topic_title\n    if (topic_description != \"\"):\n        topic_full = topic_full + \"\\r\\n\" + \"description: \" + topic_description\n\n    return topic_full\n\ndef get_parents(df, row):\n    topic_id = row[\"topic_id\"]\n    topic_title_full = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_parent = row[\"topic_parent\"]\n    topic_level = row[\"topic_level\"]\n    topic_full = get_topic_full(row)\n    while not pd.isnull(topic_parent):\n        subset = df.loc[df['topic_id'] == topic_parent]\n        for index, r in subset.iterrows():\n            t_full = get_topic_full(r)\n            topic_full = t_full + \"\\r\\n\" + topic_full \n            t_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n            topic_title_full = t_title + \".\" + topic_title_full \n            topic_parent = r[\"topic_parent\"]\n            topic_level = r[\"topic_level\"]\n            break\n\n    return topic_title_full, topic_full\n\n\ndef refresh_topic(topics):\n\n    df_topics = topics\n    print(f\"Before expand topic full ...\")\n    print(df_topics.head(100))\n\n    topic_title_full = []\n    topic_full = []\n\n#     for index, row in tqdm(df_topics.iterrows(), total=df_topics.shape[0]):\n    for index, row in df_topics.iterrows():\n        t_title_full, t_full = get_parents(df_topics, row)\n        topic_title_full.append(t_title_full)\n        topic_full.append(t_full)\n        if (index % 10000 == 0):\n            print(f\"processing df_topics: \\n{index}, {row}\")\n\n    df_topics['topic_title_full'] = topic_title_full\n    df_topics['topic_full'] = topic_full\n\n    df_topics.to_csv(Topic_Full_Data_File) \n    print(f\"Finished processing df_tpocs, and saved to {Topic_Full_Data_File}\")\n    return df_topics\n\nif (Refresh_Topic):\n    print(\"Freshing topic...\")\n    df_topics = refresh_topic(topics)\nelse:\n    print(f\"Load df_topcis from {Topic_Full_Data_File}\")\n    df_topics = pd.read_csv(Topic_Full_Data_File) ","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:02:40.884033Z","iopub.execute_input":"2023-03-02T17:02:40.884556Z","iopub.status.idle":"2023-03-02T17:02:42.447880Z","shell.execute_reply.started":"2023-03-02T17:02:40.884510Z","shell.execute_reply":"2023-03-02T17:02:42.446332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"After expand topic full ...\")\nprint(df_topics.head(100))\nprint(f\"df_topics value counet for each columns: \\n{df_topics.nunique()}\")","metadata":{"_uuid":"4dd7bfd3-28de-443f-950b-f8935263482b","_cell_guid":"750adcf0-246f-457b-9ec2-b01a07d8f0c1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-02T17:02:42.449690Z","iopub.execute_input":"2023-03-02T17:02:42.450231Z","iopub.status.idle":"2023-03-02T17:02:42.898924Z","shell.execute_reply.started":"2023-03-02T17:02:42.450155Z","shell.execute_reply":"2023-03-02T17:02:42.897461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load train data by combining correlation table with topic and content tables.","metadata":{}},{"cell_type":"code","source":"def load_train_data(topics):\n    train_df_columns = [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_full\", \"topic_id\",\"content_id\", \"content_description\", \"content_text\" ]\n    \n    correlations[\"content_id\"] = correlations[\"content_ids\"].str.split(\" \")\n    corr = correlations.explode(\"content_id\").drop(columns=[\"content_ids\"])\n\n    corr = corr.merge(df_topics, how=\"left\", on=\"topic_id\")\n    corr = corr.merge(content, how=\"left\", on=\"content_id\")\n\n    train_df = pd.DataFrame(corr[train_df_columns])\n    cols = ['content_title', 'content_description', \"content_text\"]\n    train_df['content_full'] = train_df[cols].apply(lambda row: '\\r\\n'.join(row.values.astype(str)), axis=1)\n    \n    final_train_data = pd.DataFrame(train_df)\n    \n    print(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\n\n    final_train_data.to_csv(Train_Data_File)\n    \n    return final_train_data","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:02:42.901860Z","iopub.execute_input":"2023-03-02T17:02:42.902452Z","iopub.status.idle":"2023-03-02T17:02:42.911911Z","shell.execute_reply.started":"2023-03-02T17:02:42.902395Z","shell.execute_reply":"2023-03-02T17:02:42.910511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Refresh_Train_Data:\n    print(f\"Refresh_Train_Data ==>>>\")\n    final_train_data = load_train_data(topics)\nelse:\n    print(f\"load final_train_data from {Train_Data_File}\")\n    final_train_data = pd.read_csv(Train_Data_File)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:02:42.913570Z","iopub.execute_input":"2023-03-02T17:02:42.914321Z","iopub.status.idle":"2023-03-02T17:03:26.456908Z","shell.execute_reply.started":"2023-03-02T17:02:42.914255Z","shell.execute_reply":"2023-03-02T17:03:26.455419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File)\nprint(dataset_dict)\n\ndataset = dataset_dict[\"train\"]\n\ndataset = dataset.train_test_split(test_size=0.2)\n\ntrain_dataset = dataset[\"train\"]\n\nkeeped_columns = [\"topic_full\", \"content_full\"]\ncolumns = train_dataset.column_names\ncolumns_to_keep = [\"topic_full\", \"content_full\"]\ncolumns_to_remove = set(columns_to_keep).symmetric_difference(columns)\ntrain_dataset = train_dataset.remove_columns(columns_to_remove)\n\n# train_dataset = train_dataset.shuffle(seed=42).select(range(10000))\n\ntrain_examples = []\neval_examples = []\n\nn_examples = train_dataset.num_rows\n\nfor dt in train_dataset:\n    # print([dt[\"topic_full\"], dt[\"content_full\"]])\n    # train_examples.append(InputExample(texts=[str(dt[\"topic_full\"]), str(dt[\"content_full\"])]))\n    first = str(dt[\"topic_full\"]) if dt[\"topic_full\"] != \"\" else \"\" \n    second = str(dt[\"content_full\"]) if str(dt[\"content_full\"]) != \"\" else \"\" \n    if (first != \"\" and second != \"\"):\n        train_examples.append(InputExample(texts=[first, second]))\n\nprint(f\"train_examples: {len(train_examples)}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-02T17:21:21.479771Z","iopub.execute_input":"2023-03-02T17:21:21.480582Z","iopub.status.idle":"2023-03-02T17:23:08.822988Z","shell.execute_reply.started":"2023-03-02T17:21:21.480507Z","shell.execute_reply":"2023-03-02T17:23:08.821477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model():\n    \n    model = SentenceTransformer(\"/kaggle/input/using-fine-tuned-sentencetransformer-env/paraphrase-multilingual-mpnet-base-v2\")\n    if not model:\n        model = SentenceTransformer(\"/kaggle/input/using-fine-tuned-sentencetransformer-env/paraphrase-multilingual-mpnet-base-v2\")\n    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)\n    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n    # num_epochs = 5\n    num_epochs = 1\n    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n\n    model.to(device)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.fit(train_objectives=[(train_dataloader, train_loss)],\n            epochs=num_epochs,\n            warmup_steps=warmup_steps)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.save(Model_File)\n    print(f\"model saved to {Model_File}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.982352Z","iopub.status.idle":"2023-03-01T19:48:58.983164Z","shell.execute_reply.started":"2023-03-01T19:48:58.982905Z","shell.execute_reply":"2023-03-01T19:48:58.982931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Train_model:\n    train_model()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.984978Z","iopub.status.idle":"2023-03-01T19:48:58.985484Z","shell.execute_reply.started":"2023-03-01T19:48:58.985229Z","shell.execute_reply":"2023-03-01T19:48:58.985254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(Model_File)\ntrained_model = AutoModel.from_pretrained(Model_File)\n\ntrained_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.987850Z","iopub.status.idle":"2023-03-01T19:48:58.988824Z","shell.execute_reply.started":"2023-03-01T19:48:58.988537Z","shell.execute_reply":"2023-03-01T19:48:58.988570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cls_pooling(model_output):\n    return model_output.last_hidden_state[:, 0]\n\ndef get_embeddings(text_list):\n    encoded_input = tokenizer(\n        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    model_output = trained_model(**encoded_input)\n    return cls_pooling(model_output)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.990516Z","iopub.status.idle":"2023-03-01T19:48:58.991046Z","shell.execute_reply.started":"2023-03-01T19:48:58.990785Z","shell.execute_reply":"2023-03-01T19:48:58.990822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build embedding based on topic_full and content_full, we also need keep topic_id and content_id.\ndef build_embedding(combined_df):\n    all_columns = combined_df.columns\n    keeped_columns = [\"topic_id\",\"topic_full\", \"content_full\",\"content_id\"]\n    removed_columns = all_columns.symmetric_difference(keeped_columns)\n    combined_df.drop(removed_columns, axis=1, inplace = True)\n    print(f\"combined_df ==> {combined_df.head(100)}\")  \n    print(f\"combined_df value counet for each columns: \\n{combined_df.nunique()}\")\n    \n    combined_df.dropna(inplace = True)\n    # combined_df.dropna(subset = [\"topic_title_full\", \"topic_full\"], inplace = True)\n    print(f\"combined_df ==> {combined_df.shape}\")  \n    print(f\"combined_df {combined_df.head(100)}\")\n\n    print(f\"building embeddings for topic_full\")\n    embeddings_topics_dataset = Dataset.from_pandas(combined_df)\n    embeddings_topics_dataset = embeddings_topics_dataset.map(\n        lambda x: {\"embeddings\": get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0]}\n    )\n\n    print(f\"building embeddings for content_full\")\n#     conts = combined_df.copy()\n    \n#     embeddings_content_dataset = Dataset.from_pandas(conts)\n#     embeddings_content_dataset = embeddings_content_dataset.map(\n#         lambda x: {\"embeddings\": get_embeddings( [str(x[\"content_full\"])]).detach().cpu().numpy()[0]}\n#     )\n#     embeeding_dataset =  concatenate_datasets([embeddings_topics_dataset, embeddings_content_dataset])\n\n    embeddings_topics_dataset.save_to_disk(Embeddings_File)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.992897Z","iopub.status.idle":"2023-03-01T19:48:58.993416Z","shell.execute_reply.started":"2023-03-01T19:48:58.993140Z","shell.execute_reply":"2023-03-01T19:48:58.993167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Build_Embedding:\n    print(f\"Build Embedding .... {final_train_data.shape}\")\n    build_embedding(final_train_data)\n\nprint(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\nembeeding_dataset = load_from_disk(Embeddings_File)\n# print(embeeding_dataset)\nembeeding_dataset.add_faiss_index(column=\"embeddings\")","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.995195Z","iopub.status.idle":"2023-03-01T19:48:58.995705Z","shell.execute_reply.started":"2023-03-01T19:48:58.995453Z","shell.execute_reply":"2023-03-01T19:48:58.995478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cosine_sim(ebd, text_embedding):\n    cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    return cosine_score.item()\n\ndef get_score_topic(text, column_list, cosine_cutoff):\n    text_embedding = get_embeddings(text).cpu().detach().numpy()\n    scores, samples = embeeding_dataset.get_nearest_examples(\n                                                    \"embeddings\", text_embedding, k=Nearest_K\n                                                    )\n    # print(scores)\n    samples_df = pd.DataFrame.from_dict(samples)\n    samples_df[\"scores\"] = scores\n    samples_df.sort_values(\"scores\", ascending=True, inplace=True)\n    \n    # embedding_list = samples_df[\"embeddings\"].values.tolist()\n    # cosine_sim_list = []\n    # for ebd in embedding_list:\n    #     cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    #     cosine_sim_list.append(cosine_score.item())\n\n    samples_df['cosine_sim'] = samples_df.apply(lambda row: get_cosine_sim(row[\"embeddings\"], text_embedding), axis=1)\n    samples_df = samples_df[samples_df['cosine_sim'] >= cosine_cutoff] \n\n    scorelist = samples_df[\"scores\"].values.tolist()\n    # scores = \" \".join([f\"{d:.10f}\" for d in scorelist])\n    cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    # cosine_sim = \" \".join([f\"{d:.10f}\" for d in cosine_sim_list])\n    if (column_list is not None):\n        result_df = samples_df[['scores', 'cosine_sim', *column_list]]\n    else:\n        result_df = samples_df\n\n    # content_id_list = samples_df[\"content_id\"].values.tolist()\n    # content_ids = \" \".join(content_id_list)\n    # topic_id_list = samples_df[\"topic_id\"].values.tolist()\n    # topic_ids = \" \".join(topic_id_list)\n\n    # return scores, topic_ids, content_ids, cosine_sim\n    return result_df\n\ndef get_score_topic_json(text, column_list, cosine_cutoff):\n\n    res_df = get_score_topic(text, column_list, cosine_cutoff)\n\n    # scorelist = samples_df[\"scores\"].values.tolist()\n    # # scores = \" \".join([f\"{d:.10f}\" for d in scorelist])\n    # cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    # # cosine_sim = \" \".join([f\"{d:.10f}\" for d in cosine_sim_list])\n    # result_df = samples_df\n\n    # content_id_list = samples_df[\"content_id\"].values.tolist()\n    # content_ids = \" \".join(content_id_list)\n    # topic_id_list = samples_df[\"topic_id\"].values.tolist()\n    # topic_ids = \" \".join(topic_id_list)\n\n    res_dict = res_df.to_dict('records')\n    return res_dict","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.997469Z","iopub.status.idle":"2023-03-01T19:48:58.997984Z","shell.execute_reply.started":"2023-03-01T19:48:58.997718Z","shell.execute_reply":"2023-03-01T19:48:58.997745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_row(row):\n    columns = [\"content_id\", \"topic_id\"]\n    res_dict = get_score_topic_json(str(row[\"topic_full\"]), columns, Cosine_Cutoff) \n    # print(f\"calculate_row dict\")\n    # print(res_dict)\n    #scores, topic_ids, content_ids, cosine_sim\n    # result.append([scores, topic_id, topic_title, **evalrow ])\n    row[\"scores\"] = \" \".join([f\"{d['scores']:.10f}\" for d in res_dict])\n    row[\"topic_ids\"] = \" \".join([f\"{d['topic_id']}\" for d in res_dict])\n    row[\"content_ids\"] = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    row[\"cosine_sim\"] = \" \".join([f\"{d['cosine_sim']:.10f}\" for d in res_dict])\n    return row\n\ndef calculate_score(eval_sampled, res_file):\n    num_rows = eval_sampled.num_rows\n    scores_column = [-2] * num_rows\n    topic_ids_column = [\"\"] * num_rows\n    content_ids_column = [\"\"] * num_rows\n    cosine_column = [-1.0] * num_rows\n    eval_sampled = eval_sampled.add_column(\"scores\", scores_column)\n    eval_sampled = eval_sampled.add_column(\"topic_ids\", topic_ids_column)\n    eval_sampled = eval_sampled.add_column(\"content_ids\", content_ids_column)\n    eval_sampled = eval_sampled.add_column(\"cosine_sim\", cosine_column)\n\n    print(f\"calculating scores mapping ...\")\n    eval_sampled = eval_sampled.map(calculate_row)\n    # print(eval_sampled['scores'][:15])\n    # print(eval_sampled['topic_ids'][:15])\n    result_df = pd.DataFrame(eval_sampled)\n    # print(result_df.head())\n    print(f\"result_df value counet for each columns: \\n{result_df.nunique()}\")\n    all_columns = result_df.columns\n    keeped_columns = [\"topic_id\",\"scores\", \"cosine_sim\", \"topic_ids\",\"content_ids\"]\n    removed_columns = all_columns.symmetric_difference(keeped_columns)\n    result_df.drop(removed_columns, axis=1, inplace=True)\n    result_df.dropna( inplace=True)\n    result_df.to_csv(res_file) \n    # df   [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_description_full\",\"topic_id\",\"content_id\"\n\nResult_file = f\"./data/result_v{dver}.csv\"\nif Calculate_Score:\n    eval_sampled =  dataset[\"test\"].shuffle(seed=42).select(range(2000))\n    calculate_score(eval_sampled, Result_file)\n    Result_file = f\"./data/result_train_v{dver}.csv\"\n    eval_sampled =  dataset[\"train\"].shuffle(seed=42).select(range(2000))\n    calculate_score(eval_sampled, Result_file)\n\ncalc_df = pd.read_csv(Result_file) \n","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:59.000696Z","iopub.status.idle":"2023-03-01T19:48:59.001564Z","shell.execute_reply.started":"2023-03-01T19:48:59.001303Z","shell.execute_reply":"2023-03-01T19:48:59.001330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(topic_id, df_topics):\n    subset = df_topics.loc[df_topics['topic_id'] == topic_id]\n    text = None\n    for index, r in subset.iterrows():\n            text = r[\"topic_full\"]\n            break\n    if text is None:\n        return \"\"\n\n    columns = [\"content_id\", \"topic_id\"]\n    res_dict = get_score_topic_json(text, columns, Cosine_Cutoff) \n\n    content_ids = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    return content_ids\n\n\ndef calculate_submission(submission_df):\n\n    submission_df['content_ids'] = submission_df.apply(lambda row: get_neighbors(row[\"topic_id\"], df_topics), axis=1)\n    submission_df.to_csv(DATA_PATH + \"submission.csv\") \n    submission_df.to_csv(Submission_File) \n    \n    submission_df.head(100)\n    \n","metadata":{"_uuid":"899ed4cf-7420-4063-a41a-ebd2c57a6e54","_cell_guid":"38085bd7-4c24-48ba-9231-f4d199f24c6b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-01T19:48:59.003102Z","iopub.status.idle":"2023-03-01T19:48:59.004005Z","shell.execute_reply.started":"2023-03-01T19:48:59.003731Z","shell.execute_reply":"2023-03-01T19:48:59.003758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (Cal_Submission):\n    print(f\"Calculating submission \")\n    submission_df = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n    calculate_submission(submission_df)\n\n# from sklearn.metrics import fbeta_score\n# fbeta_score(y_true, y_pred, average='macro', beta=2)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:59.005520Z","iopub.status.idle":"2023-03-01T19:48:59.006413Z","shell.execute_reply.started":"2023-03-01T19:48:59.006117Z","shell.execute_reply":"2023-03-01T19:48:59.006147Z"},"trusted":true},"execution_count":null,"outputs":[]}]}