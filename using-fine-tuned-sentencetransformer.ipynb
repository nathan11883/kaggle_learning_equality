{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**The reference:**\n1. https://www.kaggle.com/code/quincyqiang/download-huggingface-pretrain-for-kaggle/notebook\n2. https://towardsdatascience.com/easy-kaggle-offline-submission-with-chaining-kernels-30bba5ea5c4d\n3. https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/109679\n4. https://www.kaggle.com/code/jamiealexandre/sample-notebook-data-exploration/notebook\n\nThe pretrained model we use:\nhttps://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n\n**The highlights:**\n1. Join the correlation with topic and content.\n2. Concatenate \"topic_title\" and \"topic_description\" for each topic, and concate all the ancestor topics into \"topic_full\" field.\n3. Concatenate 'content_title', 'content_description', \"content_text\" for each content into \"content_full\" field.\n4. Feed \"topic_full\" and \"content_full\" as embeding pair into the model and fintune the model.\n5. Using the fine-tuned model, generate the embeding for \"topic_full\" and \"content_full\", and put them into embeding dataset.\n6. Create the faiss index by calling add_faiss_index.\n7. For each topic that need to be predict, search in the embeding dataset with faiss index using content_full, find the nearest K content (current value is 20), then filter out the result.\n8. Furhter filter out the result by calculating the cosine_sim between each content and the topic_full, and cutting off using Cosine_Cutoff (current value is larger than 0.99995).\n9. Output the result.","metadata":{}},{"cell_type":"code","source":"# !pip install sentence-transformers\n# !pip install faiss-gpu\n# !pip install faiss-cpu\n# !pip install tqdm\n# !pip install nvidia-ml-py3\n# !pip install accelerate\n# !pip install scikit-learn","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:18:05.507573Z","iopub.execute_input":"2023-03-01T19:18:05.508715Z","iopub.status.idle":"2023-03-01T19:18:05.514489Z","shell.execute_reply.started":"2023-03-01T19:18:05.508673Z","shell.execute_reply":"2023-03-01T19:18:05.513271Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh |  bash\n!apt-get install -y --allow-unauthenticated git-lfs","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:18:05.516679Z","iopub.execute_input":"2023-03-01T19:18:05.517140Z","iopub.status.idle":"2023-03-01T19:18:29.467136Z","shell.execute_reply.started":"2023-03-01T19:18:05.517100Z","shell.execute_reply":"2023-03-01T19:18:29.465862Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ngit-lfs is already the newest version (3.3.0).\n0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"!git lfs install\n!git clone https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n# if you want to clone without large files – just their pointers\n# prepend your git clone with the following env var:\n!GIT_LFS_SKIP_SMUDGE=1","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:18:29.470061Z","iopub.execute_input":"2023-03-01T19:18:29.470381Z","iopub.status.idle":"2023-03-01T19:18:32.496106Z","shell.execute_reply.started":"2023-03-01T19:18:29.470350Z","shell.execute_reply":"2023-03-01T19:18:32.494733Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Git LFS initialized.\nfatal: destination path 'paraphrase-multilingual-mpnet-base-v2' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sentence-transformers --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/sentence-transformers-2.2.2.tar.gz --no-index\n!pip install faiss-gpu  --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index\n!pip install faiss-cpu  --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_cpu-1.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  --no-index\n!pip install tqdm --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/tqdm-4.64.1-py2.py3-none-any.whl --no-index\n!pip install nvidia-ml-py3 --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/nvidia-ml-py3-7.352.0.tar.gz --no-index\n!pip install accelerate --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/accelerate-0.16.0-py3-none-any.whl --no-index\n!pip install scikit-learn --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:18:32.498194Z","iopub.execute_input":"2023-03-01T19:18:32.498549Z","iopub.status.idle":"2023-03-01T19:19:41.218617Z","shell.execute_reply.started":"2023-03-01T19:18:32.498513Z","shell.execute_reply":"2023-03-01T19:19:41.217253Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/sentence-transformers-2.2.2.tar.gz\nRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.7/site-packages (2.2.2)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.26.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.13.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.14.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.64.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.97)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.12.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: faiss-gpu in /opt/conda/lib/python3.7/site-packages (1.7.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_cpu-1.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: faiss-cpu in /opt/conda/lib/python3.7/site-packages (1.7.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/tqdm-4.64.1-py2.py3-none-any.whl\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/nvidia-ml-py3-7.352.0.tar.gz\nRequirement already satisfied: nvidia-ml-py3 in /opt/conda/lib/python3.7/site-packages (7.352.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/accelerate-0.16.0-py3-none-any.whl\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.7/site-packages (0.12.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (23.0)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.13.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.21.6)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from accelerate) (6.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/using-fine-tuned-sentencetransformer-env/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.21.6)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sentence_transformers import SentenceTransformer, models, InputExample, losses, util\nfrom datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nimport torch \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:19:41.222786Z","iopub.execute_input":"2023-03-01T19:19:41.223269Z","iopub.status.idle":"2023-03-01T19:19:41.231962Z","shell.execute_reply.started":"2023-03-01T19:19:41.223201Z","shell.execute_reply":"2023-03-01T19:19:41.230256Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# model = SentenceTransformer(\"/kaggle/working/paraphrase-multilingual-mpnet-base-v2\")","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:19:41.233916Z","iopub.execute_input":"2023-03-01T19:19:41.234812Z","iopub.status.idle":"2023-03-01T19:19:41.245713Z","shell.execute_reply.started":"2023-03-01T19:19:41.234773Z","shell.execute_reply":"2023-03-01T19:19:41.244653Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"!ls -l","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:19:41.247477Z","iopub.execute_input":"2023-03-01T19:19:41.248084Z","iopub.status.idle":"2023-03-01T19:19:42.242687Z","shell.execute_reply.started":"2023-03-01T19:19:41.248016Z","shell.execute_reply":"2023-03-01T19:19:42.241387Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"total 2696672\n-rw-r--r-- 1 root root   3360718 Mar  1 17:42 Pillow-9.4.0-cp37-cp37m-manylinux_2_28_x86_64.whl\n-rw-r--r-- 1 root root    596265 Mar  1 17:42 PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n---------- 1 root root       263 Mar  1 17:42 __notebook_source__.ipynb\n-rw-r--r-- 1 root root    155255 Mar  1 17:42 certifi-2022.12.7-py3-none-any.whl\n-rw-r--r-- 1 root root    170531 Mar  1 17:42 charset_normalizer-3.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n-rw-r--r-- 1 root root     96588 Mar  1 17:42 click-8.1.3-py3-none-any.whl\n-rw-r--r-- 1 root root      9729 Mar  1 17:42 filelock-3.9.0-py3-none-any.whl\n-rw-r--r-- 1 root root    190265 Mar  1 17:42 huggingface_hub-0.12.1-py3-none-any.whl\n-rw-r--r-- 1 root root     61538 Mar  1 17:42 idna-3.4-py3-none-any.whl\n-rw-r--r-- 1 root root     21859 Mar  1 17:42 importlib_metadata-6.0.0-py3-none-any.whl\n-rw-r--r-- 1 root root    297969 Mar  1 17:42 joblib-1.2.0-py3-none-any.whl\n-rw-r--r-- 1 root root   1510663 Mar  1 17:42 nltk-3.8.1-py3-none-any.whl\n-rw-r--r-- 1 root root  15702369 Mar  1 17:42 numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n-rw-r--r-- 1 root root 317097917 Mar  1 17:42 nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl\n-rw-r--r-- 1 root root  21011023 Mar  1 17:42 nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl\n-rw-r--r-- 1 root root    849253 Mar  1 17:42 nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl\n-rw-r--r-- 1 root root 557141533 Mar  1 17:42 nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl\n-rw-r--r-- 1 root root     42678 Mar  1 17:42 packaging-23.0-py3-none-any.whl\ndrwxr-xr-x 4 root root      4096 Mar  1 19:13 paraphrase-multilingual-mpnet-base-v2\n-rw-r--r-- 1 root root    757070 Mar  1 17:42 regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n-rw-r--r-- 1 root root     62822 Mar  1 17:42 requests-2.28.2-py3-none-any.whl\n-rw-r--r-- 1 root root  24842013 Mar  1 17:42 scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n-rw-r--r-- 1 root root  38121069 Mar  1 17:42 scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n-rw-r--r-- 1 root root     85953 Mar  1 17:42 sentence-transformers-2.2.2.tar.gz\n-rw-r--r-- 1 root root   1276637 Mar  1 17:42 sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n-rw-r--r-- 1 root root   1088291 Mar  1 17:42 setuptools-67.4.0-py3-none-any.whl\n-rw-r--r-- 1 root root 850832418 Mar  1 17:42 state.db\n-rw-r--r-- 1 root root     14928 Mar  1 17:42 threadpoolctl-3.1.0-py3-none-any.whl\n-rw-r--r-- 1 root root   7604027 Mar  1 17:42 tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n-rw-r--r-- 1 root root 887515540 Mar  1 17:43 torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl\n-rw-r--r-- 1 root root  24174641 Mar  1 17:42 torchvision-0.14.1-cp37-cp37m-manylinux1_x86_64.whl\n-rw-r--r-- 1 root root     78468 Mar  1 17:42 tqdm-4.64.1-py2.py3-none-any.whl\n-rw-r--r-- 1 root root   6327182 Mar  1 17:42 transformers-4.26.1-py3-none-any.whl\n-rw-r--r-- 1 root root     27736 Mar  1 17:42 typing_extensions-4.5.0-py3-none-any.whl\n-rw-r--r-- 1 root root    140642 Mar  1 17:42 urllib3-1.26.14-py2.py3-none-any.whl\n-rw-r--r-- 1 root root     36051 Mar  1 17:42 wheel-0.38.4-py3-none-any.whl\n-rw-r--r-- 1 root root      6758 Mar  1 17:42 zipp-3.15.0-py3-none-any.whl\n","output_type":"stream"}]},{"cell_type":"code","source":"Refresh_Topic = True\nRefresh_Train_Data = True\nTrain_model = True\nBuild_Embedding = True \nCalculate_Score = True \nCal_Submission = True\nCosine_Cutoff = 0.99995\nNearest_K = 20\ndver = 505\n\nTopic_Full_Data_File = f\"/kaggle/working/df_topics_full_{dver}.csv\"\nTrain_Data_File= f\"/kaggle/working/df_train_v{dver}.csv\"\nEmbeddings_File = f'/kaggle/working/embeddings_topics_dataset_v{dver}'\n\nSubmission_File = f\"/kaggle/working/submission_v{dver}.csv\"\nModel_File = f\"/kaggle/working/paraphrase-multilingual-mpnet-base-v{dver}-exp\"\n\nprint(f\"Topic_Full_Data_File {Topic_Full_Data_File}\")\nprint(f\"Train_Data_File {Train_Data_File}\")\nprint(f\"Embeddings_File {Embeddings_File}\")\nprint(f\"Submission_File {Submission_File}\")    ","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:19:42.245023Z","iopub.execute_input":"2023-03-01T19:19:42.245454Z","iopub.status.idle":"2023-03-01T19:19:42.253948Z","shell.execute_reply.started":"2023-03-01T19:19:42.245408Z","shell.execute_reply":"2023-03-01T19:19:42.252875Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Topic_Full_Data_File /kaggle/working/df_topics_full_505.csv\nTrain_Data_File /kaggle/working/df_train_v505.csv\nEmbeddings_File /kaggle/working/embeddings_topics_dataset_v505\nSubmission_File /kaggle/working/submission_v505.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# pd.set_option('display.max_columns', 9)\n# pd.set_option('display.max_rows', 200)\n# pd.set_option('display.min_rows', 10)\n# pd.set_option(\"expand_frame_repr\", True)\n# pd.set_option('display.width', None)\n# pd.set_option('display.max_colwidth', 50)\nDATA_PATH = \"/kaggle/input/learning-equality-curriculum-recommendations/\"\n# DATA_PATH = \"./data/input/learning-equality-curriculum-recommendations/\"\ntopics = pd.read_csv(DATA_PATH + \"topics.csv\")\ncontent = pd.read_csv(DATA_PATH + \"content.csv\")\ncorrelations = pd.read_csv(DATA_PATH + \"correlations.csv\")\n# sample_submission = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n\nprint(f\"DATA_PATH {DATA_PATH}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:19:42.255716Z","iopub.execute_input":"2023-03-01T19:19:42.256473Z","iopub.status.idle":"2023-03-01T19:20:02.445426Z","shell.execute_reply.started":"2023-03-01T19:19:42.256436Z","shell.execute_reply":"2023-03-01T19:20:02.444272Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"DATA_PATH /kaggle/input/learning-equality-curriculum-recommendations/\n","output_type":"stream"}]},{"cell_type":"code","source":"df_topics = None\n\nif ( not topics.columns[0].startswith(\"topic_\")):\n    print(f\"renaming topics ...\")\n    topics.rename(columns=lambda x: \"topic_\" + x, inplace=True)\n    content.rename(columns=lambda x: \"content_\" + x, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:20:02.447127Z","iopub.execute_input":"2023-03-01T19:20:02.447533Z","iopub.status.idle":"2023-03-01T19:20:02.458164Z","shell.execute_reply.started":"2023-03-01T19:20:02.447501Z","shell.execute_reply":"2023-03-01T19:20:02.455694Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"renaming topics ...\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_topic_full(row):\n    topic_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_description = str(row[\"topic_description\"]) if pd.notna(row[\"topic_description\"]) else \"\"\n    topic_full = \"title: \" + topic_title\n    if (topic_description != \"\"):\n        topic_full = topic_full + \"\\r\\n\" + \"description: \" + topic_description\n\n    return topic_full\n\ndef get_parents(df, row):\n    topic_id = row[\"topic_id\"]\n    topic_title_full = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_parent = row[\"topic_parent\"]\n    topic_level = row[\"topic_level\"]\n    topic_full = get_topic_full(row)\n    while not pd.isnull(topic_parent):\n        subset = df.loc[df['topic_id'] == topic_parent]\n        for index, r in subset.iterrows():\n            t_full = get_topic_full(r)\n            topic_full = t_full + \"\\r\\n\" + topic_full \n            t_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n            topic_title_full = t_title + \".\" + topic_title_full \n            topic_parent = r[\"topic_parent\"]\n            topic_level = r[\"topic_level\"]\n            break\n\n    return topic_title_full, topic_full\n\n\ndef refresh_topic(topics):\n\n    df_topics = topics\n    print(f\"Before expand topic full ...\")\n    print(df_topics.head())\n\n    topic_title_full = []\n    topic_full = []\n\n#     for index, row in tqdm(df_topics.iterrows(), total=df_topics.shape[0]):\n    for index, row in df_topics.iterrows():\n        t_title_full, t_full = get_parents(df_topics, row)\n        topic_title_full.append(t_title_full)\n        topic_full.append(t_full)\n        if (index % 1000 == 0):\n            print(f\"processing df_topics: {index}, {row}\")\n\n    df_topics['topic_title_full'] = topic_title_full\n    df_topics['topic_full'] = topic_full\n\n    df_topics.to_csv(Topic_Full_Data_File) \n    print(f\"Finished processing df_tpocs, and saved to {Topic_Full_Data_File}\")\n    return df_topics\n\nif (Refresh_Topic):\n    print(\"Freshing topic...\")\n    df_topics = refresh_topic(topics)\nelse:\n    print(f\"Load df_topcis from {Topic_Full_Data_File}\")\n    df_topics = pd.read_csv(Topic_Full_Data_File) ","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:20:02.465153Z","iopub.execute_input":"2023-03-01T19:20:02.465770Z","iopub.status.idle":"2023-03-01T19:47:08.256351Z","shell.execute_reply.started":"2023-03-01T19:20:02.465728Z","shell.execute_reply":"2023-03-01T19:47:08.255258Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Freshing topic...\nBefore expand topic full ...\n         topic_id                                        topic_title  \\\n0  t_00004da3a1b2                         Откриването на резисторите   \n1  t_000095e03056             Unit 3.3 Enlargements and Similarities   \n2  t_00068291e9a4                    Entradas e saídas de uma função   \n3  t_00069b63a70a                                        Transcripts   \n4  t_0006d41a73a8  Графики на експоненциални функции (Алгебра 2 н...   \n\n                                   topic_description topic_channel  \\\n0  Изследване на материали, които предизвикват на...        000cf7   \n1                                                NaN        b3f329   \n2               Entenda um pouco mais sobre funções.        8e286a   \n3                                                NaN        6e3ba4   \n4  Научи повече за графиките на сложните показате...        000cf7   \n\n  topic_category  topic_level topic_language    topic_parent  \\\n0         source            4             bg  t_16e29365b50d   \n1        aligned            2             en  t_aa32fb6252dc   \n2         source            4             pt  t_d14b6c2a2b70   \n3         source            3             en  t_4054df11a74e   \n4         source            4             bg  t_e2452e21d252   \n\n   topic_has_content  \n0               True  \n1              False  \n2               True  \n3               True  \n4               True  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/76972 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36ddc33b96c14340bc8b16b90a1b23ba"}},"metadata":{}}]},{"cell_type":"code","source":"print(f\"After expand topic full ...\")\nprint(df_topics.head())\nprint(f\"df_topics value counet for each columns: \\n{df_topics.nunique()}\")","metadata":{"_uuid":"4dd7bfd3-28de-443f-950b-f8935263482b","_cell_guid":"750adcf0-246f-457b-9ec2-b01a07d8f0c1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-01T19:47:08.258078Z","iopub.execute_input":"2023-03-01T19:47:08.258474Z","iopub.status.idle":"2023-03-01T19:47:08.591063Z","shell.execute_reply.started":"2023-03-01T19:47:08.258435Z","shell.execute_reply":"2023-03-01T19:47:08.589836Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"After expand topic full ...\n         topic_id                                        topic_title  \\\n0  t_00004da3a1b2                         Откриването на резисторите   \n1  t_000095e03056             Unit 3.3 Enlargements and Similarities   \n2  t_00068291e9a4                    Entradas e saídas de uma função   \n3  t_00069b63a70a                                        Transcripts   \n4  t_0006d41a73a8  Графики на експоненциални функции (Алгебра 2 н...   \n\n                                   topic_description topic_channel  \\\n0  Изследване на материали, които предизвикват на...        000cf7   \n1                                                NaN        b3f329   \n2               Entenda um pouco mais sobre funções.        8e286a   \n3                                                NaN        6e3ba4   \n4  Научи повече за графиките на сложните показате...        000cf7   \n\n  topic_category  topic_level topic_language    topic_parent  \\\n0         source            4             bg  t_16e29365b50d   \n1        aligned            2             en  t_aa32fb6252dc   \n2         source            4             pt  t_d14b6c2a2b70   \n3         source            3             en  t_4054df11a74e   \n4         source            4             bg  t_e2452e21d252   \n\n   topic_has_content                                   topic_title_full  \\\n0               True  Откриването на резисторите.Откриването на рези...   \n1              False  Unit 3.3 Enlargements and Similarities.Unit 3....   \n2               True  Entradas e saídas de uma função.Entradas e saí...   \n3               True    Transcripts.Transcripts.Transcripts.Transcripts   \n4               True  Графики на експоненциални функции (Алгебра 2 н...   \n\n                                          topic_full  \n0  title: Khan Academy (български език)\\r\\ntitle:...  \n1  title: Ghana JHS Curriculum (in progress)\\r\\nt...  \n2  title: Khan Academy (Português (Brasil))\\r\\nti...  \n3  title: MIT Blossoms\\r\\ntitle: Engineering\\r\\nd...  \n4  title: Khan Academy (български език)\\r\\ntitle:...  \ndf_topics value counet for each columns: \ntopic_id             76972\ntopic_title          45082\ntopic_description    23067\ntopic_channel          171\ntopic_category           3\ntopic_level             11\ntopic_language          28\ntopic_parent         17512\ntopic_has_content        2\ntopic_title_full     50086\ntopic_full           76823\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Load train data by combining correlation table with topic and content tables.","metadata":{}},{"cell_type":"code","source":"def load_train_data(topics):\n    train_df_columns = [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_full\", \"topic_id\",\"content_id\", \"content_description\", \"content_text\" ]\n    \n    correlations[\"content_id\"] = correlations[\"content_ids\"].str.split(\" \")\n    corr = correlations.explode(\"content_id\").drop(columns=[\"content_ids\"])\n\n    corr = corr.merge(df_topics, how=\"left\", on=\"topic_id\")\n    corr = corr.merge(content, how=\"left\", on=\"content_id\")\n\n    train_df = pd.DataFrame(corr[train_df_columns])\n    cols = ['content_title', 'content_description', \"content_text\"]\n    train_df['content_full'] = train_df[cols].apply(lambda row: '\\r\\n'.join(row.values.astype(str)), axis=1)\n    \n    final_train_data = pd.DataFrame(train_df)\n    \n    print(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\n\n    final_train_data.to_csv(Train_Data_File)\n    \n    return final_train_data","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:47:08.594043Z","iopub.execute_input":"2023-03-01T19:47:08.594398Z","iopub.status.idle":"2023-03-01T19:47:08.602272Z","shell.execute_reply.started":"2023-03-01T19:47:08.594358Z","shell.execute_reply":"2023-03-01T19:47:08.601114Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"if Refresh_Train_Data:\n    print(f\"Refresh_Train_Data ==>>>\")\n    final_train_data = load_train_data(topics)\nelse:\n    print(f\"load final_train_data from {Train_Data_File}\")\n    final_train_data = pd.read_csv(Train_Data_File)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:47:08.603578Z","iopub.execute_input":"2023-03-01T19:47:08.604627Z","iopub.status.idle":"2023-03-01T19:48:13.879495Z","shell.execute_reply.started":"2023-03-01T19:47:08.604590Z","shell.execute_reply":"2023-03-01T19:48:13.878353Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Refresh_Train_Data ==>>>\nfinal_train_data value counet for each columns: \ntopic_title             36597\ncontent_title          130937\ntopic_title_full        39942\ntopic_full              61394\ntopic_id                61517\ncontent_id             154047\ncontent_description     76305\ncontent_text            70687\ncontent_full           152701\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File)\nprint(dataset_dict)\n\ndataset = dataset_dict[\"train\"]\n\ndataset = dataset.train_test_split(test_size=0.2)\n\ntrain_dataset = dataset[\"train\"]\n\nkeeped_columns = [\"topic_full\", \"content_full\"]\ncolumns = train_dataset.column_names\ncolumns_to_keep = [\"topic_full\", \"content_full\"]\ncolumns_to_remove = set(columns_to_keep).symmetric_difference(columns)\ntrain_dataset = train_dataset.remove_columns(columns_to_remove)\n\n# train_dataset = train_dataset.shuffle(seed=42).select(range(10000))\n\ntrain_examples = []\neval_examples = []\n\nn_examples = train_dataset.num_rows","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:13.881043Z","iopub.execute_input":"2023-03-01T19:48:13.881431Z","iopub.status.idle":"2023-03-01T19:48:58.978327Z","shell.execute_reply.started":"2023-03-01T19:48:13.881392Z","shell.execute_reply":"2023-03-01T19:48:58.976518Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-2e8129281e246200/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1067c9550e2642d3aa944ae8ad0cc827"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53f55d886d40413dac996bc39c6ddcf0"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/2126015266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrain_Data_File\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1694\u001b[0m         \u001b[0mignore_verifications\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_verifications\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m         \u001b[0mtry_from_hf_gcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_from_hf_gcs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m         \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m     )\n\u001b[1;32m   1698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdownloaded_from_gcs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                         self._download_and_prepare(\n\u001b[0;32m--> 606\u001b[0;31m                             \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                         )\n\u001b[1;32m    608\u001b[0m                     \u001b[0;31m# Sync info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m                 \u001b[0;31m# Prepare split will record examples associated to the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m                 raise OSError(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[0;34m(self, split_generator)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             for key, table in logging.tqdm(\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" tables\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m  \u001b[0;31m# not logging.is_progress_bar_enabled()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             ):\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;31m# (note: keep this check outside the loop for performance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/packaged_modules/csv/csv.py\u001b[0m in \u001b[0;36m_generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mcsv_file_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                     \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0;31m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."],"ename":"ParserError","evalue":"Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.","output_type":"error"}]},{"cell_type":"code","source":"for dt in train_dataset:\n    # print([dt[\"topic_full\"], dt[\"content_full\"]])\n    # train_examples.append(InputExample(texts=[str(dt[\"topic_full\"]), str(dt[\"content_full\"])]))\n    first = str(dt[\"topic_full\"]) if dt[\"topic_full\"] != \"\" else \"\" \n    second = str(dt[\"content_full\"]) if str(dt[\"content_full\"]) != \"\" else \"\" \n    if (first != \"\" and second != \"\"):\n        train_examples.append(InputExample(texts=[first, second]))\n\nprint(f\"train_examples: {len(train_examples)}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.979729Z","iopub.status.idle":"2023-03-01T19:48:58.980238Z","shell.execute_reply.started":"2023-03-01T19:48:58.979974Z","shell.execute_reply":"2023-03-01T19:48:58.980000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model():\n\n    model = SentenceTransformer(\"/kaggle/working/paraphrase-multilingual-mpnet-base-v2\")\n\n    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n    # num_epochs = 5\n    num_epochs = 1\n    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n\n    model.to(device)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.fit(train_objectives=[(train_dataloader, train_loss)],\n            epochs=num_epochs,\n            warmup_steps=warmup_steps)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.save(Model_File)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.982352Z","iopub.status.idle":"2023-03-01T19:48:58.983164Z","shell.execute_reply.started":"2023-03-01T19:48:58.982905Z","shell.execute_reply":"2023-03-01T19:48:58.982931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Train_model:\n    train_model()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.984978Z","iopub.status.idle":"2023-03-01T19:48:58.985484Z","shell.execute_reply.started":"2023-03-01T19:48:58.985229Z","shell.execute_reply":"2023-03-01T19:48:58.985254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(Model_File)\ntrained_model = AutoModel.from_pretrained(Model_File)\n\ntrained_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.987850Z","iopub.status.idle":"2023-03-01T19:48:58.988824Z","shell.execute_reply.started":"2023-03-01T19:48:58.988537Z","shell.execute_reply":"2023-03-01T19:48:58.988570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cls_pooling(model_output):\n    return model_output.last_hidden_state[:, 0]\n\ndef get_embeddings(text_list):\n    encoded_input = tokenizer(\n        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    model_output = trained_model(**encoded_input)\n    return cls_pooling(model_output)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.990516Z","iopub.status.idle":"2023-03-01T19:48:58.991046Z","shell.execute_reply.started":"2023-03-01T19:48:58.990785Z","shell.execute_reply":"2023-03-01T19:48:58.990822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build embedding based on topic_full and content_full, we also need keep topic_id and content_id.\ndef build_embedding(combined_df):\n    all_columns = combined_df.columns\n    keeped_columns = [\"topic_id\",\"topic_full\", \"content_full\",\"content_id\"]\n    removed_columns = all_columns.symmetric_difference(keeped_columns)\n    combined_df.drop(removed_columns, axis=1, inplace = True)\n    print(f\"combined_df ==> {combined_df.head()}\")  \n    print(f\"combined_df value counet for each columns: \\n{combined_df.nunique()}\")\n    \n    combined_df.dropna(inplace = True)\n    # combined_df.dropna(subset = [\"topic_title_full\", \"topic_full\"], inplace = True)\n    print(f\"combined_df ==> {combined_df.shape}\")  \n    print(f\"combined_df {combined_df.head()}\")\n\n    print(f\"building embeddings for topic_full\")\n    embeddings_topics_dataset = Dataset.from_pandas(combined_df)\n    embeddings_topics_dataset = embeddings_topics_dataset.map(\n        lambda x: {\"embeddings\": get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0]}\n    )\n\n    print(f\"building embeddings for content_full\")\n    conts = combined_df.copy()\n    \n    embeddings_content_dataset = Dataset.from_pandas(conts)\n    embeddings_content_dataset = embeddings_content_dataset.map(\n        lambda x: {\"embeddings\": get_embeddings( [str(x[\"content_full\"])]).detach().cpu().numpy()[0]}\n    )\n    embeeding_dataset =  concatenate_datasets([embeddings_topics_dataset, embeddings_content_dataset])\n\n    embeeding_dataset.save_to_disk(Embeddings_File)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.992897Z","iopub.status.idle":"2023-03-01T19:48:58.993416Z","shell.execute_reply.started":"2023-03-01T19:48:58.993140Z","shell.execute_reply":"2023-03-01T19:48:58.993167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Build_Embedding:\n    print(f\"Build Embedding .... {final_train_data.shape}\")\n    build_embedding(final_train_data)\n\nprint(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\nembeeding_dataset = load_from_disk(Embeddings_File)\nprint(embeeding_dataset)\nembeeding_dataset.add_faiss_index(column=\"embeddings\")","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.995195Z","iopub.status.idle":"2023-03-01T19:48:58.995705Z","shell.execute_reply.started":"2023-03-01T19:48:58.995453Z","shell.execute_reply":"2023-03-01T19:48:58.995478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cosine_sim(ebd, text_embedding):\n    cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    return cosine_score.item()\n\ndef get_score_topic(text, column_list, cosine_cutoff):\n    text_embedding = get_embeddings(text).cpu().detach().numpy()\n    scores, samples = embeeding_dataset.get_nearest_examples(\n                                                    \"embeddings\", text_embedding, k=Nearest_K\n                                                    )\n    # print(scores)\n    samples_df = pd.DataFrame.from_dict(samples)\n    samples_df[\"scores\"] = scores\n    samples_df.sort_values(\"scores\", ascending=True, inplace=True)\n    \n    # embedding_list = samples_df[\"embeddings\"].values.tolist()\n    # cosine_sim_list = []\n    # for ebd in embedding_list:\n    #     cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    #     cosine_sim_list.append(cosine_score.item())\n\n    samples_df['cosine_sim'] = samples_df.apply(lambda row: get_cosine_sim(row[\"embeddings\"], text_embedding), axis=1)\n    samples_df = samples_df[samples_df['cosine_sim'] >= cosine_cutoff] \n\n    scorelist = samples_df[\"scores\"].values.tolist()\n    # scores = \" \".join([f\"{d:.10f}\" for d in scorelist])\n    cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    # cosine_sim = \" \".join([f\"{d:.10f}\" for d in cosine_sim_list])\n    if (column_list is not None):\n        result_df = samples_df[['scores', 'cosine_sim', *column_list]]\n    else:\n        result_df = samples_df\n\n    # content_id_list = samples_df[\"content_id\"].values.tolist()\n    # content_ids = \" \".join(content_id_list)\n    # topic_id_list = samples_df[\"topic_id\"].values.tolist()\n    # topic_ids = \" \".join(topic_id_list)\n\n    # return scores, topic_ids, content_ids, cosine_sim\n    return result_df\n\ndef get_score_topic_json(text, column_list, cosine_cutoff):\n\n    res_df = get_score_topic(text, column_list, cosine_cutoff)\n\n    # scorelist = samples_df[\"scores\"].values.tolist()\n    # # scores = \" \".join([f\"{d:.10f}\" for d in scorelist])\n    # cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    # # cosine_sim = \" \".join([f\"{d:.10f}\" for d in cosine_sim_list])\n    # result_df = samples_df\n\n    # content_id_list = samples_df[\"content_id\"].values.tolist()\n    # content_ids = \" \".join(content_id_list)\n    # topic_id_list = samples_df[\"topic_id\"].values.tolist()\n    # topic_ids = \" \".join(topic_id_list)\n\n    res_dict = res_df.to_dict('records')\n    return res_dict","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:58.997469Z","iopub.status.idle":"2023-03-01T19:48:58.997984Z","shell.execute_reply.started":"2023-03-01T19:48:58.997718Z","shell.execute_reply":"2023-03-01T19:48:58.997745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_row(row):\n    columns = [\"content_id\", \"topic_id\"]\n    res_dict = get_score_topic_json(str(row[\"topic_full\"]), columns, Cosine_Cutoff) \n    # print(f\"calculate_row dict\")\n    # print(res_dict)\n    #scores, topic_ids, content_ids, cosine_sim\n    # result.append([scores, topic_id, topic_title, **evalrow ])\n    row[\"scores\"] = \" \".join([f\"{d['scores']:.10f}\" for d in res_dict])\n    row[\"topic_ids\"] = \" \".join([f\"{d['topic_id']}\" for d in res_dict])\n    row[\"content_ids\"] = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    row[\"cosine_sim\"] = \" \".join([f\"{d['cosine_sim']:.10f}\" for d in res_dict])\n    return row\n\ndef calculate_score(eval_sampled, res_file):\n    num_rows = eval_sampled.num_rows\n    scores_column = [-2] * num_rows\n    topic_ids_column = [\"\"] * num_rows\n    content_ids_column = [\"\"] * num_rows\n    cosine_column = [-1.0] * num_rows\n    eval_sampled = eval_sampled.add_column(\"scores\", scores_column)\n    eval_sampled = eval_sampled.add_column(\"topic_ids\", topic_ids_column)\n    eval_sampled = eval_sampled.add_column(\"content_ids\", content_ids_column)\n    eval_sampled = eval_sampled.add_column(\"cosine_sim\", cosine_column)\n\n    print(f\"calculating scores mapping ...\")\n    eval_sampled = eval_sampled.map(calculate_row)\n    # print(eval_sampled['scores'][:15])\n    # print(eval_sampled['topic_ids'][:15])\n    result_df = pd.DataFrame(eval_sampled)\n    # print(result_df.head())\n    print(f\"result_df value counet for each columns: \\n{result_df.nunique()}\")\n    all_columns = result_df.columns\n    keeped_columns = [\"topic_id\",\"scores\", \"cosine_sim\", \"topic_ids\",\"content_ids\"]\n    removed_columns = all_columns.symmetric_difference(keeped_columns)\n    result_df.drop(removed_columns, axis=1, inplace=True)\n    result_df.dropna( inplace=True)\n    result_df.to_csv(res_file) \n    # df   [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_description_full\",\"topic_id\",\"content_id\"\n\nResult_file = f\"./data/result_v{dver}.csv\"\nif Calculate_Score:\n    eval_sampled =  dataset[\"test\"].shuffle(seed=42).select(range(2000))\n    calculate_score(eval_sampled, Result_file)\n    Result_file = f\"./data/result_train_v{dver}.csv\"\n    eval_sampled =  dataset[\"train\"].shuffle(seed=42).select(range(2000))\n    calculate_score(eval_sampled, Result_file)\n\ncalc_df = pd.read_csv(Result_file) \n","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:59.000696Z","iopub.status.idle":"2023-03-01T19:48:59.001564Z","shell.execute_reply.started":"2023-03-01T19:48:59.001303Z","shell.execute_reply":"2023-03-01T19:48:59.001330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(topic_id, df_topics):\n    subset = df_topics.loc[df_topics['topic_id'] == topic_id]\n    text = None\n    for index, r in subset.iterrows():\n            text = r[\"topic_full\"]\n            break\n    if text is None:\n        return \"\"\n\n    columns = [\"content_id\", \"topic_id\"]\n    res_dict = get_score_topic_json(text, columns, Cosine_Cutoff) \n\n    content_ids = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    return content_ids\n\n\ndef calculate_submission(submission_df):\n\n    submission_df['content_ids'] = submission_df.apply(lambda row: get_neighbors(row[\"topic_id\"], df_topics), axis=1)\n    submission_df.to_csv(DATA_PATH + \"submission.csv\") \n    submission_df.to_csv(Submission_File) \n    \n    submission_df.head()\n    \n","metadata":{"_uuid":"899ed4cf-7420-4063-a41a-ebd2c57a6e54","_cell_guid":"38085bd7-4c24-48ba-9231-f4d199f24c6b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-03-01T19:48:59.003102Z","iopub.status.idle":"2023-03-01T19:48:59.004005Z","shell.execute_reply.started":"2023-03-01T19:48:59.003731Z","shell.execute_reply":"2023-03-01T19:48:59.003758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (Cal_Submission):\n    print(f\"Calculating submission \")\n    submission_df = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n    calculate_submission(submission_df)\n\n# from sklearn.metrics import fbeta_score\n# fbeta_score(y_true, y_pred, average='macro', beta=2)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T19:48:59.005520Z","iopub.status.idle":"2023-03-01T19:48:59.006413Z","shell.execute_reply.started":"2023-03-01T19:48:59.006117Z","shell.execute_reply":"2023-03-01T19:48:59.006147Z"},"trusted":true},"execution_count":null,"outputs":[]}]}