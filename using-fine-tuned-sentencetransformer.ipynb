{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers\n!pip install faiss-gpu\n!pip install faiss-cpu\n!pip install tqdm\n!pip install nvidia-ml-py3\n!pip install accelerate\n!pip install scikit-learn","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:11:48.059588Z","iopub.status.idle":"2023-02-28T22:11:48.059974Z","shell.execute_reply.started":"2023-02-28T22:11:48.059791Z","shell.execute_reply":"2023-02-28T22:11:48.059809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sentence_transformers import SentenceTransformer, models, InputExample, losses, util\nfrom datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nimport torch \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:11:48.062417Z","iopub.status.idle":"2023-02-28T22:11:48.063316Z","shell.execute_reply.started":"2023-02-28T22:11:48.063030Z","shell.execute_reply":"2023-02-28T22:11:48.063057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -l","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:15:09.780249Z","iopub.execute_input":"2023-02-28T22:15:09.780899Z","iopub.status.idle":"2023-02-28T22:15:10.845833Z","shell.execute_reply.started":"2023-02-28T22:15:09.780863Z","shell.execute_reply":"2023-02-28T22:15:10.844513Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"total 4\n---------- 1 root root 263 Feb 28 22:11 __notebook_source__.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"Refresh_Topic = True\nRefresh_Train_Data = True\nTrain_model = True\nBuild_Embedding = True \nCalculate_Score = True \nCal_Submission = True\nCosine_Cutoff = 0.99995\ndver = 502\n\nTopic_Full_Data_File = f\"/kaggle/working/df_topics_full_{dver}.csv\"\nTrain_Data_File= f\"/kaggle/working/df_train_v{dver}.csv\"\nEmbeddings_File = f'/kaggle/working/embeddings_topics_dataset_v{dver}'\n\nSubmission_File = f\"/kaggle/working/submission_v{dver}.csv\"\nModel_File = f\"/kaggle/working/paraphrase-multilingual-mpnet-base-v{dver}-exp\"\n","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:17:24.996272Z","iopub.execute_input":"2023-02-28T22:17:24.996705Z","iopub.status.idle":"2023-02-28T22:17:25.003658Z","shell.execute_reply.started":"2023-02-28T22:17:24.996672Z","shell.execute_reply":"2023-02-28T22:17:25.002246Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# pd.set_option('display.max_columns', 9)\n# pd.set_option('display.max_rows', 200)\n# pd.set_option('display.min_rows', 10)\n# pd.set_option(\"expand_frame_repr\", True)\n# pd.set_option('display.width', None)\n# pd.set_option('display.max_colwidth', 50)\nDATA_PATH = \"/kaggle/input/learning-equality-curriculum-recommendations/\"\n# DATA_PATH = \"./data/input/learning-equality-curriculum-recommendations/\"\ntopics = pd.read_csv(DATA_PATH + \"topics.csv\")\ncontent = pd.read_csv(DATA_PATH + \"content.csv\")\ncorrelations = pd.read_csv(DATA_PATH + \"correlations.csv\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:17:29.448349Z","iopub.execute_input":"2023-02-28T22:17:29.449102Z","iopub.status.idle":"2023-02-28T22:17:41.115458Z","shell.execute_reply.started":"2023-02-28T22:17:29.449063Z","shell.execute_reply":"2023-02-28T22:17:41.114305Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_topics = None\n\nif ( not topics.columns[0].startswith(\"topic_\")):\n    print(f\"renaming topics ...\")\n    topics.rename(columns=lambda x: \"topic_\" + x, inplace=True)\n    content.rename(columns=lambda x: \"content_\" + x, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:17:41.118659Z","iopub.execute_input":"2023-02-28T22:17:41.119035Z","iopub.status.idle":"2023-02-28T22:17:41.126351Z","shell.execute_reply.started":"2023-02-28T22:17:41.118998Z","shell.execute_reply":"2023-02-28T22:17:41.125168Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"renaming topics ...\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_topic_full(row):\n    topic_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_description = str(row[\"topic_description\"]) if pd.notna(row[\"topic_description\"]) else \"\"\n    topic_full = \"title: \" + topic_title\n    if (topic_description != \"\"):\n        topic_full = topic_full + \"\\r\\n\" + \"description: \" + topic_description\n\n    return topic_full\n\ndef get_parents(df, row):\n    topic_id = row[\"topic_id\"]\n    topic_title_full = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_parent = row[\"topic_parent\"]\n    topic_level = row[\"topic_level\"]\n    topic_full = get_topic_full(row)\n    while not pd.isnull(topic_parent):\n        subset = df.loc[df['topic_id'] == topic_parent]\n        for index, r in subset.iterrows():\n            t_full = get_topic_full(r)\n            topic_full = t_full + \"\\r\\n\" + topic_full \n            t_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n            topic_title_full = t_title + \".\" + topic_title_full \n            topic_parent = r[\"topic_parent\"]\n            topic_level = r[\"topic_level\"]\n            break\n\n    return topic_title_full, topic_full\n\n\ndef refresh_topic(topics):\n\n    df_topics = topics\n    print(f\"Before expand topic full ...\")\n    print(df_topics.head())\n\n    topic_title_full = []\n    topic_full = []\n\n\n    for index, row in tqdm(df_topics.iterrows(), total=df_topics.shape[0]):\n        t_title_full, t_full = get_parents(df_topics, row)\n        topic_title_full.append(t_title_full)\n        topic_full.append(t_full)\n\n    df_topics['topic_title_full'] = topic_title_full\n    df_topics['topic_full'] = topic_full\n\n    df_topics.to_csv(Topic_Full_Data_File) \n    return df_topics\n\nif (Refresh_Topic):\n    print(\"Freshing topic...\")\n    df_topics = refresh_topic(topics)\nelse:\n    df_topics = pd.read_csv(Topic_Full_Data_File) ","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:17:41.128338Z","iopub.execute_input":"2023-02-28T22:17:41.129343Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Freshing topic...\nBefore expand topic full ...\n         topic_id                                        topic_title  \\\n0  t_00004da3a1b2                         Откриването на резисторите   \n1  t_000095e03056             Unit 3.3 Enlargements and Similarities   \n2  t_00068291e9a4                    Entradas e saídas de uma função   \n3  t_00069b63a70a                                        Transcripts   \n4  t_0006d41a73a8  Графики на експоненциални функции (Алгебра 2 н...   \n\n                                   topic_description topic_channel  \\\n0  Изследване на материали, които предизвикват на...        000cf7   \n1                                                NaN        b3f329   \n2               Entenda um pouco mais sobre funções.        8e286a   \n3                                                NaN        6e3ba4   \n4  Научи повече за графиките на сложните показате...        000cf7   \n\n  topic_category  topic_level topic_language    topic_parent  \\\n0         source            4             bg  t_16e29365b50d   \n1        aligned            2             en  t_aa32fb6252dc   \n2         source            4             pt  t_d14b6c2a2b70   \n3         source            3             en  t_4054df11a74e   \n4         source            4             bg  t_e2452e21d252   \n\n   topic_has_content  \n0               True  \n1              False  \n2               True  \n3               True  \n4               True  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/76972 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ad125206f1b42df8edb3040a8593fcd"}},"metadata":{}}]},{"cell_type":"code","source":"print(f\"After expand topic full ...\")\nprint(df_topics.head())\nprint(f\"df_topics value counet for each columns: \\n{df_topics.nunique()}\")\n\n","metadata":{"_uuid":"4dd7bfd3-28de-443f-950b-f8935263482b","_cell_guid":"750adcf0-246f-457b-9ec2-b01a07d8f0c1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_train_data(topics):\n    train_df_columns = [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_full\", \"topic_id\",\"content_id\", \"content_description\", \"content_text\" ]\n    \n    correlations[\"content_id\"] = correlations[\"content_ids\"].str.split(\" \")\n    corr = correlations.explode(\"content_id\").drop(columns=[\"content_ids\"])\n\n    corr = corr.merge(df_topics, how=\"left\", on=\"topic_id\")\n    corr = corr.merge(content, how=\"left\", on=\"content_id\")\n\n\n    train_df = pd.DataFrame(corr[train_df_columns])\n    cols = ['content_title', 'content_description', \"content_text\"]\n    train_df['content_full'] = train_df[cols].apply(lambda row: '\\r\\n'.join(row.values.astype(str)), axis=1)\n    \n    final_train_data = pd.DataFrame(train_df)\n    \n    print(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\n\n    final_train_data.to_csv(Train_Data_File)\n    \n    return final_train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Refresh_Train_Data:\n    print(f\"Refresh_Train_Data ==>>>\")\n    final_train_data = load_train_data(topics)\nelse:\n    print(f\"load final_train_data from {Train_Data_File}\")\n    final_train_data = pd.read_csv(Train_Data_File)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File)\nprint(dataset_dict)\n\ndataset = dataset_dict[\"train\"]\n\ndataset = dataset.train_test_split(test_size=0.2)\n\ntrain_dataset = dataset[\"train\"]\n\nkeeped_columns = [\"topic_full\", \"content_full\"]\ncolumns = train_dataset.column_names\ncolumns_to_keep = [\"topic_full\", \"content_full\"]\ncolumns_to_remove = set(columns_to_keep).symmetric_difference(columns)\ntrain_dataset = train_dataset.remove_columns(columns_to_remove)\n\n# train_dataset = train_dataset.shuffle(seed=42).select(range(10000))\n\ntrain_examples = []\neval_examples = []\n\nn_examples = train_dataset.num_rows","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dt in train_dataset:\n    # print([dt[\"topic_full\"], dt[\"content_full\"]])\n    # train_examples.append(InputExample(texts=[str(dt[\"topic_full\"]), str(dt[\"content_full\"])]))\n    first = str(dt[\"topic_full\"]) if dt[\"topic_full\"] != \"\" else \"\" \n    second = str(dt[\"content_full\"]) if str(dt[\"content_full\"]) != \"\" else \"\" \n    if (first != \"\" and second != \"\"):\n        train_examples.append(InputExample(texts=[first, second]))\n\nprint(f\"train_examples: {len(train_examples)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model():\n\n    model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n\n    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)\n    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n    # num_epochs = 5\n    num_epochs = 1\n    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n\n    model.to(device)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.fit(train_objectives=[(train_dataloader, train_loss)],\n            epochs=num_epochs,\n            warmup_steps=warmup_steps)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.save(Model_File)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Train_model:\n    train_model()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(Model_File)\ntrained_model = AutoModel.from_pretrained(Model_File)\n\ntrained_model.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cls_pooling(model_output):\n    return model_output.last_hidden_state[:, 0]\n\ndef get_embeddings(text_list):\n    encoded_input = tokenizer(\n        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    model_output = trained_model(**encoded_input)\n    return cls_pooling(model_output)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build embedding based on topic_full and content_full, we also need keep topic_id and content_id.\ndef build_embedding(combined_df):\n    all_columns = combined_df.columns\n    keeped_columns = [\"topic_id\",\"topic_full\", \"content_full\",\"content_id\"]\n    removed_columns = all_columns.symmetric_difference(keeped_columns)\n    combined_df.drop(removed_columns, axis=1, inplace = True)\n    print(f\"combined_df ==> {combined_df.head()}\")  \n    print(f\"combined_df value counet for each columns: \\n{combined_df.nunique()}\")\n    \n    combined_df.dropna(inplace = True)\n    # combined_df.dropna(subset = [\"topic_title_full\", \"topic_full\"], inplace = True)\n    print(f\"combined_df ==> {combined_df.shape}\")  \n    print(f\"combined_df {combined_df.head()}\")\n\n    print(f\"building embeddings for topic_full\")\n    embeddings_topics_dataset = Dataset.from_pandas(combined_df)\n    embeddings_topics_dataset = embeddings_topics_dataset.map(\n        lambda x: {\"embeddings\": get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0]}\n    )\n\n    print(f\"building embeddings for content_full\")\n    conts = combined_df.copy()\n    \n    embeddings_content_dataset = Dataset.from_pandas(conts)\n    embeddings_content_dataset = embeddings_content_dataset.map(\n        lambda x: {\"embeddings\": get_embeddings( [str(x[\"content_full\"])]).detach().cpu().numpy()[0]}\n    )\n    embeeding_dataset =  concatenate_datasets([embeddings_topics_dataset, embeddings_content_dataset])\n\n    embeeding_dataset.save_to_disk(Embeddings_File)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Build_Embedding:\n    print(f\"Build Embedding .... {final_train_data.shape}\")\n    build_embedding(final_train_data)\n\nprint(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\nembeeding_dataset = load_from_disk(Embeddings_File)\nprint(embeeding_dataset)\nembeeding_dataset.add_faiss_index(column=\"embeddings\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cosine_sim(ebd, text_embedding):\n    cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    return cosine_score.item()\n\ndef get_score_topic(text, column_list, cosine_cutoff):\n    text_embedding = get_embeddings(text).cpu().detach().numpy()\n    scores, samples = embeeding_dataset.get_nearest_examples(\n                                                    \"embeddings\", text_embedding, k=10\n                                                    )\n    # print(scores)\n    samples_df = pd.DataFrame.from_dict(samples)\n    samples_df[\"scores\"] = scores\n    samples_df.sort_values(\"scores\", ascending=True, inplace=True)\n    \n    # embedding_list = samples_df[\"embeddings\"].values.tolist()\n    # cosine_sim_list = []\n    # for ebd in embedding_list:\n    #     cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    #     cosine_sim_list.append(cosine_score.item())\n\n    samples_df['cosine_sim'] = samples_df.apply(lambda row: get_cosine_sim(row[\"embeddings\"], text_embedding), axis=1)\n    samples_df = samples_df[samples_df['cosine_sim'] >= cosine_cutoff] \n\n    scorelist = samples_df[\"scores\"].values.tolist()\n    # scores = \" \".join([f\"{d:.10f}\" for d in scorelist])\n    cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    # cosine_sim = \" \".join([f\"{d:.10f}\" for d in cosine_sim_list])\n    if (column_list is not None):\n        result_df = samples_df[['scores', 'cosine_sim', *column_list]]\n    else:\n        result_df = samples_df\n\n    # content_id_list = samples_df[\"content_id\"].values.tolist()\n    # content_ids = \" \".join(content_id_list)\n    # topic_id_list = samples_df[\"topic_id\"].values.tolist()\n    # topic_ids = \" \".join(topic_id_list)\n\n    # return scores, topic_ids, content_ids, cosine_sim\n    return result_df\n\ndef get_score_topic_json(text, column_list, cosine_cutoff):\n\n    res_df = get_score_topic(text, column_list, cosine_cutoff)\n\n    # scorelist = samples_df[\"scores\"].values.tolist()\n    # # scores = \" \".join([f\"{d:.10f}\" for d in scorelist])\n    # cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    # # cosine_sim = \" \".join([f\"{d:.10f}\" for d in cosine_sim_list])\n    # result_df = samples_df\n\n    # content_id_list = samples_df[\"content_id\"].values.tolist()\n    # content_ids = \" \".join(content_id_list)\n    # topic_id_list = samples_df[\"topic_id\"].values.tolist()\n    # topic_ids = \" \".join(topic_id_list)\n\n    res_dict = res_df.to_dict('records')\n    return res_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_row(row):\n    columns = [\"content_id\", \"topic_id\"]\n    res_dict = get_score_topic_json(str(row[\"topic_full\"]), columns, Cosine_Cutoff) \n    # print(f\"calculate_row dict\")\n    # print(res_dict)\n    #scores, topic_ids, content_ids, cosine_sim\n    # result.append([scores, topic_id, topic_title, **evalrow ])\n    row[\"scores\"] = \" \".join([f\"{d['scores']:.10f}\" for d in res_dict])\n    row[\"topic_ids\"] = \" \".join([f\"{d['topic_id']}\" for d in res_dict])\n    row[\"content_ids\"] = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    row[\"cosine_sim\"] = \" \".join([f\"{d['cosine_sim']:.10f}\" for d in res_dict])\n    return row\n\ndef calculate_score(eval_sampled, res_file):\n    num_rows = eval_sampled.num_rows\n    scores_column = [-2] * num_rows\n    topic_ids_column = [\"\"] * num_rows\n    content_ids_column = [\"\"] * num_rows\n    cosine_column = [-1.0] * num_rows\n    eval_sampled = eval_sampled.add_column(\"scores\", scores_column)\n    eval_sampled = eval_sampled.add_column(\"topic_ids\", topic_ids_column)\n    eval_sampled = eval_sampled.add_column(\"content_ids\", content_ids_column)\n    eval_sampled = eval_sampled.add_column(\"cosine_sim\", cosine_column)\n\n    print(f\"calculating scores mapping ...\")\n    eval_sampled = eval_sampled.map(calculate_row)\n    # print(eval_sampled['scores'][:15])\n    # print(eval_sampled['topic_ids'][:15])\n    result_df = pd.DataFrame(eval_sampled)\n    # print(result_df.head())\n    print(f\"result_df value counet for each columns: \\n{result_df.nunique()}\")\n    all_columns = result_df.columns\n    keeped_columns = [\"topic_id\",\"scores\", \"cosine_sim\", \"topic_ids\",\"content_ids\"]\n    removed_columns = all_columns.symmetric_difference(keeped_columns)\n    result_df.drop(removed_columns, axis=1, inplace=True)\n    result_df.dropna( inplace=True)\n    result_df.to_csv(res_file) \n    # df   [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_description_full\",\"topic_id\",\"content_id\"\n\nResult_file = f\"./data/result_v{dver}.csv\"\nif Calculate_Score:\n    eval_sampled =  dataset[\"test\"].shuffle(seed=42).select(range(2000))\n    calculate_score(eval_sampled, Result_file)\n    Result_file = f\"./data/result_train_v{dver}.csv\"\n    eval_sampled =  dataset[\"train\"].shuffle(seed=42).select(range(2000))\n    calculate_score(eval_sampled, Result_file)\n\ncalc_df = pd.read_csv(Result_file) \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(topic_id, df_topics):\n    subset = df_topics.loc[df_topics['topic_id'] == topic_id]\n    text = None\n    for index, r in subset.iterrows():\n            text = r[\"topic_full\"]\n            break\n    if text is None:\n        return \"\"\n\n    columns = [\"content_id\", \"topic_id\"]\n    res_dict = get_score_topic_json(text, columns, Cosine_Cutoff) \n\n    content_ids = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    return content_ids\n\n\ndef calculate_submission(submission_df):\n\n    submission_df['content_ids'] = submission_df.apply(lambda row: get_neighbors(row[\"topic_id\"], df_topics), axis=1)\n    submission_df.to_csv(Submission_File) \n    \n","metadata":{"_uuid":"899ed4cf-7420-4063-a41a-ebd2c57a6e54","_cell_guid":"38085bd7-4c24-48ba-9231-f4d199f24c6b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (Cal_Submission):\n    print(f\"Calculating submission \")\n    submission_df = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n    calculate_submission(submission_df)\n\n# from sklearn.metrics import fbeta_score\n# fbeta_score(y_true, y_pred, average='macro', beta=2)","metadata":{},"execution_count":null,"outputs":[]}]}