{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -U install sentence-transformers\n!pip install faiss-gpu\n!pip install faiss-cpu\n!pip install tqdm\n!pip install nvidia-ml-py3\n!pip install accelerate\n!pip install scikit-learn\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sentence_transformers import SentenceTransformer, models, InputExample, losses, util\nfrom datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nimport torch \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:56:34.268805Z","iopub.execute_input":"2023-02-28T22:56:34.269239Z","iopub.status.idle":"2023-02-28T22:56:40.656705Z","shell.execute_reply.started":"2023-02-28T22:56:34.269194Z","shell.execute_reply":"2023-02-28T22:56:40.655290Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"!ls -l","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:56:40.658755Z","iopub.execute_input":"2023-02-28T22:56:40.659760Z","iopub.status.idle":"2023-02-28T22:56:41.884974Z","shell.execute_reply.started":"2023-02-28T22:56:40.659718Z","shell.execute_reply":"2023-02-28T22:56:41.883629Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"total 2817452\n---------- 1 root root        263 Feb 28 22:11 __notebook_source__.ipynb\n-rw-r--r-- 1 root root   70660715 Feb 28 22:49 df_topics_full_502.csv\n-rw-r--r-- 1 root root 2814397864 Feb 28 22:50 df_train_v502.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"Refresh_Topic = False\nRefresh_Train_Data = False\nTrain_model = True\nBuild_Embedding = True \nCalculate_Score = True \nCal_Submission = True\nCosine_Cutoff = 0.99995\ndver = 502\n\nTopic_Full_Data_File = f\"/kaggle/working/df_topics_full_{dver}.csv\"\nTrain_Data_File= f\"/kaggle/working/df_train_v{dver}.csv\"\nEmbeddings_File = f'/kaggle/working/embeddings_topics_dataset_v{dver}'\n\nSubmission_File = f\"/kaggle/working/submission_v{dver}.csv\"\nModel_File = f\"/kaggle/working/paraphrase-multilingual-mpnet-base-v{dver}-exp\"\n","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:56:41.888284Z","iopub.execute_input":"2023-02-28T22:56:41.888625Z","iopub.status.idle":"2023-02-28T22:56:41.896206Z","shell.execute_reply.started":"2023-02-28T22:56:41.888591Z","shell.execute_reply":"2023-02-28T22:56:41.894884Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# pd.set_option('display.max_columns', 9)\n# pd.set_option('display.max_rows', 200)\n# pd.set_option('display.min_rows', 10)\n# pd.set_option(\"expand_frame_repr\", True)\n# pd.set_option('display.width', None)\n# pd.set_option('display.max_colwidth', 50)\nDATA_PATH = \"/kaggle/input/learning-equality-curriculum-recommendations/\"\n# DATA_PATH = \"./data/input/learning-equality-curriculum-recommendations/\"\ntopics = pd.read_csv(DATA_PATH + \"topics.csv\")\ncontent = pd.read_csv(DATA_PATH + \"content.csv\")\ncorrelations = pd.read_csv(DATA_PATH + \"correlations.csv\")\n# sample_submission = pd.read_csv(DATA_PATH + \"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:56:41.898203Z","iopub.execute_input":"2023-02-28T22:56:41.898975Z","iopub.status.idle":"2023-02-28T22:57:02.693840Z","shell.execute_reply.started":"2023-02-28T22:56:41.898935Z","shell.execute_reply":"2023-02-28T22:57:02.692731Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df_topics = None\n\nif ( not topics.columns[0].startswith(\"topic_\")):\n    print(f\"renaming topics ...\")\n    topics.rename(columns=lambda x: \"topic_\" + x, inplace=True)\n    content.rename(columns=lambda x: \"content_\" + x, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:57:02.695606Z","iopub.execute_input":"2023-02-28T22:57:02.695985Z","iopub.status.idle":"2023-02-28T22:57:02.772667Z","shell.execute_reply.started":"2023-02-28T22:57:02.695948Z","shell.execute_reply":"2023-02-28T22:57:02.770964Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"renaming topics ...\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_topic_full(row):\n    topic_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_description = str(row[\"topic_description\"]) if pd.notna(row[\"topic_description\"]) else \"\"\n    topic_full = \"title: \" + topic_title\n    if (topic_description != \"\"):\n        topic_full = topic_full + \"\\r\\n\" + \"description: \" + topic_description\n\n    return topic_full\n\ndef get_parents(df, row):\n    topic_id = row[\"topic_id\"]\n    topic_title_full = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_parent = row[\"topic_parent\"]\n    topic_level = row[\"topic_level\"]\n    topic_full = get_topic_full(row)\n    while not pd.isnull(topic_parent):\n        subset = df.loc[df['topic_id'] == topic_parent]\n        for index, r in subset.iterrows():\n            t_full = get_topic_full(r)\n            topic_full = t_full + \"\\r\\n\" + topic_full \n            t_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n            topic_title_full = t_title + \".\" + topic_title_full \n            topic_parent = r[\"topic_parent\"]\n            topic_level = r[\"topic_level\"]\n            break\n\n    return topic_title_full, topic_full\n\n\ndef refresh_topic(topics):\n\n    df_topics = topics\n    print(f\"Before expand topic full ...\")\n    print(df_topics.head())\n\n    topic_title_full = []\n    topic_full = []\n\n\n    for index, row in tqdm(df_topics.iterrows(), total=df_topics.shape[0]):\n        t_title_full, t_full = get_parents(df_topics, row)\n        topic_title_full.append(t_title_full)\n        topic_full.append(t_full)\n\n    df_topics['topic_title_full'] = topic_title_full\n    df_topics['topic_full'] = topic_full\n\n    df_topics.to_csv(Topic_Full_Data_File) \n    return df_topics\n\nif (Refresh_Topic):\n    print(\"Freshing topic...\")\n    df_topics = refresh_topic(topics)\nelse:\n    df_topics = pd.read_csv(Topic_Full_Data_File) ","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:57:02.776287Z","iopub.execute_input":"2023-02-28T22:57:02.776757Z","iopub.status.idle":"2023-02-28T22:57:03.999814Z","shell.execute_reply.started":"2023-02-28T22:57:02.776724Z","shell.execute_reply":"2023-02-28T22:57:03.998725Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(f\"After expand topic full ...\")\nprint(df_topics.head())\nprint(f\"df_topics value counet for each columns: \\n{df_topics.nunique()}\")","metadata":{"_uuid":"4dd7bfd3-28de-443f-950b-f8935263482b","_cell_guid":"750adcf0-246f-457b-9ec2-b01a07d8f0c1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-02-28T22:57:04.001265Z","iopub.execute_input":"2023-02-28T22:57:04.001694Z","iopub.status.idle":"2023-02-28T22:57:04.360033Z","shell.execute_reply.started":"2023-02-28T22:57:04.001644Z","shell.execute_reply":"2023-02-28T22:57:04.358888Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"After expand topic full ...\n   Unnamed: 0        topic_id  \\\n0           0  t_00004da3a1b2   \n1           1  t_000095e03056   \n2           2  t_00068291e9a4   \n3           3  t_00069b63a70a   \n4           4  t_0006d41a73a8   \n\n                                         topic_title  \\\n0                         Откриването на резисторите   \n1             Unit 3.3 Enlargements and Similarities   \n2                    Entradas e saídas de uma função   \n3                                        Transcripts   \n4  Графики на експоненциални функции (Алгебра 2 н...   \n\n                                   topic_description topic_channel  \\\n0  Изследване на материали, които предизвикват на...        000cf7   \n1                                                NaN        b3f329   \n2               Entenda um pouco mais sobre funções.        8e286a   \n3                                                NaN        6e3ba4   \n4  Научи повече за графиките на сложните показате...        000cf7   \n\n  topic_category  topic_level topic_language    topic_parent  \\\n0         source            4             bg  t_16e29365b50d   \n1        aligned            2             en  t_aa32fb6252dc   \n2         source            4             pt  t_d14b6c2a2b70   \n3         source            3             en  t_4054df11a74e   \n4         source            4             bg  t_e2452e21d252   \n\n   topic_has_content                                   topic_title_full  \\\n0               True  Откриването на резисторите.Откриването на рези...   \n1              False  Unit 3.3 Enlargements and Similarities.Unit 3....   \n2               True  Entradas e saídas de uma função.Entradas e saí...   \n3               True    Transcripts.Transcripts.Transcripts.Transcripts   \n4               True  Графики на експоненциални функции (Алгебра 2 н...   \n\n                                          topic_full  \n0  title: Khan Academy (български език)\\r\\ntitle:...  \n1  title: Ghana JHS Curriculum (in progress)\\r\\nt...  \n2  title: Khan Academy (Português (Brasil))\\r\\nti...  \n3  title: MIT Blossoms\\r\\ntitle: Engineering\\r\\nd...  \n4  title: Khan Academy (български език)\\r\\ntitle:...  \ndf_topics value counet for each columns: \nUnnamed: 0           76972\ntopic_id             76972\ntopic_title          45082\ntopic_description    23067\ntopic_channel          171\ntopic_category           3\ntopic_level             11\ntopic_language          28\ntopic_parent         17512\ntopic_has_content        2\ntopic_title_full     50086\ntopic_full           76823\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Load train data by combining correlation table with topic and content tables.","metadata":{}},{"cell_type":"code","source":"def load_train_data(topics):\n    train_df_columns = [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_full\", \"topic_id\",\"content_id\", \"content_description\", \"content_text\" ]\n    \n    correlations[\"content_id\"] = correlations[\"content_ids\"].str.split(\" \")\n    corr = correlations.explode(\"content_id\").drop(columns=[\"content_ids\"])\n\n    corr = corr.merge(df_topics, how=\"left\", on=\"topic_id\")\n    corr = corr.merge(content, how=\"left\", on=\"content_id\")\n\n    train_df = pd.DataFrame(corr[train_df_columns])\n    cols = ['content_title', 'content_description', \"content_text\"]\n    train_df['content_full'] = train_df[cols].apply(lambda row: '\\r\\n'.join(row.values.astype(str)), axis=1)\n    \n    final_train_data = pd.DataFrame(train_df)\n    \n    print(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\n\n    final_train_data.to_csv(Train_Data_File)\n    \n    return final_train_data","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:57:04.361833Z","iopub.execute_input":"2023-02-28T22:57:04.362251Z","iopub.status.idle":"2023-02-28T22:57:04.372600Z","shell.execute_reply.started":"2023-02-28T22:57:04.362211Z","shell.execute_reply":"2023-02-28T22:57:04.371327Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"if Refresh_Train_Data:\n    print(f\"Refresh_Train_Data ==>>>\")\n    final_train_data = load_train_data(topics)\nelse:\n    print(f\"load final_train_data from {Train_Data_File}\")\n    final_train_data = pd.read_csv(Train_Data_File)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:49:24.903571Z","iopub.execute_input":"2023-02-28T22:49:24.903872Z","iopub.status.idle":"2023-02-28T22:50:35.099018Z","shell.execute_reply.started":"2023-02-28T22:49:24.903843Z","shell.execute_reply":"2023-02-28T22:50:35.097940Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Refresh_Train_Data ==>>>\nfinal_train_data value counet for each columns: \ntopic_title             36597\ncontent_title          130937\ntopic_title_full        39942\ntopic_full              61394\ntopic_id                61517\ncontent_id             154047\ncontent_description     76305\ncontent_text            70687\ncontent_full           152701\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File)\nprint(dataset_dict)\n\ndataset = dataset_dict[\"train\"]\n\ndataset = dataset.train_test_split(test_size=0.2)\n\ntrain_dataset = dataset[\"train\"]\n\nkeeped_columns = [\"topic_full\", \"content_full\"]\ncolumns = train_dataset.column_names\ncolumns_to_keep = [\"topic_full\", \"content_full\"]\ncolumns_to_remove = set(columns_to_keep).symmetric_difference(columns)\ntrain_dataset = train_dataset.remove_columns(columns_to_remove)\n\n# train_dataset = train_dataset.shuffle(seed=42).select(range(10000))\n\ntrain_examples = []\neval_examples = []\n\nn_examples = train_dataset.num_rows","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:57:04.376909Z","iopub.execute_input":"2023-02-28T22:57:04.377227Z","iopub.status.idle":"2023-02-28T22:58:17.170705Z","shell.execute_reply.started":"2023-02-28T22:57:04.377199Z","shell.execute_reply":"2023-02-28T22:58:17.168507Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-529176dc8de67005/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a9ec0e3f12646e28c9b550b1d8078f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a55093499e14cfa9dae33f3858ea51f"}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-529176dc8de67005/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"161ebf69d7494b23b6ab58da4e6735e6"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['Unnamed: 0', 'topic_title', 'content_title', 'topic_title_full', 'topic_full', 'topic_id', 'content_id', 'content_description', 'content_text', 'content_full'],\n        num_rows: 279919\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"for dt in train_dataset:\n    # print([dt[\"topic_full\"], dt[\"content_full\"]])\n    # train_examples.append(InputExample(texts=[str(dt[\"topic_full\"]), str(dt[\"content_full\"])]))\n    first = str(dt[\"topic_full\"]) if dt[\"topic_full\"] != \"\" else \"\" \n    second = str(dt[\"content_full\"]) if str(dt[\"content_full\"]) != \"\" else \"\" \n    if (first != \"\" and second != \"\"):\n        train_examples.append(InputExample(texts=[first, second]))\n\nprint(f\"train_examples: {len(train_examples)}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:58:17.176509Z","iopub.execute_input":"2023-02-28T22:58:17.177329Z","iopub.status.idle":"2023-02-28T22:59:01.940951Z","shell.execute_reply.started":"2023-02-28T22:58:17.177281Z","shell.execute_reply":"2023-02-28T22:59:01.939499Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"train_examples: 223935\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model():\n\n    model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n\n    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)\n    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n    # num_epochs = 5\n    num_epochs = 5\n    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n\n    model.to(device)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.fit(train_objectives=[(train_dataloader, train_loss)],\n            epochs=num_epochs,\n            warmup_steps=warmup_steps)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.save(Model_File)","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:59:01.944486Z","iopub.execute_input":"2023-02-28T22:59:01.945823Z","iopub.status.idle":"2023-02-28T22:59:01.956346Z","shell.execute_reply.started":"2023-02-28T22:59:01.945772Z","shell.execute_reply":"2023-02-28T22:59:01.954770Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"if Train_model:\n    train_model()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:59:01.957775Z","iopub.execute_input":"2023-02-28T22:59:01.958191Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/690 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"170741761891441e85b0a3daa523a850"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"553edf09fae24fb297647bbc588ec85f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/3.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d719738b25b4293a9078a4344635dc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/723 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e124bc7e2247f488da382e7afb9a8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f69b884b89234691b98679901713c2ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e865ffda13d447a97a0c57b7ebf65de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd998bccf45d40c3973e9955fe1acebf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c46e7a3d1a41d8bd94117717d911df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f191ccb39f4b43899005e4f75856bdbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05a0d7c00c744b56a1cac0187d4701fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/402 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d6dcb3fb58b4f299dbb7afc7c1a1ac2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44babbc6ad7844769d3b04bc8a7b984e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34cc1ccc399445899aaefb702bd71c83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/55984 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f25c50a8c6424aa3029747e8bb9265"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/55984 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8955d522940548da96ea797dc26ffb91"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(Model_File)\ntrained_model = AutoModel.from_pretrained(Model_File)\n\ntrained_model.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cls_pooling(model_output):\n    return model_output.last_hidden_state[:, 0]\n\ndef get_embeddings(text_list):\n    encoded_input = tokenizer(\n        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    model_output = trained_model(**encoded_input)\n    return cls_pooling(model_output)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build embedding based on topic_full and content_full, we also need keep topic_id and content_id.\ndef build_embedding(combined_df):\n    all_columns = combined_df.columns\n    keeped_columns = [\"topic_id\",\"topic_full\", \"content_full\",\"content_id\"]\n    removed_columns = all_columns.symmetric_difference(keeped_columns)\n    combined_df.drop(removed_columns, axis=1, inplace = True)\n    print(f\"combined_df ==> {combined_df.head()}\")  \n    print(f\"combined_df value counet for each columns: \\n{combined_df.nunique()}\")\n    \n    combined_df.dropna(inplace = True)\n    # combined_df.dropna(subset = [\"topic_title_full\", \"topic_full\"], inplace = True)\n    print(f\"combined_df ==> {combined_df.shape}\")  \n    print(f\"combined_df {combined_df.head()}\")\n\n    print(f\"building embeddings for topic_full\")\n    embeddings_topics_dataset = Dataset.from_pandas(combined_df)\n    embeddings_topics_dataset = embeddings_topics_dataset.map(\n        lambda x: {\"embeddings\": get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0]}\n    )\n\n    print(f\"building embeddings for content_full\")\n    conts = combined_df.copy()\n    \n    embeddings_content_dataset = Dataset.from_pandas(conts)\n    embeddings_content_dataset = embeddings_content_dataset.map(\n        lambda x: {\"embeddings\": get_embeddings( [str(x[\"content_full\"])]).detach().cpu().numpy()[0]}\n    )\n    embeeding_dataset =  concatenate_datasets([embeddings_topics_dataset, embeddings_content_dataset])\n\n    embeeding_dataset.save_to_disk(Embeddings_File)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Build_Embedding:\n    print(f\"Build Embedding .... {final_train_data.shape}\")\n    build_embedding(final_train_data)\n\nprint(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\nembeeding_dataset = load_from_disk(Embeddings_File)\nprint(embeeding_dataset)\nembeeding_dataset.add_faiss_index(column=\"embeddings\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cosine_sim(ebd, text_embedding):\n    cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    return cosine_score.item()\n\ndef get_score_topic(text, column_list, cosine_cutoff):\n    text_embedding = get_embeddings(text).cpu().detach().numpy()\n    scores, samples = embeeding_dataset.get_nearest_examples(\n                                                    \"embeddings\", text_embedding, k=10\n                                                    )\n    # print(scores)\n    samples_df = pd.DataFrame.from_dict(samples)\n    samples_df[\"scores\"] = scores\n    samples_df.sort_values(\"scores\", ascending=True, inplace=True)\n    \n    # embedding_list = samples_df[\"embeddings\"].values.tolist()\n    # cosine_sim_list = []\n    # for ebd in embedding_list:\n    #     cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    #     cosine_sim_list.append(cosine_score.item())\n\n    samples_df['cosine_sim'] = samples_df.apply(lambda row: get_cosine_sim(row[\"embeddings\"], text_embedding), axis=1)\n    samples_df = samples_df[samples_df['cosine_sim'] >= cosine_cutoff] \n\n    scorelist = samples_df[\"scores\"].values.tolist()\n    # scores = \" \".join([f\"{d:.10f}\" for d in scorelist])\n    cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    # cosine_sim = \" \".join([f\"{d:.10f}\" for d in cosine_sim_list])\n    if (column_list is not None):\n        result_df = samples_df[['scores', 'cosine_sim', *column_list]]\n    else:\n        result_df = samples_df\n\n    # content_id_list = samples_df[\"content_id\"].values.tolist()\n    # content_ids = \" \".join(content_id_list)\n    # topic_id_list = samples_df[\"topic_id\"].values.tolist()\n    # topic_ids = \" \".join(topic_id_list)\n\n    # return scores, topic_ids, content_ids, cosine_sim\n    return result_df\n\ndef get_score_topic_json(text, column_list, cosine_cutoff):\n\n    res_df = get_score_topic(text, column_list, cosine_cutoff)\n\n    # scorelist = samples_df[\"scores\"].values.tolist()\n    # # scores = \" \".join([f\"{d:.10f}\" for d in scorelist])\n    # cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    # # cosine_sim = \" \".join([f\"{d:.10f}\" for d in cosine_sim_list])\n    # result_df = samples_df\n\n    # content_id_list = samples_df[\"content_id\"].values.tolist()\n    # content_ids = \" \".join(content_id_list)\n    # topic_id_list = samples_df[\"topic_id\"].values.tolist()\n    # topic_ids = \" \".join(topic_id_list)\n\n    res_dict = res_df.to_dict('records')\n    return res_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_row(row):\n    columns = [\"content_id\", \"topic_id\"]\n    res_dict = get_score_topic_json(str(row[\"topic_full\"]), columns, Cosine_Cutoff) \n    # print(f\"calculate_row dict\")\n    # print(res_dict)\n    #scores, topic_ids, content_ids, cosine_sim\n    # result.append([scores, topic_id, topic_title, **evalrow ])\n    row[\"scores\"] = \" \".join([f\"{d['scores']:.10f}\" for d in res_dict])\n    row[\"topic_ids\"] = \" \".join([f\"{d['topic_id']}\" for d in res_dict])\n    row[\"content_ids\"] = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    row[\"cosine_sim\"] = \" \".join([f\"{d['cosine_sim']:.10f}\" for d in res_dict])\n    return row\n\ndef calculate_score(eval_sampled, res_file):\n    num_rows = eval_sampled.num_rows\n    scores_column = [-2] * num_rows\n    topic_ids_column = [\"\"] * num_rows\n    content_ids_column = [\"\"] * num_rows\n    cosine_column = [-1.0] * num_rows\n    eval_sampled = eval_sampled.add_column(\"scores\", scores_column)\n    eval_sampled = eval_sampled.add_column(\"topic_ids\", topic_ids_column)\n    eval_sampled = eval_sampled.add_column(\"content_ids\", content_ids_column)\n    eval_sampled = eval_sampled.add_column(\"cosine_sim\", cosine_column)\n\n    print(f\"calculating scores mapping ...\")\n    eval_sampled = eval_sampled.map(calculate_row)\n    # print(eval_sampled['scores'][:15])\n    # print(eval_sampled['topic_ids'][:15])\n    result_df = pd.DataFrame(eval_sampled)\n    # print(result_df.head())\n    print(f\"result_df value counet for each columns: \\n{result_df.nunique()}\")\n    all_columns = result_df.columns\n    keeped_columns = [\"topic_id\",\"scores\", \"cosine_sim\", \"topic_ids\",\"content_ids\"]\n    removed_columns = all_columns.symmetric_difference(keeped_columns)\n    result_df.drop(removed_columns, axis=1, inplace=True)\n    result_df.dropna( inplace=True)\n    result_df.to_csv(res_file) \n    # df   [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_description_full\",\"topic_id\",\"content_id\"\n\nResult_file = f\"./data/result_v{dver}.csv\"\nif Calculate_Score:\n    eval_sampled =  dataset[\"test\"].shuffle(seed=42).select(range(2000))\n    calculate_score(eval_sampled, Result_file)\n    Result_file = f\"./data/result_train_v{dver}.csv\"\n    eval_sampled =  dataset[\"train\"].shuffle(seed=42).select(range(2000))\n    calculate_score(eval_sampled, Result_file)\n\ncalc_df = pd.read_csv(Result_file) \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(topic_id, df_topics):\n    subset = df_topics.loc[df_topics['topic_id'] == topic_id]\n    text = None\n    for index, r in subset.iterrows():\n            text = r[\"topic_full\"]\n            break\n    if text is None:\n        return \"\"\n\n    columns = [\"content_id\", \"topic_id\"]\n    res_dict = get_score_topic_json(text, columns, Cosine_Cutoff) \n\n    content_ids = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    return content_ids\n\n\ndef calculate_submission(submission_df):\n\n    submission_df['content_ids'] = submission_df.apply(lambda row: get_neighbors(row[\"topic_id\"], df_topics), axis=1)\n    submission_df.to_csv(DATA_PATH + \"sample_submission.csv\") \n    submission_df.to_csv(Submission_File) \n    \n","metadata":{"_uuid":"899ed4cf-7420-4063-a41a-ebd2c57a6e54","_cell_guid":"38085bd7-4c24-48ba-9231-f4d199f24c6b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (Cal_Submission):\n    print(f\"Calculating submission \")\n    submission_df = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n    calculate_submission(submission_df)\n\n# from sklearn.metrics import fbeta_score\n# fbeta_score(y_true, y_pred, average='macro', beta=2)","metadata":{},"execution_count":null,"outputs":[]}]}