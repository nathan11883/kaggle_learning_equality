{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install sentence-transformers\n# !pip install faiss-gpu\n# !pip install faiss-cpu\n# !pip install tqdm\n# !pip install nvidia-ml-py3\n# !pip install accelerate\n# !pip install scikit-learn","metadata":{"execution":{"iopub.status.busy":"2023-03-01T16:23:30.391360Z","iopub.execute_input":"2023-03-01T16:23:30.393373Z","iopub.status.idle":"2023-03-01T16:25:41.390841Z","shell.execute_reply.started":"2023-03-01T16:23:30.393300Z","shell.execute_reply":"2023-03-01T16:25:41.389113Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m304.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.26.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.64.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.13.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.14.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.97)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.12.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=e1770f6943a8515a2542d66dd278b102588935861ac99539172fbd6a8b02b95c\n  Stored in directory: /root/.cache/pip/wheels/83/71/2b/40d17d21937fed496fb99145227eca8f20b4891240ff60c86f\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting faiss-cpu\n  Downloading faiss_cpu-1.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.7.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting nvidia-ml-py3\n  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: nvidia-ml-py3\n  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19190 sha256=4f5454fa13ce6935d18b7362448033656fe4c532b777f64a0203b3c2eec7a05a\n  Stored in directory: /root/.cache/pip/wheels/74/d2/c1/2ea351258984f451bd34e5ff2928621ab030e2eda5ffd2fdec\nSuccessfully built nvidia-ml-py3\nInstalling collected packages: nvidia-ml-py3\nSuccessfully installed nvidia-ml-py3-7.352.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: accelerate in /opt/conda/lib/python3.7/site-packages (0.12.0)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.13.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from accelerate) (6.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (23.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.21.6)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.21.6)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/sentence-transformers-2.2.2.tar.gz --no-index\n!pip install faiss-gpu --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_cpu-1.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index\n!pip install faiss-cpu --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index\n!pip install tqdm --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/tqdm-4.64.1-py2.py3-none-any.whl --no-index\n!pip install nvidia-ml-py3 --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/nvidia-ml-py3-7.352.0.tar.gz --no-index\n!pip install accelerate --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/accelerate-0.16.0-py3-none-any.whl --no-index\n!pip install scikit-learn --find-links /kaggle/input/using-fine-tuned-sentencetransformer-env/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sentence_transformers import SentenceTransformer, models, InputExample, losses, util\nfrom datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nimport torch \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-03-01T16:25:55.563678Z","iopub.execute_input":"2023-03-01T16:25:55.564222Z","iopub.status.idle":"2023-03-01T16:26:12.544051Z","shell.execute_reply.started":"2023-03-01T16:25:55.564179Z","shell.execute_reply":"2023-03-01T16:26:12.542306Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!ls -l","metadata":{"execution":{"iopub.status.busy":"2023-03-01T16:15:19.933619Z","iopub.execute_input":"2023-03-01T16:15:19.933971Z","iopub.status.idle":"2023-03-01T16:15:20.933178Z","shell.execute_reply.started":"2023-03-01T16:15:19.933926Z","shell.execute_reply":"2023-03-01T16:15:20.931838Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"total 4\n---------- 1 root root 263 Mar  1 15:44 __notebook_source__.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"Refresh_Topic = True\nRefresh_Train_Data = True\nTrain_model = True\nBuild_Embedding = True \nCalculate_Score = True \nCal_Submission = True\nCosine_Cutoff = 0.99995\ndver = 505\n\nTopic_Full_Data_File = f\"/kaggle/working/df_topics_full_{dver}.csv\"\nTrain_Data_File= f\"/kaggle/working/df_train_v{dver}.csv\"\nEmbeddings_File = f'/kaggle/working/embeddings_topics_dataset_v{dver}'\n\nSubmission_File = f\"/kaggle/working/submission_v{dver}.csv\"\nModel_File = f\"/kaggle/working/paraphrase-multilingual-mpnet-base-v{dver}-exp\"\n","metadata":{"execution":{"iopub.status.busy":"2023-03-01T16:19:24.521729Z","iopub.execute_input":"2023-03-01T16:19:24.522102Z","iopub.status.idle":"2023-03-01T16:19:24.528659Z","shell.execute_reply.started":"2023-03-01T16:19:24.522071Z","shell.execute_reply":"2023-03-01T16:19:24.527486Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# pd.set_option('display.max_columns', 9)\n# pd.set_option('display.max_rows', 200)\n# pd.set_option('display.min_rows', 10)\n# pd.set_option(\"expand_frame_repr\", True)\n# pd.set_option('display.width', None)\n# pd.set_option('display.max_colwidth', 50)\nDATA_PATH = \"/kaggle/input/learning-equality-curriculum-recommendations/\"\n# DATA_PATH = \"./data/input/learning-equality-curriculum-recommendations/\"\ntopics = pd.read_csv(DATA_PATH + \"topics.csv\")\ncontent = pd.read_csv(DATA_PATH + \"content.csv\")\ncorrelations = pd.read_csv(DATA_PATH + \"correlations.csv\")\n# sample_submission = pd.read_csv(DATA_PATH + \"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-03-01T16:19:28.260208Z","iopub.execute_input":"2023-03-01T16:19:28.260941Z","iopub.status.idle":"2023-03-01T16:19:38.893255Z","shell.execute_reply.started":"2023-03-01T16:19:28.260891Z","shell.execute_reply":"2023-03-01T16:19:38.892211Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_topics = None\n\nif ( not topics.columns[0].startswith(\"topic_\")):\n    print(f\"renaming topics ...\")\n    topics.rename(columns=lambda x: \"topic_\" + x, inplace=True)\n    content.rename(columns=lambda x: \"content_\" + x, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T16:19:43.000864Z","iopub.execute_input":"2023-03-01T16:19:43.001257Z","iopub.status.idle":"2023-03-01T16:19:43.008488Z","shell.execute_reply.started":"2023-03-01T16:19:43.001226Z","shell.execute_reply":"2023-03-01T16:19:43.007476Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"renaming topics ...\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_topic_full(row):\n    topic_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_description = str(row[\"topic_description\"]) if pd.notna(row[\"topic_description\"]) else \"\"\n    topic_full = \"title: \" + topic_title\n    if (topic_description != \"\"):\n        topic_full = topic_full + \"\\r\\n\" + \"description: \" + topic_description\n\n    return topic_full\n\ndef get_parents(df, row):\n    topic_id = row[\"topic_id\"]\n    topic_title_full = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n    topic_parent = row[\"topic_parent\"]\n    topic_level = row[\"topic_level\"]\n    topic_full = get_topic_full(row)\n    while not pd.isnull(topic_parent):\n        subset = df.loc[df['topic_id'] == topic_parent]\n        for index, r in subset.iterrows():\n            t_full = get_topic_full(r)\n            topic_full = t_full + \"\\r\\n\" + topic_full \n            t_title = str(row[\"topic_title\"]) if pd.notna(row[\"topic_title\"]) else \"\"\n            topic_title_full = t_title + \".\" + topic_title_full \n            topic_parent = r[\"topic_parent\"]\n            topic_level = r[\"topic_level\"]\n            break\n\n    return topic_title_full, topic_full\n\n\ndef refresh_topic(topics):\n\n    df_topics = topics\n    print(f\"Before expand topic full ...\")\n    print(df_topics.head())\n\n    topic_title_full = []\n    topic_full = []\n\n\n    for index, row in tqdm(df_topics.iterrows(), total=df_topics.shape[0]):\n        t_title_full, t_full = get_parents(df_topics, row)\n        topic_title_full.append(t_title_full)\n        topic_full.append(t_full)\n\n    df_topics['topic_title_full'] = topic_title_full\n    df_topics['topic_full'] = topic_full\n\n    df_topics.to_csv(Topic_Full_Data_File) \n    return df_topics\n\nif (Refresh_Topic):\n    print(\"Freshing topic...\")\n    df_topics = refresh_topic(topics)\nelse:\n    df_topics = pd.read_csv(Topic_Full_Data_File) ","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:57:02.776287Z","iopub.execute_input":"2023-02-28T22:57:02.776757Z","iopub.status.idle":"2023-02-28T22:57:03.999814Z","shell.execute_reply.started":"2023-02-28T22:57:02.776724Z","shell.execute_reply":"2023-02-28T22:57:03.998725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"After expand topic full ...\")\nprint(df_topics.head())\nprint(f\"df_topics value counet for each columns: \\n{df_topics.nunique()}\")","metadata":{"_uuid":"4dd7bfd3-28de-443f-950b-f8935263482b","_cell_guid":"750adcf0-246f-457b-9ec2-b01a07d8f0c1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-02-28T22:57:04.001265Z","iopub.execute_input":"2023-02-28T22:57:04.001694Z","iopub.status.idle":"2023-02-28T22:57:04.360033Z","shell.execute_reply.started":"2023-02-28T22:57:04.001644Z","shell.execute_reply":"2023-02-28T22:57:04.358888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load train data by combining correlation table with topic and content tables.","metadata":{}},{"cell_type":"code","source":"def load_train_data(topics):\n    train_df_columns = [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_full\", \"topic_id\",\"content_id\", \"content_description\", \"content_text\" ]\n    \n    correlations[\"content_id\"] = correlations[\"content_ids\"].str.split(\" \")\n    corr = correlations.explode(\"content_id\").drop(columns=[\"content_ids\"])\n\n    corr = corr.merge(df_topics, how=\"left\", on=\"topic_id\")\n    corr = corr.merge(content, how=\"left\", on=\"content_id\")\n\n    train_df = pd.DataFrame(corr[train_df_columns])\n    cols = ['content_title', 'content_description', \"content_text\"]\n    train_df['content_full'] = train_df[cols].apply(lambda row: '\\r\\n'.join(row.values.astype(str)), axis=1)\n    \n    final_train_data = pd.DataFrame(train_df)\n    \n    print(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\n\n    final_train_data.to_csv(Train_Data_File)\n    \n    return final_train_data","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:57:04.361833Z","iopub.execute_input":"2023-02-28T22:57:04.362251Z","iopub.status.idle":"2023-02-28T22:57:04.372600Z","shell.execute_reply.started":"2023-02-28T22:57:04.362211Z","shell.execute_reply":"2023-02-28T22:57:04.371327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Refresh_Train_Data:\n    print(f\"Refresh_Train_Data ==>>>\")\n    final_train_data = load_train_data(topics)\nelse:\n    print(f\"load final_train_data from {Train_Data_File}\")\n    final_train_data = pd.read_csv(Train_Data_File)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:49:24.903571Z","iopub.execute_input":"2023-02-28T22:49:24.903872Z","iopub.status.idle":"2023-02-28T22:50:35.099018Z","shell.execute_reply.started":"2023-02-28T22:49:24.903843Z","shell.execute_reply":"2023-02-28T22:50:35.097940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dict = load_dataset(\"csv\", data_files=Train_Data_File)\nprint(dataset_dict)\n\ndataset = dataset_dict[\"train\"]\n\ndataset = dataset.train_test_split(test_size=0.2)\n\ntrain_dataset = dataset[\"train\"]\n\nkeeped_columns = [\"topic_full\", \"content_full\"]\ncolumns = train_dataset.column_names\ncolumns_to_keep = [\"topic_full\", \"content_full\"]\ncolumns_to_remove = set(columns_to_keep).symmetric_difference(columns)\ntrain_dataset = train_dataset.remove_columns(columns_to_remove)\n\n# train_dataset = train_dataset.shuffle(seed=42).select(range(10000))\n\ntrain_examples = []\neval_examples = []\n\nn_examples = train_dataset.num_rows","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:57:04.376909Z","iopub.execute_input":"2023-02-28T22:57:04.377227Z","iopub.status.idle":"2023-02-28T22:58:17.170705Z","shell.execute_reply.started":"2023-02-28T22:57:04.377199Z","shell.execute_reply":"2023-02-28T22:58:17.168507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dt in train_dataset:\n    # print([dt[\"topic_full\"], dt[\"content_full\"]])\n    # train_examples.append(InputExample(texts=[str(dt[\"topic_full\"]), str(dt[\"content_full\"])]))\n    first = str(dt[\"topic_full\"]) if dt[\"topic_full\"] != \"\" else \"\" \n    second = str(dt[\"content_full\"]) if str(dt[\"content_full\"]) != \"\" else \"\" \n    if (first != \"\" and second != \"\"):\n        train_examples.append(InputExample(texts=[first, second]))\n\nprint(f\"train_examples: {len(train_examples)}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:58:17.176509Z","iopub.execute_input":"2023-02-28T22:58:17.177329Z","iopub.status.idle":"2023-02-28T22:59:01.940951Z","shell.execute_reply.started":"2023-02-28T22:58:17.177281Z","shell.execute_reply":"2023-02-28T22:59:01.939499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model():\n\n    model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n\n    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n    # num_epochs = 5\n    num_epochs = 1\n    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n\n    model.to(device)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.fit(train_objectives=[(train_dataloader, train_loss)],\n            epochs=num_epochs,\n            warmup_steps=warmup_steps)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.save(Model_File)","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:59:01.944486Z","iopub.execute_input":"2023-02-28T22:59:01.945823Z","iopub.status.idle":"2023-02-28T22:59:01.956346Z","shell.execute_reply.started":"2023-02-28T22:59:01.945772Z","shell.execute_reply":"2023-02-28T22:59:01.954770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Train_model:\n    train_model()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-28T22:59:01.957775Z","iopub.execute_input":"2023-02-28T22:59:01.958191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(Model_File)\ntrained_model = AutoModel.from_pretrained(Model_File)\n\ntrained_model.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cls_pooling(model_output):\n    return model_output.last_hidden_state[:, 0]\n\ndef get_embeddings(text_list):\n    encoded_input = tokenizer(\n        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    model_output = trained_model(**encoded_input)\n    return cls_pooling(model_output)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build embedding based on topic_full and content_full, we also need keep topic_id and content_id.\ndef build_embedding(combined_df):\n    all_columns = combined_df.columns\n    keeped_columns = [\"topic_id\",\"topic_full\", \"content_full\",\"content_id\"]\n    removed_columns = all_columns.symmetric_difference(keeped_columns)\n    combined_df.drop(removed_columns, axis=1, inplace = True)\n    print(f\"combined_df ==> {combined_df.head()}\")  \n    print(f\"combined_df value counet for each columns: \\n{combined_df.nunique()}\")\n    \n    combined_df.dropna(inplace = True)\n    # combined_df.dropna(subset = [\"topic_title_full\", \"topic_full\"], inplace = True)\n    print(f\"combined_df ==> {combined_df.shape}\")  \n    print(f\"combined_df {combined_df.head()}\")\n\n    print(f\"building embeddings for topic_full\")\n    embeddings_topics_dataset = Dataset.from_pandas(combined_df)\n    embeddings_topics_dataset = embeddings_topics_dataset.map(\n        lambda x: {\"embeddings\": get_embeddings( str(x[\"topic_full\"])).detach().cpu().numpy()[0]}\n    )\n\n    print(f\"building embeddings for content_full\")\n    conts = combined_df.copy()\n    \n    embeddings_content_dataset = Dataset.from_pandas(conts)\n    embeddings_content_dataset = embeddings_content_dataset.map(\n        lambda x: {\"embeddings\": get_embeddings( [str(x[\"content_full\"])]).detach().cpu().numpy()[0]}\n    )\n    embeeding_dataset =  concatenate_datasets([embeddings_topics_dataset, embeddings_content_dataset])\n\n    embeeding_dataset.save_to_disk(Embeddings_File)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Build_Embedding:\n    print(f\"Build Embedding .... {final_train_data.shape}\")\n    build_embedding(final_train_data)\n\nprint(f\"final_train_data value counet for each columns: \\n{final_train_data.nunique()}\")\nembeeding_dataset = load_from_disk(Embeddings_File)\nprint(embeeding_dataset)\nembeeding_dataset.add_faiss_index(column=\"embeddings\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cosine_sim(ebd, text_embedding):\n    cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    return cosine_score.item()\n\ndef get_score_topic(text, column_list, cosine_cutoff):\n    text_embedding = get_embeddings(text).cpu().detach().numpy()\n    scores, samples = embeeding_dataset.get_nearest_examples(\n                                                    \"embeddings\", text_embedding, k=10\n                                                    )\n    # print(scores)\n    samples_df = pd.DataFrame.from_dict(samples)\n    samples_df[\"scores\"] = scores\n    samples_df.sort_values(\"scores\", ascending=True, inplace=True)\n    \n    # embedding_list = samples_df[\"embeddings\"].values.tolist()\n    # cosine_sim_list = []\n    # for ebd in embedding_list:\n    #     cosine_score = util.pytorch_cos_sim(text_embedding, ebd)\n    #     cosine_sim_list.append(cosine_score.item())\n\n    samples_df['cosine_sim'] = samples_df.apply(lambda row: get_cosine_sim(row[\"embeddings\"], text_embedding), axis=1)\n    samples_df = samples_df[samples_df['cosine_sim'] >= cosine_cutoff] \n\n    scorelist = samples_df[\"scores\"].values.tolist()\n    # scores = \" \".join([f\"{d:.10f}\" for d in scorelist])\n    cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    # cosine_sim = \" \".join([f\"{d:.10f}\" for d in cosine_sim_list])\n    if (column_list is not None):\n        result_df = samples_df[['scores', 'cosine_sim', *column_list]]\n    else:\n        result_df = samples_df\n\n    # content_id_list = samples_df[\"content_id\"].values.tolist()\n    # content_ids = \" \".join(content_id_list)\n    # topic_id_list = samples_df[\"topic_id\"].values.tolist()\n    # topic_ids = \" \".join(topic_id_list)\n\n    # return scores, topic_ids, content_ids, cosine_sim\n    return result_df\n\ndef get_score_topic_json(text, column_list, cosine_cutoff):\n\n    res_df = get_score_topic(text, column_list, cosine_cutoff)\n\n    # scorelist = samples_df[\"scores\"].values.tolist()\n    # # scores = \" \".join([f\"{d:.10f}\" for d in scorelist])\n    # cosine_sim_list = samples_df[\"cosine_sim\"].values.tolist()\n    # # cosine_sim = \" \".join([f\"{d:.10f}\" for d in cosine_sim_list])\n    # result_df = samples_df\n\n    # content_id_list = samples_df[\"content_id\"].values.tolist()\n    # content_ids = \" \".join(content_id_list)\n    # topic_id_list = samples_df[\"topic_id\"].values.tolist()\n    # topic_ids = \" \".join(topic_id_list)\n\n    res_dict = res_df.to_dict('records')\n    return res_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_row(row):\n    columns = [\"content_id\", \"topic_id\"]\n    res_dict = get_score_topic_json(str(row[\"topic_full\"]), columns, Cosine_Cutoff) \n    # print(f\"calculate_row dict\")\n    # print(res_dict)\n    #scores, topic_ids, content_ids, cosine_sim\n    # result.append([scores, topic_id, topic_title, **evalrow ])\n    row[\"scores\"] = \" \".join([f\"{d['scores']:.10f}\" for d in res_dict])\n    row[\"topic_ids\"] = \" \".join([f\"{d['topic_id']}\" for d in res_dict])\n    row[\"content_ids\"] = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    row[\"cosine_sim\"] = \" \".join([f\"{d['cosine_sim']:.10f}\" for d in res_dict])\n    return row\n\ndef calculate_score(eval_sampled, res_file):\n    num_rows = eval_sampled.num_rows\n    scores_column = [-2] * num_rows\n    topic_ids_column = [\"\"] * num_rows\n    content_ids_column = [\"\"] * num_rows\n    cosine_column = [-1.0] * num_rows\n    eval_sampled = eval_sampled.add_column(\"scores\", scores_column)\n    eval_sampled = eval_sampled.add_column(\"topic_ids\", topic_ids_column)\n    eval_sampled = eval_sampled.add_column(\"content_ids\", content_ids_column)\n    eval_sampled = eval_sampled.add_column(\"cosine_sim\", cosine_column)\n\n    print(f\"calculating scores mapping ...\")\n    eval_sampled = eval_sampled.map(calculate_row)\n    # print(eval_sampled['scores'][:15])\n    # print(eval_sampled['topic_ids'][:15])\n    result_df = pd.DataFrame(eval_sampled)\n    # print(result_df.head())\n    print(f\"result_df value counet for each columns: \\n{result_df.nunique()}\")\n    all_columns = result_df.columns\n    keeped_columns = [\"topic_id\",\"scores\", \"cosine_sim\", \"topic_ids\",\"content_ids\"]\n    removed_columns = all_columns.symmetric_difference(keeped_columns)\n    result_df.drop(removed_columns, axis=1, inplace=True)\n    result_df.dropna( inplace=True)\n    result_df.to_csv(res_file) \n    # df   [\"topic_title\", \"content_title\", \"topic_title_full\", \"topic_description_full\",\"topic_id\",\"content_id\"\n\nResult_file = f\"./data/result_v{dver}.csv\"\nif Calculate_Score:\n    eval_sampled =  dataset[\"test\"].shuffle(seed=42).select(range(2000))\n    calculate_score(eval_sampled, Result_file)\n    Result_file = f\"./data/result_train_v{dver}.csv\"\n    eval_sampled =  dataset[\"train\"].shuffle(seed=42).select(range(2000))\n    calculate_score(eval_sampled, Result_file)\n\ncalc_df = pd.read_csv(Result_file) \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(topic_id, df_topics):\n    subset = df_topics.loc[df_topics['topic_id'] == topic_id]\n    text = None\n    for index, r in subset.iterrows():\n            text = r[\"topic_full\"]\n            break\n    if text is None:\n        return \"\"\n\n    columns = [\"content_id\", \"topic_id\"]\n    res_dict = get_score_topic_json(text, columns, Cosine_Cutoff) \n\n    content_ids = \" \".join([f\"{d['content_id']}\" for d in res_dict])\n    return content_ids\n\n\ndef calculate_submission(submission_df):\n\n    submission_df['content_ids'] = submission_df.apply(lambda row: get_neighbors(row[\"topic_id\"], df_topics), axis=1)\n    submission_df.to_csv(DATA_PATH + \"submission.csv\") \n    submission_df.to_csv(Submission_File) \n    \n","metadata":{"_uuid":"899ed4cf-7420-4063-a41a-ebd2c57a6e54","_cell_guid":"38085bd7-4c24-48ba-9231-f4d199f24c6b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (Cal_Submission):\n    print(f\"Calculating submission \")\n    submission_df = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n    calculate_submission(submission_df)\n\n# from sklearn.metrics import fbeta_score\n# fbeta_score(y_true, y_pred, average='macro', beta=2)","metadata":{},"execution_count":null,"outputs":[]}]}